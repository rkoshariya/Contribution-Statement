idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,BIO_2,offset1,pro1,offset2,pro2,offset3,pro3,mask,bi_labels,labels
1,title,,,text-classification,6,,,,0,0.0,0,0.0,0,0.0,1,0,
2,Universal Sentence Encoder,title,,text-classification,6,"['B', 'I', 'I']","['B-n', 'I-n', 'I-n']","['B-ob', 'I-ob', 'I-ob']",1,0.0,1,0.006756756756756757,1,0.0,1,1,research-problem
3,abstract,,,text-classification,6,,,,0,0.0,2,0.013513513513513514,0,0.0,1,0,
4,We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .,abstract,abstract,text-classification,6,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.02027027027027027,1,0.1111111111111111,1,1,research-problem
5,The models are efficient and result in accurate performance on diverse transfer tasks .,abstract,abstract,text-classification,6,,,,2,0.2222222222222222,4,0.02702702702702703,2,0.2222222222222222,1,0,
6,Two variants of the encoding models allow for trade - offs between accuracy and compute resources .,abstract,abstract,text-classification,6,,,,3,0.3333333333333333,5,0.033783783783783786,3,0.3333333333333333,1,0,
7,"For both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance .",abstract,abstract,text-classification,6,,,,4,0.4444444444444444,6,0.04054054054054054,4,0.4444444444444444,1,0,
8,Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning .,abstract,abstract,text-classification,6,,,,5,0.5555555555555556,7,0.0472972972972973,5,0.5555555555555556,1,0,
9,We find that transfer learning using sentence embeddings tends to outperform word level transfer .,abstract,abstract,text-classification,6,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.6666666666666666,8,0.05405405405405406,6,0.6666666666666666,1,1,research-problem
10,"With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .",abstract,abstract,text-classification,6,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.7777777777777778,9,0.060810810810810814,7,0.7777777777777778,1,1,research-problem
11,We obtain encouraging results on Word Embedding Association Tests ( WEAT ) targeted at detecting model bias .,abstract,abstract,text-classification,6,,,,8,0.8888888888888888,10,0.06756756756756757,8,0.8888888888888888,1,0,
12,Our pre-trained sentence encoding models are made freely available for download and on TF Hub .,abstract,abstract,text-classification,6,,,,9,1.0,11,0.07432432432432433,9,1.0,1,0,
13,Introduction,,,text-classification,6,,,,0,0.0,12,0.08108108108108109,0,0.0,1,0,
14,Limited amounts of training data are available for many NLP tasks .,Introduction,Introduction,text-classification,6,,,,1,0.06666666666666667,13,0.08783783783783784,1,0.06666666666666667,1,0,
15,This presents a challenge for data hungry deep learning methods .,Introduction,Introduction,text-classification,6,,,,2,0.13333333333333333,14,0.0945945945945946,2,0.13333333333333333,1,0,
16,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .",Introduction,Introduction,text-classification,6,,,,3,0.2,15,0.10135135135135136,3,0.2,1,0,
17,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,Introduction,Introduction,text-classification,6,,,,4,0.26666666666666666,16,0.10810810810810811,4,0.26666666666666666,1,0,
18,"However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .",Introduction,Introduction,text-classification,6,,,,5,0.3333333333333333,17,0.11486486486486487,5,0.3333333333333333,1,0,
19,"In this paper , we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks .",Introduction,Introduction,text-classification,6,,,,6,0.4,18,0.12162162162162163,6,0.4,1,0,
20,We include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size .,Introduction,Introduction,text-classification,6,,,,7,0.4666666666666667,19,0.12837837837837837,7,0.4666666666666667,1,0,
21,We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data .,Introduction,Introduction,text-classification,6,,,,8,0.5333333333333333,20,0.13513513513513514,8,0.5333333333333333,1,0,
22,The sentence encoding models are made publicly available on TF Hub .,Introduction,Introduction,text-classification,6,,,,9,0.6,21,0.14189189189189189,9,0.6,1,0,
23,Engineering characteristics of models used for transfer learning are an important consideration .,Introduction,Introduction,text-classification,6,,,,10,0.6666666666666666,22,0.14864864864864866,10,0.6666666666666666,1,0,
24,We discuss modeling trade - offs regarding memory requirements as well as compute time on CPU and GPU .,Introduction,Introduction,text-classification,6,,,,11,0.7333333333333333,23,0.1554054054054054,11,0.7333333333333333,1,0,
25,Resource consumption comparisons are made for sentences of varying lengths .,Introduction,Introduction,text-classification,6,,,,12,0.8,24,0.16216216216216217,12,0.8,1,0,
26,"import tensorflow_hub as hub embed = hub.Module ( "" https://tfhub.dev/google/ "" "" universal- sentence - encoder / 1 "" ) embedding = embed ( [",Introduction,Introduction,text-classification,6,,,,13,0.8666666666666667,25,0.16891891891891891,13,0.8666666666666667,1,0,
27,""" The quick brown fox jumps over the lazy dog . "" ] )",Introduction,Introduction,text-classification,6,,,,14,0.9333333333333333,26,0.17567567567567569,14,0.9333333333333333,1,0,
28,Listing 1 : Python example code for using the universal sentence encoder .,Introduction,Introduction,text-classification,6,,,,15,1.0,27,0.18243243243243243,15,1.0,1,0,
29,Model Toolkit,,,text-classification,6,,,,0,0.0,28,0.1891891891891892,0,0.0,1,0,
30,We make available two new models for encoding sentences into embedding vectors .,Model Toolkit,Model Toolkit,text-classification,6,,,,1,0.03225806451612903,29,0.19594594594594594,1,0.125,1,0,
31,"One makes use of the transformer architecture , while the other is formulated as a deep averaging network ( DAN ) .",Model Toolkit,Model Toolkit,text-classification,6,,,,2,0.06451612903225806,30,0.20270270270270271,2,0.25,1,0,
32,Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1,Model Toolkit,Model Toolkit,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob']",3,0.0967741935483871,31,0.20945945945945946,3,0.375,1,1,code
33,The models take as input English strings and produce as output a fixed dimensional embedding representation of the string .,Model Toolkit,Model Toolkit,text-classification,6,,,,4,0.12903225806451613,32,0.21621621621621623,4,0.5,1,0,
34,Listing 1 provides a minimal code snippet to convert a sentence into a tensor containing its sentence embedding .,Model Toolkit,Model Toolkit,text-classification,6,,,,5,0.16129032258064516,33,0.22297297297297297,5,0.625,1,0,
35,The embedding tensor can be used directly or incorporated into larger model graphs for specific tasks .,Model Toolkit,Model Toolkit,text-classification,6,,,,6,0.1935483870967742,34,0.22972972972972974,6,0.75,1,0,
36,"As illustrated in , the sentence embeddings can be trivially used to compute sentence level semantic similarity scores that achieve excellent performance on the semantic textual similarity ( STS ) Benchmark .",Model Toolkit,Model Toolkit,text-classification,6,,,,7,0.22580645161290322,35,0.23648648648648649,7,0.875,1,0,
37,"When included within larger models , the sentence encoding models can be fine tuned for specific tasks using gradient based updates .",Model Toolkit,Model Toolkit,text-classification,6,,,,8,0.25806451612903225,36,0.24324324324324326,8,1.0,1,0,
38,Encoders,Model Toolkit,,text-classification,6,,,,9,0.2903225806451613,37,0.25,0,0.0,1,0,
39,We introduce the model architecture for our two encoding models in this section .,Model Toolkit,Encoders,text-classification,6,,,,10,0.3225806451612903,38,0.25675675675675674,1,0.25,1,0,
40,Our two encoders have different design goals .,Model Toolkit,Encoders,text-classification,6,,,,11,0.3548387096774194,39,0.2635135135135135,2,0.5,1,0,
41,One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption .,Model Toolkit,Encoders,text-classification,6,,,,12,0.3870967741935484,40,0.2702702702702703,3,0.75,1,0,
42,The other targets efficient inference with slightly reduced accuracy .,Model Toolkit,Encoders,text-classification,6,,,,13,0.41935483870967744,41,0.27702702702702703,4,1.0,1,0,
43,Transformer,Model Toolkit,,text-classification,6,,,,14,0.45161290322580644,42,0.28378378378378377,0,0.0,1,0,
44,The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",15,0.4838709677419355,43,0.2905405405405405,1,0.058823529411764705,1,1,model
45,This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",16,0.5161290322580645,44,0.2972972972972973,2,0.11764705882352941,1,1,model
46,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,Model Toolkit,Transformer,text-classification,6,"['O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.5483870967741935,45,0.30405405405405406,3,0.17647058823529413,1,1,model
47,The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding .,Model Toolkit,Transformer,text-classification,6,,,,18,0.5806451612903226,46,0.3108108108108108,4,0.23529411764705882,1,0,
48,The encoding model is designed to be as general purpose as possible .,Model Toolkit,Transformer,text-classification,6,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O']",19,0.6129032258064516,47,0.31756756756756754,5,0.29411764705882354,1,1,model
49,This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",20,0.6451612903225806,48,0.32432432432432434,6,0.35294117647058826,1,1,model
50,The supported tasks include : a Skip - Thought like task for the unsupervised learning from arbitrary running text ; a conversational input - response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data .,Model Toolkit,Transformer,text-classification,6,,,,21,0.6774193548387096,49,0.3310810810810811,7,0.4117647058823529,1,0,
51,The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .,Model Toolkit,Transformer,text-classification,6,,,,22,0.7096774193548387,50,0.33783783783783783,8,0.47058823529411764,1,0,
52,"As will be shown in the experimental results below , the transformer based encoder achieves the best over all transfer task performance .",Model Toolkit,Transformer,text-classification,6,,,,23,0.7419354838709677,51,0.34459459459459457,9,0.5294117647058824,1,0,
53,"However , this comes at the cost of compute time and memory usage scaling dramatically with sentence length .",Model Toolkit,Transformer,text-classification,6,,,,24,0.7741935483870968,52,0.35135135135135137,10,0.5882352941176471,1,0,
54,Deep Averaging Network ( DAN ),Model Toolkit,Transformer,text-classification,6,"['B', 'I', 'I', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b']",25,0.8064516129032258,53,0.3581081081081081,11,0.6470588235294118,1,1,model
55,The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",26,0.8387096774193549,54,0.36486486486486486,12,0.7058823529411765,1,1,model
56,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O']",,,27,0.8709677419354839,55,0.3716216216216216,13,0.7647058823529411,1,1,model
57,The DAN encoder is trained similarly to the Transformer based encoder .,Model Toolkit,Transformer,text-classification,6,,,,28,0.9032258064516129,56,0.3783783783783784,14,0.8235294117647058,1,0,
58,We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,Model Toolkit,Transformer,text-classification,6,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",29,0.9354838709677419,57,0.38513513513513514,15,0.8823529411764706,1,1,model
59,The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .,Model Toolkit,Transformer,text-classification,6,"['O', 'B', 'I', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'B-p', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-ob', 'I-ob', 'O']",30,0.967741935483871,58,0.3918918918918919,16,0.9411764705882353,1,1,model
60,"Similar to , our results demonstrate that DANs achieve strong baseline performance on text classification tasks .",Model Toolkit,Transformer,text-classification,6,,,,31,1.0,59,0.39864864864864863,17,1.0,1,0,
61,Encoder Training Data,,,text-classification,6,,,,0,0.0,60,0.40540540540540543,0,0.0,1,0,
62,Unsupervised training data for the sentence encoding models are drawn from a variety of web sources .,Encoder Training Data,Encoder Training Data,text-classification,6,,,,1,0.05,61,0.41216216216216217,1,0.25,1,0,
63,"The sources are Wikipedia , web news , web question - answer pages and discussion forums .",Encoder Training Data,Encoder Training Data,text-classification,6,,,,2,0.1,62,0.4189189189189189,2,0.5,1,0,
64,We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference ( SNLI ) corpus .,Encoder Training Data,Encoder Training Data,text-classification,6,,,,3,0.15,63,0.42567567567567566,3,0.75,1,0,
65,"Similar to the findings of , we observe that training to SNLI improves transfer performance .",Encoder Training Data,Encoder Training Data,text-classification,6,,,,4,0.2,64,0.43243243243243246,4,1.0,1,0,
66,Transfer Tasks,Encoder Training Data,,text-classification,6,,,,5,0.25,65,0.4391891891891892,0,0.0,1,0,
67,This section presents an overview of the data used for the transfer learning experiments and the Word Embedding Association Test ( WEAT ) data used to characterize model bias .,Encoder Training Data,Transfer Tasks,text-classification,6,,,,6,0.3,66,0.44594594594594594,1,0.06666666666666667,1,0,
68,"summarizes the number of samples provided by the test portion of each evaluation set and , when available , the size of the dev and training data .",Encoder Training Data,Transfer Tasks,text-classification,6,,,,7,0.35,67,0.4527027027027027,2,0.13333333333333333,1,0,
69,MR : Movie review snippet sentiment on a five star scale .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",8,0.4,68,0.4594594594594595,3,0.2,1,1,tasks
70,CR : Sentiment of sentences mined from customer reviews .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",9,0.45,69,0.46621621621621623,4,0.26666666666666666,1,1,tasks
71,SUBJ : Subjectivity of sentences from movie reviews and plot summaries .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.5,70,0.47297297297297297,5,0.3333333333333333,1,1,tasks
72,MPQA : Phrase level opinion polarity from news data .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",11,0.55,71,0.4797297297297297,6,0.4,1,1,tasks
73,TREC : Fine grained question classification sourced from TREC .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'O']",,,12,0.6,72,0.4864864864864865,7,0.4666666666666667,1,1,tasks
74,SST : Binary phrase level sentiment classification .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",13,0.65,73,0.49324324324324326,8,0.5333333333333333,1,1,tasks
75,STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",14,0.7,74,0.5,9,0.6,1,1,tasks
76,WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .,Encoder Training Data,Transfer Tasks,text-classification,6,"['B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.75,75,0.5067567567567568,10,0.6666666666666666,1,1,tasks
77,"For sentence classification transfer tasks , the output of the transformer and DAN sentence encoders are provided to a task specific DNN .",Encoder Training Data,Transfer Tasks,text-classification,6,,,,16,0.8,76,0.5135135135135135,11,0.7333333333333333,1,0,
78,"For the pairwise semantic similarity task , we directly assess the similarity of the sentence embeddings produced by our two encoders .",Encoder Training Data,Transfer Tasks,text-classification,6,,,,17,0.85,77,0.5202702702702703,12,0.8,1,0,
79,"As shown Eq. 1 , we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance .",Encoder Training Data,Transfer Tasks,text-classification,6,,,,18,0.9,78,0.527027027027027,13,0.8666666666666667,1,0,
80,"5 sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /?",Encoder Training Data,Transfer Tasks,text-classification,6,,,,19,0.95,79,0.5337837837837838,14,0.9333333333333333,1,0,
81,( 1 ),Encoder Training Data,Transfer Tasks,text-classification,6,,,,20,1.0,80,0.5405405405405406,15,1.0,1,0,
82,Baselines,,,text-classification,6,,,,0,0.0,81,0.5472972972972973,0,0.0,1,0,
83,"For each transfer task , we include baselines that only make use of word level transfer and baselines that make use of no transfer learning at all .",Baselines,Baselines,text-classification,6,,,,1,0.2,82,0.5540540540540541,1,0.2,1,0,
84,"For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .",Baselines,Baselines,text-classification,6,"['B', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'O']",2,0.4,83,0.5608108108108109,2,0.4,1,1,baselines
85,The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .,Baselines,Baselines,text-classification,6,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'O', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-ob', 'O', 'O', 'O', 'B-ob', 'O']",3,0.6,84,0.5675675675675675,3,0.6,1,1,baselines
86,The baselines that use pretrained word embeddings allow us to contrast word versus sentence level transfer .,Baselines,Baselines,text-classification,6,,,,4,0.8,85,0.5743243243243243,4,0.8,1,0,
87,Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .,Baselines,Baselines,text-classification,6,"['B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,1.0,86,0.581081081081081,5,1.0,1,1,baselines
88,Combined Transfer Models,,,text-classification,6,,,,0,0.0,87,0.5878378378378378,0,0.0,1,0,
89,We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation :,Combined Transfer Models,Combined Transfer Models,text-classification,6,,,,1,0.09090909090909091,88,0.5945945945945946,1,0.09090909090909091,1,0,
90,Model performance on transfer tasks .,Combined Transfer Models,Combined Transfer Models,text-classification,6,,,,2,0.18181818181818182,89,0.6013513513513513,2,0.18181818181818182,1,0,
91,USE,Combined Transfer Models,,text-classification,6,,,,3,0.2727272727272727,90,0.6081081081081081,3,0.2727272727272727,1,0,
92,T is the universal sentence encoder ( USE ) using Transformer .,Combined Transfer Models,USE,text-classification,6,,,,4,0.36363636363636365,91,0.6148648648648649,4,0.36363636363636365,1,0,
93,USE,Combined Transfer Models,,text-classification,6,,,,5,0.45454545454545453,92,0.6216216216216216,5,0.45454545454545453,1,0,
94,Dis the universal encoder DAN model .,Combined Transfer Models,USE,text-classification,6,,,,6,0.5454545454545454,93,0.6283783783783784,6,0.5454545454545454,1,0,
95,"Models tagged with w2 v w.e. make use of pre-training word2vec skip - gram embeddings for the transfer task model , while models tagged with lrn w.e. use randomly initialized word embeddings that are learned only on the transfer task data .",Combined Transfer Models,USE,text-classification,6,,,,7,0.6363636363636364,94,0.6351351351351351,7,0.6363636363636364,1,0,
96,Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments .,Combined Transfer Models,USE,text-classification,6,,,,8,0.7272727272727273,95,0.6418918918918919,8,0.7272727272727273,1,0,
97,Pairwise similarity scores are computed directly using the sentence embeddings from the universal sentence encoder as in Eq. ( 1 ) .,Combined Transfer Models,USE,text-classification,6,,,,9,0.8181818181818182,96,0.6486486486486487,9,0.8181818181818182,1,0,
98,to the transfer task classification layers .,Combined Transfer Models,USE,text-classification,6,,,,10,0.9090909090909091,97,0.6554054054054054,10,0.9090909090909091,1,0,
99,"For completeness , we also explore concatenating the representations from sentence level transfer models with the baseline models that do not make use of word level transfer learning .",Combined Transfer Models,USE,text-classification,6,,,,11,1.0,98,0.6621621621621622,11,1.0,1,0,
100,Experiments,,,text-classification,6,,,,0,0.0,99,0.668918918918919,0,0.0,1,0,
101,Transfer task model hyperparamaters are tuned using a combination of Vizier and light manual tuning .,Experiments,Experiments,text-classification,6,,,,1,0.1111111111111111,100,0.6756756756756757,1,0.1111111111111111,1,0,
102,"When available , model hyperparameters are tuned using task dev sets .",Experiments,Experiments,text-classification,6,,,,2,0.2222222222222222,101,0.6824324324324325,2,0.2222222222222222,1,0,
103,"Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .",Experiments,Experiments,text-classification,6,,,,3,0.3333333333333333,102,0.6891891891891891,3,0.3333333333333333,1,0,
104,Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs .,Experiments,Experiments,text-classification,6,,,,4,0.4444444444444444,103,0.6959459459459459,4,0.4444444444444444,1,0,
105,Transfer learning is critically important when training data for a target task is limited .,Experiments,Experiments,text-classification,6,,,,5,0.5555555555555556,104,0.7027027027027027,5,0.5555555555555556,1,0,
106,We explore the impact on task performance of varying the amount of training data available for the task both with and without the use of transfer learning .,Experiments,Experiments,text-classification,6,,,,6,0.6666666666666666,105,0.7094594594594594,6,0.6666666666666666,1,0,
107,"Contrasting the transformer and DAN based encoders , we demonstrate trade - offs in model complexity and the amount of data required to reach a desired level of accuracy on a task .",Experiments,Experiments,text-classification,6,,,,7,0.7777777777777778,106,0.7162162162162162,7,0.7777777777777778,1,0,
108,"To assess bias in our encoding models , we evaluate the strength of various associations learned by our model on WEAT word lists .",Experiments,Experiments,text-classification,6,,,,8,0.8888888888888888,107,0.722972972972973,8,0.8888888888888888,1,0,
109,We compare our result to those of who discovered that word embeddings could be used to reproduce human performance on implicit association tasks for both benign and potentially undesirable associations .,Experiments,Experiments,text-classification,6,,,,9,1.0,108,0.7297297297297297,9,1.0,1,0,
110,Results,,,text-classification,6,,,,0,0.0,109,0.7364864864864865,0,0.0,1,0,
111,Transfer task performance is summarized in Table 2 . ,Results,Results,text-classification,6,,,,1,0.03225806451612903,110,0.7432432432432432,1,0.08333333333333333,1,0,
112,We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .,Results,Results,text-classification,6,"['O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'O']",,,2,0.06451612903225806,111,0.75,2,0.16666666666666666,1,1,results
113,"Hoewver , transfer learning using the simpler and fast DAN encoder can for some tasks perform as well or better than the more sophisticated transformer encoder .",Results,Results,text-classification,6,,,,3,0.0967741935483871,112,0.7567567567567568,3,0.25,1,0,
114,Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .,Results,Results,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.12903225806451613,113,0.7635135135135135,4,0.3333333333333333,1,1,results
115,The best performance on most tasks is obtained by models that make use of both sentence and word level transfer .,Results,Results,text-classification,6,,,,5,0.16129032258064516,114,0.7702702702702703,5,0.4166666666666667,1,0,
116,illustrates transfer task performance for varying amounts of training data .,Results,Results,text-classification,6,,,,6,0.1935483870967742,115,0.777027027027027,6,0.5,1,0,
117,"We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .",Results,Results,text-classification,6,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.22580645161290322,116,0.7837837837837838,7,0.5833333333333334,1,1,results
118,"As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .",Results,Results,text-classification,6,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",8,0.25806451612903225,117,0.7905405405405406,8,0.6666666666666666,1,1,results
119,contrasts 's findings on bias within GloVe embeddings with the DAN variant of the universal encoder .,Results,Results,text-classification,6,,,,9,0.2903225806451613,118,0.7972972972972973,9,0.75,1,0,
120,"Similar to GloVe , our model reproduces human associations between flowers vs. insects and pleasantness vs. unpleasantness .",Results,Results,text-classification,6,,,,10,0.3225806451612903,119,0.8040540540540541,10,0.8333333333333334,1,0,
121,"However , our model demonstrates weaker associations than GloVe for probes targeted at revealing at ageism , racism and sexism .",Results,Results,text-classification,6,,,,11,0.3548387096774194,120,0.8108108108108109,11,0.9166666666666666,1,0,
122,The differences in word association patterns can be attributed to differences in the training data composition and the mixture of tasks used to train the sentence embeddings .,Results,Results,text-classification,6,,,,12,0.3870967741935484,121,0.8175675675675675,12,1.0,1,0,
123,Discussion,Results,,text-classification,6,,,,13,0.41935483870967744,122,0.8243243243243243,0,0.0,1,0,
124,Transfer learning leads to performance improvements on many tasks .,Results,Discussion,text-classification,6,,,,14,0.45161290322580644,123,0.831081081081081,1,0.2,1,0,
125,Using transfer learning is more critical when less training data is available .,Results,Discussion,text-classification,6,,,,15,0.4838709677419355,124,0.8378378378378378,2,0.4,1,0,
126,"When task performance is close , the correct modeling choice should take into account engineering trade - offs regarding the memory and compute 6 Researchers and developers are strongly encouraged to independently verify whether biases in their over all model or model components impacts their use case .",Results,Discussion,text-classification,6,,,,16,0.5161290322580645,125,0.8445945945945946,3,0.6,1,0,
127,For resources on ML fairness visit https://developers.google.com/machinelearning/fairness-overview/.,Results,Discussion,text-classification,6,,,,17,0.5483870967741935,126,0.8513513513513513,4,0.8,1,0,
128,resource requirements introduced by the different models that could be used .,Results,Discussion,text-classification,6,,,,18,0.5806451612903226,127,0.8581081081081081,5,1.0,1,0,
129,Resource Usage,Results,,text-classification,6,,,,19,0.6129032258064516,128,0.8648648648648649,0,0.0,1,0,
130,This section describes memory and compute resource usage for the transformer and DAN sentence encoding models for different sentence lengths .,Results,Resource Usage,text-classification,6,,,,20,0.6451612903225806,129,0.8716216216216216,1,0.08333333333333333,1,0,
131,Figure 2 plots model resource usage against sentence length .,Results,Resource Usage,text-classification,6,,,,21,0.6774193548387096,130,0.8783783783783784,2,0.16666666666666666,1,0,
132,Compute Usage,Results,,text-classification,6,,,,22,0.7096774193548387,131,0.8851351351351351,3,0.25,1,0,
133,"The transformer model time complexity is O ( n 2 ) in sentence length , while the DAN model is O ( n ) .",Results,Compute Usage,text-classification,6,,,,23,0.7419354838709677,132,0.8918918918918919,4,0.3333333333333333,1,0,
134,"As seen in ( a - b ) , for short sentences , the transformer encoding model is only moderately slower than the much simpler DAN model .",Results,Compute Usage,text-classification,6,,,,24,0.7741935483870968,133,0.8986486486486487,5,0.4166666666666667,1,0,
135,"However , compute time for transformer increases noticeably as sentence length increases .",Results,Compute Usage,text-classification,6,,,,25,0.8064516129032258,134,0.9054054054054054,6,0.5,1,0,
136,"In contrast , the compute time for the DAN model stays nearly constant as sentence length is increased .",Results,Compute Usage,text-classification,6,,,,26,0.8387096774193549,135,0.9121621621621622,7,0.5833333333333334,1,0,
137,"Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .",Results,Compute Usage,text-classification,6,,,,27,0.8709677419354839,136,0.918918918918919,8,0.6666666666666666,1,0,
138,Memory Usage,Results,,text-classification,6,,,,28,0.9032258064516129,137,0.9256756756756757,9,0.75,1,0,
139,"The transformer model space complexity also scales quadratically , O ( n 2 ) , in sentence length , while the DAN model space complexity is constant in the length of the sentence . Similar to compute usage , memory usage for the transformer model increases quickly with sentence length , while the memory usage for the DAN model remains constant .",Results,Memory Usage,text-classification,6,,,,29,0.9354838709677419,138,0.9324324324324325,10,0.8333333333333334,1,0,
140,"We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .",Results,Memory Usage,text-classification,6,,,,30,0.967741935483871,139,0.9391891891891891,11,0.9166666666666666,1,0,
141,"Since the transformer model only needs to store unigram embeddings , for short sequences it requires nearly half as much memory as the DAN model .",Results,Memory Usage,text-classification,6,,,,31,1.0,140,0.9459459459459459,12,1.0,1,0,
142,Conclusion,,,text-classification,6,,,,0,0.0,141,0.9527027027027027,0,0.0,1,0,
143,Both the transformer and DAN based universal encoding models provide sentence level embeddings that demonstrate strong transfer performance on a number of NLP tasks .,Conclusion,Conclusion,text-classification,6,,,,1,0.16666666666666666,142,0.9594594594594594,1,0.16666666666666666,0,0,
144,The sentence level embeddings surpass the performance of transfer learning using word level embeddings alone .,Conclusion,Conclusion,text-classification,6,,,,2,0.3333333333333333,143,0.9662162162162162,2,0.3333333333333333,0,0,
145,Models that make use of sentence and word level transfer achieve the best over all performance .,Conclusion,Conclusion,text-classification,6,,,,3,0.5,144,0.972972972972973,3,0.5,0,0,
146,We observe that transfer learning is most helpful when limited training data is available for the transfer task .,Conclusion,Conclusion,text-classification,6,,,,4,0.6666666666666666,145,0.9797297297297297,4,0.6666666666666666,0,0,
147,The encoding models make different trade - offs regarding accuracy and model complexity that should be considered when choosing the best model for a particular application .,Conclusion,Conclusion,text-classification,6,,,,5,0.8333333333333334,146,0.9864864864864865,5,0.8333333333333334,0,0,
148,The pre-trained encoding models will be made publicly available for research and use in applications that can benefit from a better understanding of natural language .,Conclusion,Conclusion,text-classification,6,,,,6,1.0,147,0.9932432432432432,6,1.0,0,0,
1,title,,,text-classification,2,,,,0,0.0,0,0.0,0,0.0,1,0,
2,Bag of Tricks for Efficient Text Classification,title,title,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'I', 'I']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob']",1,0.0,1,0.010752688172043012,1,0.0,1,1,research-problem
3,abstract,,,text-classification,2,,,,0,0.0,2,0.021505376344086023,0,0.0,1,0,
4,This paper explores a simple and efficient baseline for text classification .,abstract,abstract,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",1,0.3333333333333333,3,0.03225806451612903,1,0.3333333333333333,1,1,research-problem
5,"Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy , and many orders of magnitude faster for training and evaluation .",abstract,abstract,text-classification,2,,,,2,0.6666666666666666,4,0.043010752688172046,2,0.6666666666666666,1,0,
6,"We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU , and classify half a million sentences among 312K classes in less than a minute .",abstract,abstract,text-classification,2,,,,3,1.0,5,0.053763440860215055,3,1.0,1,0,
7,Introduction,,,text-classification,2,,,,0,0.0,6,0.06451612903225806,0,0.0,1,0,
8,"Text classification is an important task in Natural Language Processing with many applications , such as web search , information retrieval , ranking and document classification .",Introduction,Introduction,text-classification,2,,,,1,0.1111111111111111,7,0.07526881720430108,1,0.1111111111111111,1,0,
9,"Recently , models based on neural networks have become increasingly popular .",Introduction,Introduction,text-classification,2,,,,2,0.2222222222222222,8,0.08602150537634409,2,0.2222222222222222,1,0,
10,"While these models achieve very good performance in practice , they tend to be relatively slow both at train and test time , limiting their use on very large datasets .",Introduction,Introduction,text-classification,2,,,,3,0.3333333333333333,9,0.0967741935483871,3,0.3333333333333333,1,0,
11,"Meanwhile , linear classifiers are often considered as strong baselines for text classification problems .",Introduction,Introduction,text-classification,2,,,,4,0.4444444444444444,10,0.10752688172043011,4,0.4444444444444444,1,0,
12,"Despite their simplicity , they often obtain stateof - the - art performances if the right features are used .",Introduction,Introduction,text-classification,2,,,,5,0.5555555555555556,11,0.11827956989247312,5,0.5555555555555556,1,0,
13,They also have the potential to scale to very large corpus .,Introduction,Introduction,text-classification,2,,,,6,0.6666666666666666,12,0.12903225806451613,6,0.6666666666666666,1,0,
14,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",Introduction,Introduction,text-classification,2,,,,7,0.7777777777777778,13,0.13978494623655913,7,0.7777777777777778,1,0,
15,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",Introduction,Introduction,text-classification,2,,,,8,0.8888888888888888,14,0.15053763440860216,8,0.8888888888888888,1,0,
16,"We evaluate the quality of our approach fastText 1 on two different tasks , namely tag prediction and sentiment analysis .",Introduction,Introduction,text-classification,2,,,,9,1.0,15,0.16129032258064516,9,1.0,1,0,
17,Model architecture,,,text-classification,2,,,,0,0.0,16,0.17204301075268819,0,0.0,1,0,
18,"A simple and efficient baseline for sentence classification is to represent sentences as bag of words ( BoW ) and train a linear classifier , e.g. , a logistic regression or an SVM .",Model architecture,Model architecture,text-classification,2,,,,1,0.03333333333333333,17,0.1827956989247312,1,0.07692307692307693,1,0,
19,"However , linear classifiers do not share parameters among features and classes .",Model architecture,Model architecture,text-classification,2,,,,2,0.06666666666666667,18,0.1935483870967742,2,0.15384615384615385,1,0,
20,This possibly limits their generalization in the context of large output space where some classes have very few examples .,Model architecture,Model architecture,text-classification,2,,,,3,0.1,19,0.20430107526881722,3,0.23076923076923078,1,0,
21,Common solutions to this problem are to factorize the linear classifier into low rank matrices or to use multilayer neural networks .,Model architecture,Model architecture,text-classification,2,,,,4,0.13333333333333333,20,0.21505376344086022,4,0.3076923076923077,1,0,
22,shows a simple linear model with rank constraint .,Model architecture,Model architecture,text-classification,2,"['B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",5,0.16666666666666666,21,0.22580645161290322,5,0.38461538461538464,1,1,model
23,The first weight matrix A is a look - up table over the words .,Model architecture,Model architecture,text-classification,2,"['O', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O']",6,0.2,22,0.23655913978494625,6,0.46153846153846156,1,1,model
24,"The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .",Model architecture,Model architecture,text-classification,2,"['O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",7,0.23333333333333334,23,0.24731182795698925,7,0.5384615384615384,1,1,model
25,The text representa - tion is an hidden variable which can be potentially be reused .,Model architecture,Model architecture,text-classification,2,,,,8,0.26666666666666666,24,0.25806451612903225,8,0.6153846153846154,1,0,
26,"This architecture is similar to the cbow model of , where the middle word is replaced by a label .",Model architecture,Model architecture,text-classification,2,,,,9,0.3,25,0.26881720430107525,9,0.6923076923076923,1,0,
27,We use the softmax function f to compute the probability distribution over the predefined classes .,Model architecture,Model architecture,text-classification,2,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",10,0.3333333333333333,26,0.27956989247311825,10,0.7692307692307693,1,1,model
28,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",Model architecture,Model architecture,text-classification,2,,,,11,0.36666666666666664,27,0.2903225806451613,11,0.8461538461538461,1,0,
29,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",Model architecture,Model architecture,text-classification,2,,,,12,0.4,28,0.3010752688172043,12,0.9230769230769231,1,0,
30,This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .,Model architecture,Model architecture,text-classification,2,,,,13,0.43333333333333335,29,0.3118279569892473,13,1.0,1,0,
31,Hierarchical softmax,Model architecture,,text-classification,2,,,,14,0.4666666666666667,30,0.3225806451612903,0,0.0,1,0,
32,"When the number of classes is large , computing the linear classifier is computationally expensive .",Model architecture,Hierarchical softmax,text-classification,2,,,,15,0.5,31,0.3333333333333333,1,0.0625,1,0,
33,"More precisely , the computational complexity is O ( kh ) where k is the number of classes and h the dimension of the text representation .",Model architecture,Hierarchical softmax,text-classification,2,,,,16,0.5333333333333333,32,0.34408602150537637,2,0.125,1,0,
34,"In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .",Model architecture,Hierarchical softmax,text-classification,2,"['O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",17,0.5666666666666667,33,0.3548387096774194,3,0.1875,1,1,model
35,"During training , the computational complexity drops to O ( h log 2 ( k ) ) .",Model architecture,Hierarchical softmax,text-classification,2,,,,18,0.6,34,0.3655913978494624,4,0.25,1,0,
36,The hierarchical softmax is also advantageous at test time when searching for the most likely class .,Model architecture,Hierarchical softmax,text-classification,2,,,,19,0.6333333333333333,35,0.3763440860215054,5,0.3125,1,0,
37,Each node is associated with a probability that is the probability of the path from the root to that node .,Model architecture,Hierarchical softmax,text-classification,2,,,,20,0.6666666666666666,36,0.3870967741935484,6,0.375,1,0,
38,"If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is",Model architecture,Hierarchical softmax,text-classification,2,,,,21,0.7,37,0.3978494623655914,7,0.4375,1,0,
39,This means that the probability of anode is always lower than the one of its parent .,Model architecture,Hierarchical softmax,text-classification,2,,,,22,0.7333333333333333,38,0.40860215053763443,8,0.5,1,0,
40,Exploring the tree with a depth first search and tracking the maximum probability among the leaves allows us to discard any branch associated with a small probability .,Model architecture,Hierarchical softmax,text-classification,2,,,,23,0.7666666666666667,39,0.41935483870967744,9,0.5625,1,0,
41,"In practice , we observe a reduction of the complexity to O ( h log 2 ( k ) ) at test time .",Model architecture,Hierarchical softmax,text-classification,2,,,,24,0.8,40,0.43010752688172044,10,0.625,1,0,
42,"This approach is further extended to compute the T - top targets at the cost of O ( log ( T ) ) , using a binary heap .",Model architecture,Hierarchical softmax,text-classification,2,,,,25,0.8333333333333334,41,0.44086021505376344,11,0.6875,1,0,
43,N - gram features,Model architecture,,text-classification,2,,,,26,0.8666666666666667,42,0.45161290322580644,12,0.75,1,0,
44,Bag of words is invariant to word order but taking explicitly this order into account is often computationally very expensive .,Model architecture,N - gram features,text-classification,2,,,,27,0.9,43,0.46236559139784944,13,0.8125,1,0,
45,"Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .",Model architecture,N - gram features,text-classification,2,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",28,0.9333333333333333,44,0.4731182795698925,14,0.875,1,1,model
46,This is very efficient in practice while achieving comparable results to methods that explicitly use the order .,Model architecture,N - gram features,text-classification,2,"['O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'O', 'B-ob', 'I-ob', 'B-p', 'I-p', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'O']",29,0.9666666666666667,45,0.4838709677419355,15,0.9375,1,1,model
47,"We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .",Model architecture,N - gram features,text-classification,2,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O']",30,1.0,46,0.4946236559139785,16,1.0,1,1,model
48,Experiments,,,text-classification,2,,,,0,0.0,47,0.5053763440860215,0,0.0,1,0,
49,We evaluate fastText on two different tasks .,Experiments,,text-classification,2,,,,1,0.2,48,0.5161290322580645,1,0.25,1,0,
50,"First , we compare it to existing text classifers on the problem of sentiment analysis .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,,,,2,0.4,49,0.5268817204301075,2,0.5,1,0,
51,"Then , we evaluate its capacity to scale to large output space on a tag prediction dataset .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,,,,3,0.6,50,0.5376344086021505,3,0.75,1,0,
52,"Note that our model could be implemented with the Vowpal Wabbit library , 2 but we observe in practice , that our tailored implementation is at least 2 - 5 faster .",Experiments,We evaluate fastText on two different tasks .,text-classification,2,,,,4,0.8,51,0.5483870967741935,4,1.0,1,0,
53,Sentiment analysis,Experiments,,text-classification,2,"['B', 'I']","['B-n', 'I-n']","['B-b', 'I-b']",5,1.0,52,0.5591397849462365,0,0.0,1,1,tasks
54,Datasets and baselines .,,,text-classification,2,,,,0,0.0,53,0.5698924731182796,1,0.06666666666666667,1,0,
55,We employ the same 8 datasets and evaluation protocol of .,Datasets and baselines .,Datasets and baselines .,text-classification,2,,,,1,0.3333333333333333,54,0.5806451612903226,2,0.13333333333333333,1,0,
56,We report the n-grams and TFIDF baselines from We also compare to following their evaluation protocol .,Datasets and baselines .,Datasets and baselines .,text-classification,2,,,,2,0.6666666666666666,55,0.5913978494623656,3,0.2,1,0,
57,We report their main baselines as well as their two approaches based on recurrent networks ( Conv - GRNN and LSTM - GRNN ) .,Datasets and baselines .,Datasets and baselines .,text-classification,2,,,,3,1.0,56,0.6021505376344086,4,0.26666666666666666,1,0,
58,Results .,,,text-classification,2,,,,0,0.0,57,0.6129032258064516,5,0.3333333333333333,1,0,
59,We present the results in .,Results .,,text-classification,2,,,,1,0.09090909090909091,58,0.6236559139784946,6,0.4,1,0,
60,"We use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'B', 'B', 'I', 'O', 'B', 'B', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-ob', 'B-p', 'I-p', 'O', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.18181818181818182,59,0.6344086021505376,7,0.4666666666666667,1,1,tasks
61,"On this task , adding bigram information improves the performance by 1 - 4 % .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.2727272727272727,60,0.6451612903225806,8,0.5333333333333333,1,1,tasks
62,"Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .",Results .,We present the results in .,text-classification,2,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'O']",4,0.36363636363636365,61,0.6559139784946236,9,0.6,1,1,tasks
63,"Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .",Results .,We present the results in .,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",5,0.45454545454545453,62,0.6666666666666666,10,0.6666666666666666,1,1,tasks
64,"Finally , shows that our method is competitive with the methods presented in .",Results .,We present the results in .,text-classification,2,,,,6,0.5454545454545454,63,0.6774193548387096,11,0.7333333333333333,1,0,
65,We tune the hyperparameters on the validation set and observe that using n-grams up to 5 leads to the best performance .,Results .,We present the results in .,text-classification,2,,,,7,0.6363636363636364,64,0.6881720430107527,12,0.8,1,0,
66,"Unlike , fastText does not use pre-trained word embeddings , which can be explained the 1 % difference in accuracy .",Results .,We present the results in .,text-classification,2,,,,8,0.7272727272727273,65,0.6989247311827957,13,0.8666666666666667,1,0,
67,We show a few correct and incorrect tag predictions .,Results .,We present the results in .,text-classification,2,,,,9,0.8181818181818182,66,0.7096774193548387,14,0.9333333333333333,1,0,
68,"up compared to neural network based methods increases with the size of the dataset , going up to at least a 15,000 speed - up .",Results .,We present the results in .,text-classification,2,,,,10,0.9090909090909091,67,0.7204301075268817,15,1.0,1,0,
69,Tag prediction,Results .,,text-classification,2,"['B', 'I']","['B-n', 'I-n']","['B-b', 'I-b']",11,1.0,68,0.7311827956989247,0,0.0,1,1,tasks
70,Dataset and baselines .,,,text-classification,2,,,,0,0.0,69,0.7419354838709677,1,0.05555555555555555,1,0,
71,"To test scalability of our approach , further evaluation is carried on the YFCC100M dataset which consists of almost 100M images with captions , titles and tags .",Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,1,0.058823529411764705,70,0.7526881720430108,2,0.1111111111111111,1,0,
72,We focus on predicting the tags according to the title and caption ( we do not use the images ) .,Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,2,0.11764705882352941,71,0.7634408602150538,3,0.16666666666666666,1,0,
73,"We remove the words and tags occurring less than 100 times and split the data into a train , validation and test set .",Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,3,0.17647058823529413,72,0.7741935483870968,4,0.2222222222222222,1,0,
74,"The train set contains 91,188,648 examples ( 1.5B tokens ) .",Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,4,0.23529411764705882,73,0.7849462365591398,5,0.2777777777777778,1,0,
75,"The validation has 930,497 examples and the test set 543,424 .",Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,5,0.29411764705882354,74,0.7956989247311828,6,0.3333333333333333,1,0,
76,"The vocabulary size is 297,141 and there are 312,116 unique tags .",Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,6,0.35294117647058826,75,0.8064516129032258,7,0.3888888888888889,1,0,
77,We will release a script that recreates this dataset so that our numbers could be reproduced .,Dataset and baselines .,Dataset and baselines .,text-classification,2,,,,7,0.4117647058823529,76,0.8172043010752689,8,0.4444444444444444,1,0,
78,We report precision at 1 .,Dataset and baselines .,,text-classification,2,,,,8,0.47058823529411764,77,0.8279569892473119,9,0.5,1,0,
79,We consider a frequency - based baseline which predicts the most frequent tag .,Dataset and baselines .,We report precision at 1 .,text-classification,2,,,,9,0.5294117647058824,78,0.8387096774193549,10,0.5555555555555556,1,0,
80,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",Dataset and baselines .,We report precision at 1 .,text-classification,2,,,,10,0.5882352941176471,79,0.8494623655913979,11,0.6111111111111112,1,0,
81,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",Dataset and baselines .,We report precision at 1 .,text-classification,2,,,,11,0.6470588235294118,80,0.8602150537634409,12,0.6666666666666666,1,0,
82,Results and training time . and 200 .,Dataset and baselines .,,text-classification,2,,,,12,0.7058823529411765,81,0.8709677419354839,13,0.7222222222222222,1,0,
83,"Both models achieve a similar performance with a small hidden layer , but adding bigrams gives us a significant boost in accuracy .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,,,,13,0.7647058823529411,82,0.8817204301075269,14,0.7777777777777778,1,0,
84,"At test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,"['B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",14,0.8235294117647058,83,0.8924731182795699,15,0.8333333333333334,1,1,tasks
85,"Overall , we are more than an order of magnitude faster to obtain model with a better quality .",Dataset and baselines .,Results and training time . and 200 .,text-classification,2,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-n', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'I-p', 'B-b', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",15,0.8823529411764706,84,0.9032258064516129,16,0.8888888888888888,1,1,tasks
86,The speedup of the test phase is even more significant ( a 600 speedup ) .,Dataset and baselines .,Results and training time . and 200 .,text-classification,2,,,,16,0.9411764705882353,85,0.9139784946236559,17,0.9444444444444444,1,0,
87,shows some qualitative examples .,Dataset and baselines .,Results and training time . and 200 .,text-classification,2,,,,17,1.0,86,0.9247311827956989,18,1.0,1,0,
88,Discussion and conclusion,,,text-classification,2,,,,0,0.0,87,0.9354838709677419,0,0.0,1,0,
89,"In this work , we propose a simple baseline method for text classification .",Discussion and conclusion,Discussion and conclusion,text-classification,2,,,,1,0.2,88,0.946236559139785,1,0.2,0,0,
90,"Unlike unsupervisedly trained word vectors from word2vec , our word features can be averaged together to form good sentence representations .",Discussion and conclusion,Discussion and conclusion,text-classification,2,,,,2,0.4,89,0.956989247311828,2,0.4,0,0,
91,"In several tasks , fastText obtains performance on par with recently proposed methods inspired by deep learning , while being much faster .",Discussion and conclusion,Discussion and conclusion,text-classification,2,,,,3,0.6,90,0.967741935483871,3,0.6,0,0,
92,"Although deep neural networks have in theory much higher representational power than shallow models , it is not clear if simple text classification problems such as sentiment analysis are the right ones to evaluate them .",Discussion and conclusion,Discussion and conclusion,text-classification,2,,,,4,0.8,91,0.978494623655914,4,0.8,0,0,
93,We will publish our code so that the research community can easily build on top of our work .,Discussion and conclusion,Discussion and conclusion,text-classification,2,,,,5,1.0,92,0.989247311827957,5,1.0,0,0,
1,title,,,question-answering,7,,,,0,0.0,0,0.0,0,0.0,1,0,
2,Neural Semantic Encoders,title,title,question-answering,7,,,,1,0.0,1,0.0036363636363636364,1,0.0,1,0,
3,abstract,,,question-answering,7,,,,0,0.0,2,0.007272727272727273,0,0.0,1,0,
4,We present a memory augmented neural network for natural language understanding : Neural Semantic Encoders .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.03571428571428571,3,0.01090909090909091,1,0.014492753623188406,1,1,research-problem
5,"NSE is equipped with a novel memory update rule and has a variable sized encoding memory that evolves overtime and maintains the understanding of input sequences through read , compose and write operations .",abstract,abstract,question-answering,7,,,,2,0.07142857142857142,4,0.014545454545454545,2,0.028985507246376812,1,0,
6,NSE can also access 1 multiple and shared memories .,abstract,abstract,question-answering,7,,,,3,0.10714285714285714,5,0.01818181818181818,3,0.043478260869565216,1,0,
7,"In this paper , we demonstrated the effectiveness and the flexibility of NSE on five different natural language tasks : natural language inference , question answering , sentence classification , document sentiment analysis and machine translation where NSE achieved state - of - the - art performance when evaluated on publically available benchmarks .",abstract,abstract,question-answering,7,,,,4,0.14285714285714285,6,0.02181818181818182,4,0.057971014492753624,1,0,
8,"For example , our shared - memory model showed an encouraging result on neural machine translation , improving an attention - based baseline by approximately 1.0 BLEU .",abstract,abstract,question-answering,7,,,,5,0.17857142857142858,7,0.025454545454545455,5,0.07246376811594203,1,0,
9,Recurrent neural networks ( RNNs ) have been successful for modeling sequences,abstract,abstract,question-answering,7,,,,6,0.21428571428571427,8,0.02909090909090909,6,0.08695652173913043,1,0,
10,[ 1 ] .,abstract,abstract,question-answering,7,,,,7,0.25,9,0.03272727272727273,7,0.10144927536231885,1,0,
11,"Particularly , RNNs equipped with internal short memories , such as long short - term memories ( LSTM )",abstract,abstract,question-answering,7,,,,8,0.2857142857142857,10,0.03636363636363636,8,0.11594202898550725,1,0,
12,"[ 2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .",abstract,abstract,question-answering,7,,,,9,0.32142857142857145,11,0.04,9,0.13043478260869565,1,0,
13,LSTM is powerful because it learns to control it s short term memories .,abstract,abstract,question-answering,7,,,,10,0.35714285714285715,12,0.04363636363636364,10,0.14492753623188406,1,0,
14,"However , the short term memories in LSTM are a part of the training parameters .",abstract,abstract,question-answering,7,,,,11,0.39285714285714285,13,0.04727272727272727,11,0.15942028985507245,1,0,
15,This imposes some practical difficulties in training and modeling long sequences with LSTM .,abstract,abstract,question-answering,7,,,,12,0.42857142857142855,14,0.05090909090909091,12,0.17391304347826086,1,0,
16,Recently several studies have explored ways of extending the neural networks with an external memory [ 5 ] [ 6 ] [ 7 ] .,abstract,abstract,question-answering,7,,,,13,0.4642857142857143,15,0.05454545454545454,13,0.18840579710144928,1,0,
17,"Unlike LSTM , the short term memories and the training parameters of such a neural network are no longer coupled and can be adapted .",abstract,abstract,question-answering,7,,,,14,0.5,16,0.05818181818181818,14,0.2028985507246377,1,0,
18,In this paper we propose a novel class of memory augmented neural networks called Neural Semantic Encoders ( NSE ) for natural language understanding .,abstract,abstract,question-answering,7,"['O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",15,0.5357142857142857,17,0.06181818181818182,15,0.21739130434782608,1,1,model
19,NSE offers several desirable properties .,abstract,,question-answering,7,,,,16,0.5714285714285714,18,0.06545454545454546,16,0.2318840579710145,1,0,
20,NSE has a variable sized encoding memory which allows the model to access entire input sequence during the reading process ; therefore efficiently delivering long - term dependencies overtime .,abstract,NSE offers several desirable properties .,question-answering,7,"['B', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O']","['B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.6071428571428571,19,0.06909090909090909,17,0.2463768115942029,1,1,model
21,"The encoding memory evolves overtime and maintains the memory of the input sequence through read , compose and write operations .",abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'B', 'I', 'B', 'B', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",18,0.6428571428571429,20,0.07272727272727272,18,0.2608695652173913,1,1,model
22,NSE sequentially processes the input and supports word compositionality inheriting both temporal and hierarchical nature of human language .,abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'B', 'I', 'B', 'I', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",19,0.6785714285714286,21,0.07636363636363637,19,0.2753623188405797,1,1,model
23,NSE can read from and write to a set of relevant encoding memories simultaneously or multiple NSEs can access a shared encoding memory effectively supporting knowledge and representation sharing .,abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",20,0.7142857142857143,22,0.08,20,0.2898550724637681,1,1,model
24,"NSE is flexible , robust and suitable for practical NLU tasks and can be trained easily by any gradient descent optimizer .",abstract,NSE offers several desirable properties .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",21,0.75,23,0.08363636363636363,21,0.30434782608695654,1,1,research-problem
25,We evaluate NSE on five different real tasks .,abstract,,question-answering,7,,,,22,0.7857142857142857,24,0.08727272727272728,22,0.3188405797101449,1,0,
26,"For four of them , our models set new state - of - theart results .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,,,,23,0.8214285714285714,25,0.09090909090909091,23,0.3333333333333333,1,0,
27,Our results suggest that a NN model with the shared memory between encoder and decoder is a promising approach for sequence transduction problems such as machine translation and abstractive summarization .,abstract,We evaluate NSE on five different real tasks .,question-answering,7,,,,24,0.8571428571428571,26,0.09454545454545454,24,0.34782608695652173,1,0,
28,"In particular , we observe that the attention - based neural machine translation can be further improved by shared - memory models .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,,,,25,0.8928571428571429,27,0.09818181818181818,25,0.36231884057971014,1,0,
29,We also analyze memory access pattern and compositionality in NSE and show that our model captures semantic and syntactic structures of input sentence .,abstract,We evaluate NSE on five different real tasks .,question-answering,7,,,,26,0.9285714285714286,28,0.10181818181818182,26,0.37681159420289856,1,0,
30,1,abstract,We evaluate NSE on five different real tasks .,question-answering,7,,,,27,0.9642857142857143,29,0.10545454545454545,27,0.391304347826087,1,0,
31,"By access we mean changing the memory states by the read , compose and write operations .",abstract,We evaluate NSE on five different real tasks .,question-answering,7,,,,28,1.0,30,0.10909090909090909,28,0.4057971014492754,1,0,
32,Related Work,,,question-answering,7,,,,0,0.0,31,0.11272727272727273,29,0.42028985507246375,1,0,
33,One of the pioneering work that attempts to extend deep neural networks with an external memory is Neural Turing Machines ( NTM ) .,Related Work,Related Work,question-answering,7,,,,1,0.025,32,0.11636363636363636,30,0.43478260869565216,0,0,
34,NTM implements a centralized controller and a fixed - sized random access memory .,Related Work,Related Work,question-answering,7,,,,2,0.05,33,0.12,31,0.4492753623188406,0,0,
35,The NTM memory is addressable by both content ( i.e. soft attention ) and location based access mechanisms .,Related Work,Related Work,question-answering,7,,,,3,0.075,34,0.12363636363636364,32,0.463768115942029,0,0,
36,The authors evaluated NTM on algorithmic tasks such as copying and sorting sequences .,Related Work,Related Work,question-answering,7,,,,4,0.1,35,0.12727272727272726,33,0.4782608695652174,0,0,
37,Comparison with Neural Turing Machines : NSE addresses certain drawbacks of NTM .,Related Work,Related Work,question-answering,7,,,,5,0.125,36,0.13090909090909092,34,0.4927536231884058,0,0,
38,"NTM has a single centralized controller , which is usually an MLP or RNN while NSE takes a modular approach .",Related Work,Related Work,question-answering,7,,,,6,0.15,37,0.13454545454545455,35,0.5072463768115942,0,0,
39,"The main controller in NSE is decomposed into three separate modules , each of which performs for read , compose or write operation .",Related Work,Related Work,question-answering,7,,,,7,0.175,38,0.13818181818181818,36,0.5217391304347826,0,0,
40,"In NSE , the compose module is introduced in addition to the standard memory update operations ( i.e. read - write ) in order to process the memory entries and input information .",Related Work,Related Work,question-answering,7,,,,8,0.2,39,0.14181818181818182,37,0.5362318840579711,0,0,
41,The main advantage of NSE over NTM is in its memory update .,Related Work,Related Work,question-answering,7,,,,9,0.225,40,0.14545454545454545,38,0.5507246376811594,0,0,
42,"Despite its sophisticated addressing mechanism , the NTM controller does not have mechanism to avoid information collision in the memory .",Related Work,Related Work,question-answering,7,,,,10,0.25,41,0.14909090909090908,39,0.5652173913043478,0,0,
43,Particularly the NTM controller emits two separate set of access weights ( i.e. read weight and erase and write weights ) that do not explicitly encode the knowledge about where information is read from and written to .,Related Work,Related Work,question-answering,7,,,,11,0.275,42,0.15272727272727274,40,0.5797101449275363,0,0,
44,Moreover the fixed - size memory in NTM has no memory allocation or de-allocation protocol .,Related Work,Related Work,question-answering,7,,,,12,0.3,43,0.15636363636363637,41,0.5942028985507246,0,0,
45,"Therefore unless the controller is intelligent enough to track the previous read / write information , which is hard for an RNN when processing long sequences , the memory content is overlapped and information is overwritten throughout different time scales .",Related Work,Related Work,question-answering,7,,,,13,0.325,44,0.16,42,0.6086956521739131,0,0,
46,We think that this is a potential reason that makes NTM hard to train and makes the training not stable .,Related Work,Related Work,question-answering,7,,,,14,0.35,45,0.16363636363636364,43,0.6231884057971014,0,0,
47,We also note that the effectiveness of the location based addressing introduced in NTM is unclear .,Related Work,Related Work,question-answering,7,,,,15,0.375,46,0.16727272727272727,44,0.6376811594202898,0,0,
48,"In NSE , we introduce a novel and systematic memory update approach based on the soft attention mechanism .",Related Work,Related Work,question-answering,7,,,,16,0.4,47,0.1709090909090909,45,0.6521739130434783,0,0,
49,NSE writes new information to the most recently read memory locations .,Related Work,Related Work,question-answering,7,,,,17,0.425,48,0.17454545454545456,46,0.6666666666666666,0,0,
50,This is accomplished by sharing the same memory key vector between the read and write modules .,Related Work,Related Work,question-answering,7,,,,18,0.45,49,0.1781818181818182,47,0.6811594202898551,0,0,
51,The NSE memory update is scalable and potentially more robust to train .,Related Work,Related Work,question-answering,7,,,,19,0.475,50,0.18181818181818182,48,0.6956521739130435,0,0,
52,"NSE is provided with a variable sized memory and thus unlike NTM , the size of the NSE memory is more relaxed .",Related Work,Related Work,question-answering,7,,,,20,0.5,51,0.18545454545454546,49,0.7101449275362319,0,0,
53,The novel memory update mechanism and the variable sized memory together prevent NSE from the information collision issue and avoid the need of the memory allocation or de-allocation protocols .,Related Work,Related Work,question-answering,7,,,,21,0.525,52,0.1890909090909091,50,0.7246376811594203,0,0,
54,Each memory location of the NSE memory stores a token representation in input sequence during encoding .,Related Work,Related Work,question-answering,7,,,,22,0.55,53,0.19272727272727272,51,0.7391304347826086,0,0,
55,"This provides NSE with an anytime - access to the entire input sequence including the tokens from the future time scales , which is not permitted in NTM , RNN and attention - based encoders .",Related Work,Related Work,question-answering,7,,,,23,0.575,54,0.19636363636363635,52,0.7536231884057971,0,0,
56,"Lastly , NTM addresses small algorithmic problems while NSE focuses on a set of large - scale language understanding tasks .",Related Work,Related Work,question-answering,7,,,,24,0.6,55,0.2,53,0.7681159420289855,0,0,
57,The RNNSearch model proposed in can be seen as a variation of memory augmented networks due to its ability to read the historic output states of RNNs with soft attention .,Related Work,Related Work,question-answering,7,,,,25,0.625,56,0.20363636363636364,54,0.782608695652174,0,0,
58,The work of combines the soft attention with Memory Networks ( Mem NNs ) .,Related Work,Related Work,question-answering,7,,,,26,0.65,57,0.20727272727272728,55,0.7971014492753623,0,0,
59,"Similar to RNNSearch , MemNNs are designed with non-writable memories .",Related Work,Related Work,question-answering,7,,,,27,0.675,58,0.2109090909090909,56,0.8115942028985508,0,0,
60,It constructs layered memory representations and showed promising results on both artificial and real question answering tasks .,Related Work,Related Work,question-answering,7,,,,28,0.7,59,0.21454545454545454,57,0.8260869565217391,0,0,
61,We note that RNNSearch and MemNNs avoid the memory update and management overhead by simply using a non-writable memory storage .,Related Work,Related Work,question-answering,7,,,,29,0.725,60,0.21818181818181817,58,0.8405797101449275,0,0,
62,Another variation of MemNNs is Dynamic Memory Network that is equipped with an episodic memory and seems to be flexible in different settings .,Related Work,Related Work,question-answering,7,,,,30,0.75,61,0.22181818181818183,59,0.855072463768116,0,0,
63,"Although NSE differs from other memory - augumented NN models in many aspects , they all use soft attention mechanism with a type of similarity measures to retrieve relevant information from the external memory .",Related Work,Related Work,question-answering,7,,,,31,0.775,62,0.22545454545454546,60,0.8695652173913043,0,0,
64,"For example , NTM implements cosine similarity and MemNNs use vector dot product .",Related Work,Related Work,question-answering,7,,,,32,0.8,63,0.2290909090909091,61,0.8840579710144928,0,0,
65,NSE uses the vector dot product for the similarity measure in NSE because it is faster to compute .,Related Work,Related Work,question-answering,7,,,,33,0.825,64,0.23272727272727273,62,0.8985507246376812,0,0,
66,"Other related work includes Neural Program - Interpreters , which learns to run sub-programs and to compose them for high - level programs .",Related Work,Related Work,question-answering,7,,,,34,0.85,65,0.23636363636363636,63,0.9130434782608695,0,0,
67,It uses execution traces to provide the full supervision .,Related Work,Related Work,question-answering,7,,,,35,0.875,66,0.24,64,0.927536231884058,0,0,
68,Researchers have also explored ways to add unbounded memory to LSTM using a particular data structure .,Related Work,Related Work,question-answering,7,,,,36,0.9,67,0.24363636363636362,65,0.9420289855072463,0,0,
69,"Although this type of architecture provides a flexible capacity to store information , the memory access is constrained by the data structure used for the memory bank , such as stack and queue .",Related Work,Related Work,question-answering,7,,,,37,0.925,68,0.24727272727272728,66,0.9565217391304348,0,0,
70,Overall it is expensive to train and to scale the previously proposed memory - based models .,Related Work,Related Work,question-answering,7,,,,38,0.95,69,0.2509090909090909,67,0.9710144927536232,0,0,
71,Most models required a set of clever engineering tricks to work successfully .,Related Work,Related Work,question-answering,7,,,,39,0.975,70,0.2545454545454545,68,0.9855072463768116,0,0,
72,Most of the aforementioned memory augmented neural networks have been tested on synthetic tasks whereas in this paper we evaluated NSE on a wide range of real and large - scale natural language applications .,Related Work,Related Work,question-answering,7,,,,40,1.0,71,0.2581818181818182,69,1.0,0,0,
73,Proposed Approach,,,question-answering,7,,,,0,0.0,72,0.26181818181818184,0,0.0,1,0,
74,Our training set consists,,,question-answering,7,,,,0,0.0,73,0.26545454545454544,1,0.027777777777777776,1,0,
75,". . , w i Ti of tokens while the output Y i can be either a single target or a sequence .",Our training set consists,Our training set consists,question-answering,7,,,,1,0.020833333333333332,74,0.2690909090909091,2,0.05555555555555555,1,0,
76,We transform each input token wt to its word embedding x t .,Our training set consists,Our training set consists,question-answering,7,,,,2,0.041666666666666664,75,0.2727272727272727,3,0.08333333333333333,1,0,
77,"Our Neural Semantic Encoders ( NSE ) model has four main components : read , compose and write modules and an encoding memory M ?",Our training set consists,Our training set consists,question-answering,7,,,,3,0.0625,76,0.27636363636363637,4,0.1111111111111111,1,0,
78,"R kl with a variable number of slots , where k is the embedding dimension and l is the length of the input sequence .",Our training set consists,Our training set consists,question-answering,7,,,,4,0.08333333333333333,77,0.28,5,0.1388888888888889,1,0,
79,Each memory slot vector mt ?,Our training set consists,Our training set consists,question-answering,7,,,,5,0.10416666666666667,78,0.28363636363636363,6,0.16666666666666666,1,0,
80,R k corresponds to the vector representation of information about word wt in memory .,Our training set consists,Our training set consists,question-answering,7,,,,6,0.125,79,0.2872727272727273,7,0.19444444444444445,1,0,
81,"In particular , the memory is initialized by the embedding vectors {x t } l t=1 and is evolved overtime , through read , compose and write operations .",Our training set consists,Our training set consists,question-answering,7,,,,7,0.14583333333333334,80,0.2909090909090909,8,0.2222222222222222,1,0,
82,"Read , Compose and Write",Our training set consists,,question-answering,7,,,,8,0.16666666666666666,81,0.29454545454545455,9,0.25,1,0,
83,NSE performs three main operations in every time step .,Our training set consists,"Read , Compose and Write",question-answering,7,,,,9,0.1875,82,0.29818181818181816,10,0.2777777777777778,1,0,
84,"After initializing the memory slots with the corresponding input representations , NSE processes an embedding vector x t and retrieves a memory slot m r,t that is expected to be associatively coherent ( i.e. semantically associated ) with the current input word wt .",Our training set consists,"Read , Compose and Write",question-answering,7,,,,10,0.20833333333333334,83,0.3018181818181818,11,0.3055555555555556,1,0,
85,The slot location r ( ranging from 1 to l ) is defined by a key vector z t which the read module emits by attending over the memory slots .,Our training set consists,"Read , Compose and Write",question-answering,7,,,,11,0.22916666666666666,84,0.3054545454545455,12,0.3333333333333333,1,0,
86,The compose module implements a composition operation that combines the memory slot with the current input .,Our training set consists,"Read , Compose and Write",question-answering,7,,,,12,0.25,85,0.3090909090909091,13,0.3611111111111111,1,0,
87,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,Our training set consists,"Read , Compose and Write",question-answering,7,,,,13,0.2708333333333333,86,0.31272727272727274,14,0.3888888888888889,1,0,
88,"Instead of composing the raw embedding vector x t , we use the hidden state o t produced by the read module at time t",Our training set consists,"Read , Compose and Write",question-answering,7,,,,14,0.2916666666666667,87,0.31636363636363635,15,0.4166666666666667,1,0,
89,"where 1 is a matrix of ones , ?",Our training set consists,"Read , Compose and Write",question-answering,7,,,,15,0.3125,88,0.32,16,0.4444444444444444,1,0,
90,denotes the outer product which duplicates it s left vector l or k times to form a matrix .,Our training set consists,"Read , Compose and Write",question-answering,7,,,,16,0.3333333333333333,89,0.3236363636363636,17,0.4722222222222222,1,0,
91,The read function f LST,Our training set consists,,question-answering,7,,,,17,0.3541666666666667,90,0.32727272727272727,18,0.5,1,0,
92,Mr sequentially maps the word embeddings to the internal space of the memory M t?1 .,Our training set consists,The read function f LST,question-answering,7,,,,18,0.375,91,0.33090909090909093,19,0.5277777777777778,1,0,
93,Then Equation 2 looks for the slots related to the input by computing association degree between each memory slot and the hidden state o t .,Our training set consists,The read function f LST,question-answering,7,,,,19,0.3958333333333333,92,0.33454545454545453,20,0.5555555555555556,1,0,
94,We calculate the association degree by the dot product and transform this scores to the fuzzy key vector z t by normalizing with sof tmax function .,Our training set consists,The read function f LST,question-answering,7,,,,20,0.4166666666666667,93,0.3381818181818182,21,0.5833333333333334,1,0,
95,"Since our key vector is fuzzy , the slot to be composed is retrieved by taking weighted sum of the all slots as in Equation .",Our training set consists,The read function f LST,question-answering,7,,,,21,0.4375,94,0.3418181818181818,22,0.6111111111111112,1,0,
96,This process can also be seen as the soft attention mechanism .,Our training set consists,The read function f LST,question-answering,7,,,,22,0.4583333333333333,95,0.34545454545454546,23,0.6388888888888888,1,0,
97,"In Equation 4 and 5 , we compose and process the retrieved slot with the current hidden state and map the resulting vector to the encoder output space .",Our training set consists,The read function f LST,question-answering,7,,,,23,0.4791666666666667,96,0.3490909090909091,24,0.6666666666666666,1,0,
98,"Finally , we write the new representation to the memory location pointed by the key vector in where the key vector z t emitted by the read module is reused to inform the write module of the most recently read slots .",Our training set consists,The read function f LST,question-answering,7,,,,24,0.5,97,0.3527272727272727,25,0.6944444444444444,1,0,
99,First the slot information that was retrieved is erased and then the new representation is located .,Our training set consists,The read function f LST,question-answering,7,,,,25,0.5208333333333334,98,0.3563636363636364,26,0.7222222222222222,1,0,
100,NSE performs this iterative process until all words in the input sequence are read .,Our training set consists,The read function f LST,question-answering,7,,,,26,0.5416666666666666,99,0.36,27,0.75,1,0,
101,The encoding memories { M } T t=1 and output states {h} T t=1 are further used for the tasks .,Our training set consists,The read function f LST,question-answering,7,,,,27,0.5625,100,0.36363636363636365,28,0.7777777777777778,1,0,
102,"Although NSE reads a single word at a time , it has an anytime - access to the entire sequence stored in the encoding memory .",Our training set consists,The read function f LST,question-answering,7,,,,28,0.5833333333333334,101,0.36727272727272725,29,0.8055555555555556,1,0,
103,"With the encoding memory , NSE maintains a mental image of the input sequence .",Our training set consists,The read function f LST,question-answering,7,,,,29,0.6041666666666666,102,0.3709090909090909,30,0.8333333333333334,1,0,
104,The memory is initialized with the raw embedding vector at time t = 0 .,Our training set consists,The read function f LST,question-answering,7,,,,30,0.625,103,0.37454545454545457,31,0.8611111111111112,1,0,
105,We term such a freshly initialized memory a baby memory .,Our training set consists,The read function f LST,question-answering,7,,,,31,0.6458333333333334,104,0.3781818181818182,32,0.8888888888888888,1,0,
106,"As NSE reads more input content in time , the baby memory evolves and refines the encoded mental image .",Our training set consists,The read function f LST,question-answering,7,,,,32,0.6666666666666666,105,0.38181818181818183,33,0.9166666666666666,1,0,
107,functions are neural networks and are the training parameters in our NSE .,Our training set consists,The read function f LST,question-answering,7,,,,33,0.6875,106,0.38545454545454544,34,0.9444444444444444,1,0,
108,"As the name suggests , we use LSTM and multi -layer perceptron ( MLP ) in this paper .",Our training set consists,The read function f LST,question-answering,7,,,,34,0.7083333333333334,107,0.3890909090909091,35,0.9722222222222222,1,0,
109,"Since NSE is fully differentiable , it can be trained with any gradient descent optimizer .",Our training set consists,The read function f LST,question-answering,7,,,,35,0.7291666666666666,108,0.3927272727272727,36,1.0,1,0,
110,Shared and Multiple Memory Accesses,Our training set consists,The read function f LST,question-answering,7,,,,36,0.75,109,0.39636363636363636,0,0.0,1,0,
111,"For sequence to sequence transduction tasks like question answering , natural language inference and machine translation , it is beneficial to access other relevant memories in addition to its own one .",Our training set consists,The read function f LST,question-answering,7,,,,37,0.7708333333333334,110,0.4,1,0.08333333333333333,1,0,
112,The shared or the multiple memory access allows a set of NSEs to exchange knowledge representations and to communicate with each other to accomplish a particular task throughout the encoding memory .,Our training set consists,The read function f LST,question-answering,7,,,,38,0.7916666666666666,111,0.4036363636363636,2,0.16666666666666666,1,0,
113,"NSE can be extended easily , so that it is able to read from and write to multiple memories simultaneously or multiple NSEs are able to access a shared memory .",Our training set consists,The read function f LST,question-answering,7,,,,39,0.8125,112,0.4072727272727273,3,0.25,1,0,
114,( b ) depicts a high - level architectural diagram of a multiple memory access - NSE ( MMA - NSE ) .,Our training set consists,The read function f LST,question-answering,7,,,,40,0.8333333333333334,113,0.4109090909090909,4,0.3333333333333333,1,0,
115,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,Our training set consists,The read function f LST,question-answering,7,,,,41,0.8541666666666666,114,0.41454545454545455,5,0.4166666666666667,1,0,
116,Given a shared memory Mn ?,Our training set consists,The read function f LST,question-answering,7,,,,42,0.875,115,0.41818181818181815,6,0.5,1,0,
117,"R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",Our training set consists,The read function f LST,question-answering,7,,,,43,0.8958333333333334,116,0.4218181818181818,7,0.5833333333333334,1,0,
118,and this is almost the same as standard NSE .,Our training set consists,The read function f LST,question-answering,7,,,,44,0.9166666666666666,117,0.4254545454545455,8,0.6666666666666666,1,0,
119,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,Our training set consists,The read function f LST,question-answering,7,,,,45,0.9375,118,0.4290909090909091,9,0.75,1,0,
120,"In MMA - NSE , the different memory slots are retrieved from the shared memories depending on their encoded semantic representations .",Our training set consists,The read function f LST,question-answering,7,,,,46,0.9583333333333334,119,0.43272727272727274,10,0.8333333333333334,1,0,
121,They are then composed together with the current input and written back to their corresponding slots .,Our training set consists,The read function f LST,question-answering,7,,,,47,0.9791666666666666,120,0.43636363636363634,11,0.9166666666666666,1,0,
122,Note that MMA - NSE is capable of accessing a variable number of relevant shared memories once a composition function that takes in dynamic inputs is chosen .,Our training set consists,The read function f LST,question-answering,7,,,,48,1.0,121,0.44,12,1.0,1,0,
123,Experiments,,,question-answering,7,,,,0,0.0,122,0.44363636363636366,0,0.0,1,0,
124,"We describe in this section experiments on five different tasks , in order to show that NSE can be effective and flexible in different settings .",Experiments,Experiments,question-answering,7,,,,1,0.007142857142857143,123,0.44727272727272727,1,0.09090909090909091,1,0,
125,"We report results on natural language inference , question answering ( QA ) , sentence classification , document sentiment analysis and machine translation .",Experiments,Experiments,question-answering,7,,,,2,0.014285714285714285,124,0.4509090909090909,2,0.18181818181818182,1,0,
126,All five tasks challenge a model in terms of language understanding and semantic reasoning .,Experiments,Experiments,question-answering,7,,,,3,0.02142857142857143,125,0.45454545454545453,3,0.2727272727272727,1,0,
127,The models are trained using Adam with hyperparameters selected on development set .,Experiments,Experiments,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",4,0.02857142857142857,126,0.4581818181818182,4,0.36363636363636365,1,1,experimental-setup
128,We chose two one - layer LSTM for read / write modules on the tasks other than QA on which we used two - layer LSTM .,Experiments,Experiments,question-answering,7,"['O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.03571428571428571,127,0.4618181818181818,5,0.45454545454545453,1,1,experimental-setup
129,The pre-trained 300 - D Glove 840B vectors and 100 - D Glove 6B vectors were obtained for the word embeddings .,Experiments,Experiments,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'O']",6,0.04285714285714286,128,0.46545454545454545,6,0.5454545454545454,1,1,experimental-setup
130,The word embeddings are fixed during training .,Experiments,,question-answering,7,,,,7,0.05,129,0.4690909090909091,7,0.6363636363636364,1,0,
131,The embeddings for out - of - vocabulary words were set to zero vector .,Experiments,The word embeddings are fixed during training .,question-answering,7,,,,8,0.05714285714285714,130,0.4727272727272727,8,0.7272727272727273,1,0,
132,We crop or pad the input sequence to a fixed length .,Experiments,The word embeddings are fixed during training .,question-answering,7,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",9,0.06428571428571428,131,0.4763636363636364,9,0.8181818181818182,1,1,experimental-setup
133,A padding vector was inserted when padding .,Experiments,,question-answering,7,,,,10,0.07142857142857142,132,0.48,10,0.9090909090909091,1,0,
134,The models were regularized by using dropouts and an l 2 weight decay .,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.07857142857142857,133,0.48363636363636364,11,1.0,1,1,experimental-setup
135,Natural Language Inference,Experiments,A padding vector was inserted when padding .,question-answering,7,"['B', 'I', 'I']","['B-p', 'I-p', 'I-p']","['B-p', 'I-p', 'I-p']",12,0.08571428571428572,134,0.48727272727272725,0,0.0,1,1,tasks
136,The natural language inference is one of the main tasks in language understanding .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,13,0.09285714285714286,135,0.4909090909090909,1,0.038461538461538464,1,0,
137,This task tests the ability of a model to reason about the semantic relationship between two sentences .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,14,0.1,136,0.49454545454545457,2,0.07692307692307693,1,0,
138,"In order to perform well on the task , NSE should be able to capture sentence semantics and be able to reason the relation between a sentence pair , i.e. , whether a premise - hypothesis pair is entailing , contradictory or neutral .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,15,0.10714285714285714,137,0.49818181818181817,3,0.11538461538461539,1,0,
139,"We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset , which consists of 549,367/9,842/9,824 premise-hypothesis pairs for train / dev / test sets and target label indicating their relation .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,16,0.11428571428571428,138,0.5018181818181818,4,0.15384615384615385,1,0,
140,"Following the setting in the NSE output for each sentence was the input to a MLP , where the input layer computes the concatenation [ h pl ; h h l ] , absolute difference hp l ?",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,17,0.12142857142857143,139,0.5054545454545455,5,0.19230769230769232,1,0,
141,h h land elementwise product hp l h h l of the two sentence representations .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,18,0.12857142857142856,140,0.509090909090909,6,0.23076923076923078,1,0,
142,"In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer .",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'B-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.1357142857142857,141,0.5127272727272727,7,0.2692307692307692,1,1,tasks
143,"We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",20,0.14285714285714285,142,0.5163636363636364,8,0.3076923076923077,1,1,tasks
144,The write / read neural nets and the last linear layer were regularized by using 30 % dropouts .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,21,0.15,143,0.52,9,0.34615384615384615,1,0,
145,We evaluated three different variations of NSE show in .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,22,0.15714285714285714,144,0.5236363636363637,10,0.38461538461538464,1,0,
146,The NSE model encodes each sentence simultaneously by using a separate memory for each sentence .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,23,0.16428571428571428,145,0.5272727272727272,11,0.4230769230769231,1,0,
147,The second model - MMA - NSE first encodes the premise and then the hypothesis sentence by sharing the premise encoded memory in addition to the hypothesis memory .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,24,0.17142857142857143,146,0.5309090909090909,12,0.46153846153846156,1,0,
148,"For the third model , we use inter-sentence attention which selectively reconstructs the premise representation .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,25,0.17857142857142858,147,0.5345454545454545,13,0.5,1,0,
149,shows the results of our models along with the results of published methods for the task .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,26,0.18571428571428572,148,0.5381818181818182,14,0.5384615384615384,1,0,
150,The classifier with handcrafted features extracts a set of lexical features .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,27,0.19285714285714287,149,0.5418181818181819,15,0.5769230769230769,1,0,
151,The next group of models are based on sentence encoding .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,28,0.2,150,0.5454545454545454,16,0.6153846153846154,1,0,
152,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,29,0.20714285714285716,151,0.5490909090909091,17,0.6538461538461539,1,0,
153,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,30,0.21428571428571427,152,0.5527272727272727,18,0.6923076923076923,1,0,
154,"However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,31,0.22142857142857142,153,0.5563636363636364,19,0.7307692307692307,1,0,
155,NSE outperformed the previous sentence encoders on this task .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,32,0.22857142857142856,154,0.56,20,0.7692307692307693,1,0,
156,"The MMA - SNE further slightly improved the result , indicating that reading the premise memory is helpful while encoding the hypothesis .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,33,0.2357142857142857,155,0.5636363636363636,21,0.8076923076923077,1,0,
157,The last set of methods designs inter-sentence relation with parameterized soft attention .,Experiments,A padding vector was inserted when padding .,question-answering,7,,,,34,0.24285714285714285,156,0.5672727272727273,22,0.8461538461538461,1,0,
158,Our MMA - NSE attention model is similar to the LSTM attention model .,Experiments,A padding vector was inserted when padding .,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",35,0.25,157,0.5709090909090909,23,0.8846153846153846,1,1,tasks
159,"Particularly , it attends over the premise encoder outputs {h p } T t= 1 in respect to the final hypothesis representation h h land constructs an attentively blended vector of the premise .",Experiments,A padding vector was inserted when padding .,question-answering,7,,,,36,0.2571428571428571,158,0.5745454545454546,24,0.9230769230769231,1,0,
160,This model obtained 85.4 % accuracy score .,Experiments,,question-answering,7,"['O', 'O', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",37,0.2642857142857143,159,0.5781818181818181,25,0.9615384615384616,1,1,tasks
161,The best performing model for this task performs tree matching with attention mechanism and LSTM .,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,,,,38,0.2714285714285714,160,0.5818181818181818,26,1.0,1,0,
162,Answer Sentence Selection,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,"['B', 'I', 'I']","['B-p', 'I-p', 'I-p']","['B-p', 'I-p', 'I-p']",39,0.2785714285714286,161,0.5854545454545454,0,0.0,1,1,tasks
163,Answer sentence selection is an integral part of the open - domain question answering .,Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,,,,40,0.2857142857142857,162,0.5890909090909091,1,0.043478260869565216,1,0,
164,"For this task , a model is trained to identify the correct sentences that answer a factual question , from a set of candidate sentences .",Experiments,This model obtained 85.4 % accuracy score .,question-answering,7,,,,41,0.29285714285714287,163,0.5927272727272728,2,0.08695652173913043,1,0,
165,We experiment on WikiQA dataset constructed from Wikipedia .,Experiments,,question-answering,7,,,,42,0.3,164,0.5963636363636363,3,0.13043478260869565,1,0,
166,"The dataset contains 20,360/2,733/6,165 QA pairs for train / dev / test sets .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,43,0.30714285714285716,165,0.6,4,0.17391304347826086,1,0,
167,"The MLP setup used in the language inference task is kept same , except that we now replace the sof tmax layer with a sigmoid layer and model the following conditional probability distribution .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,44,0.3142857142857143,166,0.6036363636363636,5,0.21739130434782608,1,0,
168,where h q land ha l are the question and the answer encoded vectors and o QA denotes the output of the hidden layer of the MLP .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,45,0.32142857142857145,167,0.6072727272727273,6,0.2608695652173913,1,0,
169,We trained the MMA - NSE attention model to minimize the sigmoid cross entropy loss .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,46,0.32857142857142857,168,0.610909090909091,7,0.30434782608695654,1,0,
170,MMA - NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,47,0.3357142857142857,169,0.6145454545454545,8,0.34782608695652173,1,0,
171,"In our preliminary experiment , we found that the multiple memory access and the attention over answer encoder outputs {h a } T t= 1 are crucial to this problem .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,48,0.34285714285714286,170,0.6181818181818182,9,0.391304347826087,1,0,
172,"Following previous work , we adopt MAP and MRR as the evaluation metrics for this task .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,49,0.35,171,0.6218181818181818,10,0.43478260869565216,1,0,
173,"We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",50,0.35714285714285715,172,0.6254545454545455,11,0.4782608695652174,1,1,tasks
174,We used 40 % dropouts afterword embeddings and no l 2 weight decay .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",51,0.36428571428571427,173,0.6290909090909091,12,0.5217391304347826,1,1,tasks
175,The word embeddings are pre-trained 300 - D Glove 840B vectors .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",52,0.37142857142857144,174,0.6327272727272727,13,0.5652173913043478,1,1,tasks
176,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",53,0.37857142857142856,175,0.6363636363636364,14,0.6086956521739131,1,1,tasks
177,presents the results of our model and the previous models for the task .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,54,0.38571428571428573,176,0.64,15,0.6521739130434783,1,0,
178,The classifier with handcrafted features is a SVM model trained with a set of features .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,55,0.39285714285714285,177,0.6436363636363637,16,0.6956521739130435,1,0,
179,The Bigram - CNN model is a simple convolutional neural net .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,56,0.4,178,0.6472727272727272,17,0.7391304347826086,1,0,
180,"While the LSTM and LSTM attention models outperform the previous best result by nearly 5 - 6 % by implementing deep LSTM with three hidden layers , NASM improves it further and sets a strong baseline by combining variational auto - encoder with the soft attention .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,57,0.40714285714285714,179,0.6509090909090909,18,0.782608695652174,1,0,
181,Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'O', 'O', 'O']",,,58,0.4142857142857143,180,0.6545454545454545,19,0.8260869565217391,1,1,tasks
182,We used trec_eval script to calculate the evaluation metrics 7 Inclusion of simple word count feature improves the performance by around 0.15 - 0.3 across the board Model MAP MRR Classifier with features 0.5993 0.6068 Paragraph Vector 0.5110 0.5160,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,59,0.42142857142857143,181,0.6581818181818182,20,0.8695652173913043,1,0,
183,Bigram- CNN 0.6190 0.6281 3 - layer LSTM 0.6552 0.6747 3 - layer LSTM attention 0.6639 0.6828 NASM 0.6705 0.6914 MMA - NSE attention 0.6811 0.6993 88.1 47.4 DRNN 86.6 49.8 2 - layer LSTM 86.3 46.0 Bi-LSTM 87.5 49.1 CT- LSTM 88.0 51.0 DMN 88.6 52.1 NSE 89.7 52.8 : Test accuracy for sentence classification .,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,60,0.42857142857142855,182,0.6618181818181819,21,0.9130434782608695,1,0,
184,Bin :,Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,61,0.4357142857142857,183,0.6654545454545454,22,0.9565217391304348,1,0,
185,"Binary , FG : fine - grained 5 classes .",Experiments,We experiment on WikiQA dataset constructed from Wikipedia .,question-answering,7,,,,62,0.44285714285714284,184,0.6690909090909091,23,1.0,1,0,
186,Sentence Classification,Experiments,,question-answering,7,"['B', 'I']","['B-p', 'I-p']","['B-p', 'I-p']",63,0.45,185,0.6727272727272727,0,0.0,1,1,tasks
187,We evaluated NSE on the Stanford Sentiment Treebank ( SST ) .,Experiments,Sentence Classification,question-answering,7,,,,64,0.45714285714285713,186,0.6763636363636364,1,0.07692307692307693,1,0,
188,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,Experiments,Sentence Classification,question-answering,7,,,,65,0.4642857142857143,187,0.68,2,0.15384615384615385,1,0,
189,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,Experiments,Sentence Classification,question-answering,7,,,,66,0.4714285714285714,188,0.6836363636363636,3,0.23076923076923078,1,0,
190,The sentence representations were passed to a two - layer MLP for classification .,Experiments,Sentence Classification,question-answering,7,,,,67,0.4785714285714286,189,0.6872727272727273,4,0.3076923076923077,1,0,
191,The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting .,Experiments,Sentence Classification,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",68,0.4857142857142857,190,0.6909090909090909,5,0.38461538461538464,1,1,tasks
192,The second layer is a sof tmax layer .,Experiments,,question-answering,7,"['O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",69,0.4928571428571429,191,0.6945454545454546,6,0.46153846153846156,1,1,tasks
193,The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors .,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",70,0.5,192,0.6981818181818182,7,0.5384615384615384,1,1,tasks
194,"We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs .",Experiments,The second layer is a sof tmax layer .,question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']",,,71,0.5071428571428571,193,0.7018181818181818,8,0.6153846153846154,1,1,tasks
195,The write / read neural nets and the last linear layer were regularized by 50 % dropouts .,Experiments,The second layer is a sof tmax layer .,question-answering,7,,,,72,0.5142857142857142,194,0.7054545454545454,9,0.6923076923076923,1,0,
196,compares the result of our model with the state - of - the - art methods on the two subtasks .,Experiments,The second layer is a sof tmax layer .,question-answering,7,,,,73,0.5214285714285715,195,0.7090909090909091,10,0.7692307692307693,1,0,
197,Most best performing methods exploited the parse tree provided in the treebank on this task with the exception of the DMN .,Experiments,The second layer is a sof tmax layer .,question-answering,7,,,,74,0.5285714285714286,196,0.7127272727272728,11,0.8461538461538461,1,0,
198,The Dynamic Memory Network ( DMN ) model is a memory - augmented network .,Experiments,The second layer is a sof tmax layer .,question-answering,7,,,,75,0.5357142857142857,197,0.7163636363636363,12,0.9230769230769231,1,0,
199,Our model outperformed the DMN and set the state - of - the - art results on both subtasks .,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['B', 'I', 'B', 'O', 'B', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'O']","['B-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",76,0.5428571428571428,198,0.72,13,1.0,1,1,tasks
200,Document Sentiment Analysis,Experiments,The second layer is a sof tmax layer .,question-answering,7,"['B', 'I', 'I']","['B-p', 'I-p', 'I-p']","['B-p', 'I-p', 'I-p']",77,0.55,199,0.7236363636363636,0,0.0,1,1,tasks
201,"We evaluated our models for document - level sentiment analysis on two publically available largescale datasets : the IMDB consisting of 335,018 movie reviews and 10 different classes and Yelp 13 consisting of 348,415 restaurant reviews and 5 different classes .",Experiments,The second layer is a sof tmax layer .,question-answering,7,,,,78,0.5571428571428572,200,0.7272727272727273,1,0.05,1,0,
202,Each document in the datasets is associated with human ratings and we used these ratings as gold labels for sentiment classification .,Experiments,The second layer is a sof tmax layer .,question-answering,7,,,,79,0.5642857142857143,201,0.730909090909091,2,0.1,1,0,
203,"Particularly , we used the pre-split datasets of .",Experiments,,question-answering,7,,,,80,0.5714285714285714,202,0.7345454545454545,3,0.15,1,0,
204,We stack a NSE or LSTM on the top of another NSE for document modeling .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",81,0.5785714285714286,203,0.7381818181818182,4,0.2,1,1,tasks
205,The first NSE encodes the sentences and the second NSE or LSTM takes sentence encoded outputs and constructs document representations .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,,,,82,0.5857142857142857,204,0.7418181818181818,5,0.25,1,0,
206,The document representation is given to a output sof tmax layer .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,,,,83,0.5928571428571429,205,0.7454545454545455,6,0.3,1,0,
207,The whole network is trained jointly by backpropagating the cross entropy loss .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",84,0.6,206,0.7490909090909091,7,0.35,1,1,tasks
208,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",85,0.6071428571428571,207,0.7527272727272727,8,0.4,1,1,tasks
209,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']",,,86,0.6142857142857143,208,0.7563636363636363,9,0.45,1,1,tasks
210,The write / read neural nets and the document - level NSE / LSTM were regularized by 15 % dropouts and the softmax layer by 20 % dropouts .,Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,,,,87,0.6214285714285714,209,0.76,10,0.5,1,0,
211,"In order to speedup the training , we created document buckets by considering the number of sentences per document , i.e. , documents with the same number of sentences were put together in the same bucket .",Experiments,"Particularly , we used the pre-split datasets of .",question-answering,7,,,,88,0.6285714285714286,210,0.7636363636363637,11,0.55,1,0,
212,The buckets were shuffled and updated per epoch .,Experiments,,question-answering,7,,,,89,0.6357142857142857,211,0.7672727272727272,12,0.6,1,0,
213,"We did not use curriculum scheduling , although it is observed to help sequence training .",Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,,,,90,0.6428571428571429,212,0.7709090909090909,13,0.65,1,0,
214,shows our results .,Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,,,,91,0.65,213,0.7745454545454545,14,0.7,1,0,
215,We report two performance metrics : accuracy and MSE .,Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,,,,92,0.6571428571428571,214,0.7781818181818182,15,0.75,1,0,
216,"The best results on the task were previously obtained by Conv - GRNN and LSTM - GRNN , which are also stacked models .",Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,,,,93,0.6642857142857143,215,0.7818181818181819,16,0.8,1,0,
217,These models first learn the sentence representations with a CNN or LSTM and then combine them for document representation using a gated recurrent neural network ( GRNN : BLEU scores for English - German translation task .,Experiments,The buckets were shuffled and updated per epoch .,question-answering,7,,,,94,0.6714285714285714,216,0.7854545454545454,17,0.85,1,0,
218,Yelp 13 dataset has five classes to distinguish .,Experiments,,question-answering,7,,,,95,0.6785714285714286,217,0.7890909090909091,18,0.9,1,0,
219,The stacked NSEs ( NSE - NSE ) performed slightly better than the NSE - LSTM on the IMDB dataset .,Experiments,Yelp 13 dataset has five classes to distinguish .,question-answering,7,,,,96,0.6857142857142857,218,0.7927272727272727,19,0.95,1,0,
220,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,Experiments,Yelp 13 dataset has five classes to distinguish .,question-answering,7,,,,97,0.6928571428571428,219,0.7963636363636364,20,1.0,1,0,
221,Machine Translation,Experiments,,question-answering,7,"['B', 'I']","['B-p', 'I-p']","['B-p', 'I-p']",98,0.7,220,0.8,0,0.0,1,1,tasks
222,"Lastly , we conducted an experiment on neural machine translation ( NMT ) .",Experiments,Machine Translation,question-answering,7,,,,99,0.7071428571428572,221,0.8036363636363636,1,0.043478260869565216,1,0,
223,The NMT problem is mostly defined within the encoder - decoder framework .,Experiments,Machine Translation,question-answering,7,,,,100,0.7142857142857143,222,0.8072727272727273,2,0.08695652173913043,1,0,
224,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation .,Experiments,Machine Translation,question-answering,7,,,,101,0.7214285714285714,223,0.8109090909090909,3,0.13043478260869565,1,0,
225,"For an efficient encoding , the attention - based NTM was introduced .",Experiments,Machine Translation,question-answering,7,,,,102,0.7285714285714285,224,0.8145454545454546,4,0.17391304347826086,1,0,
226,"For NTM , we implemented three different models .",Experiments,,question-answering,7,,,,103,0.7357142857142858,225,0.8181818181818182,5,0.21739130434782608,1,0,
227,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,104,0.7428571428571429,226,0.8218181818181818,6,0.2608695652173913,1,0,
228,"This model ( LSTM - LSTM ) has two LSTM for the encoder / decoder and has the soft attention neural net , which attends over the source sentence and constructs a focused encoding vector for each target word .",Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,105,0.75,227,0.8254545454545454,7,0.30434782608695654,1,0,
229,The second model is an NSE - LSTM encoder - decoder which encodes the source sentence with NSE and generates the targets with the LSTM network by using the NSE output states and the attention network .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,106,0.7571428571428571,228,0.8290909090909091,8,0.34782608695652173,1,0,
230,"The last model is an NSE - NSE setup , where the encoding part is the same as the NSE - LSTM while the decoder NSE now uses the output state and has an access to the encoder memory , i.e. , the encoder and the decoder NSEs access a shared memory .",Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,107,0.7642857142857142,229,0.8327272727272728,9,0.391304347826087,1,0,
231,The memory is encoded by the first NSEs and then read / written by the decoder NSEs .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,108,0.7714285714285715,230,0.8363636363636363,10,0.43478260869565216,1,0,
232,We used the English - German translation corpus from the IWSLT 2014 evaluation campaign .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,109,0.7785714285714286,231,0.84,11,0.4782608695652174,1,0,
233,The corpus consists of sentence - aligned translation of TED talks .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,110,0.7857142857142857,232,0.8436363636363636,12,0.5217391304347826,1,0,
234,The data was pre-processed and lowercased with the Moses toolkit .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,111,0.7928571428571428,233,0.8472727272727273,13,0.5652173913043478,1,0,
235,"We merged the dev2010 and dev2012 sets for development and the tst2010 , tst2011 and tst 2012 sets for test data :",Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,112,0.8,234,0.850909090909091,14,0.6086956521739131,1,0,
236,Word association or composition graphs produced by NSE memory access .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,113,0.8071428571428572,235,0.8545454545454545,15,0.6521739130434783,1,0,
237,The directed arcs connect the words thatare composed via compose module .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,114,0.8142857142857143,236,0.8581818181818182,16,0.6956521739130435,1,0,
238,The source nodes are input words and the destination nodes ( pointed by the arrows ) correspond to the accessed memory slots .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,115,0.8214285714285714,237,0.8618181818181818,17,0.7391304347826086,1,0,
239,< S > denotes the beginning of sequence .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,116,0.8285714285714286,238,0.8654545454545455,18,0.782608695652174,1,0,
240,the number of parameters of the models is roughly the equal .,Experiments,"For NTM , we implemented three different models .",question-answering,7,,,,117,0.8357142857142857,239,0.8690909090909091,19,0.8260869565217391,1,0,
241,The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts .,Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",118,0.8428571428571429,240,0.8727272727272727,20,0.8695652173913043,1,1,tasks
242,"We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs .",Experiments,"For NTM , we implemented three different models .",question-answering,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O']",,,119,0.85,241,0.8763636363636363,21,0.9130434782608695,1,1,tasks
243,We report BLEU score for each models .,Experiments,,question-answering,7,,,,120,0.8571428571428571,242,0.88,22,0.9565217391304348,1,0,
244,11 5 Qualitative Analysis,Experiments,We report BLEU score for each models .,question-answering,7,,,,121,0.8642857142857143,243,0.8836363636363637,23,1.0,1,0,
245,Memory Access and Compositionality,Experiments,,question-answering,7,,,,122,0.8714285714285714,244,0.8872727272727273,0,0.0,1,0,
246,NSE is capabable of performing multiscale composition by retrieving associative slots for a particular input at a time step .,Experiments,Memory Access and Compositionality,question-answering,7,,,,123,0.8785714285714286,245,0.8909090909090909,1,0.05555555555555555,1,0,
247,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,Experiments,Memory Access and Compositionality,question-answering,7,,,,124,0.8857142857142857,246,0.8945454545454545,2,0.1111111111111111,1,0,
248,shows the word association graphs for the two sentence picked from SNLI test set .,Experiments,Memory Access and Compositionality,question-answering,7,,,,125,0.8928571428571429,247,0.8981818181818182,3,0.16666666666666666,1,0,
249,The association graph was constructed by inspecting the key vector z .,Experiments,Memory Access and Compositionality,question-answering,7,,,,126,0.9,248,0.9018181818181819,4,0.2222222222222222,1,0,
250,"For an input word , we connect it to the most active slot pointed by z 12 .",Experiments,Memory Access and Compositionality,question-answering,7,,,,127,0.9071428571428571,249,0.9054545454545454,5,0.2777777777777778,1,0,
251,"Note the graph components clustered around the semantically rich words : "" sits "" , "" wall "" and "" autumn "" ( a ) and "" Three "" , "" puppies "" , "" tub "" and "" vet "" ( b ) .",Experiments,Memory Access and Compositionality,question-answering,7,,,,128,0.9142857142857143,250,0.9090909090909091,6,0.3333333333333333,1,0,
252,The memory slots corresponding to words thatare semantically rich in the current context are the most frequently accessed .,Experiments,Memory Access and Compositionality,question-answering,7,,,,129,0.9214285714285714,251,0.9127272727272727,7,0.3888888888888889,1,0,
253,"The graph is able to capture certain syntactic structures including phrases ( e.g. , "" hand built rock wall "" ) and modifier relations ( between "" sits "" and "" quietly "" and between "" tub "" and "" sprayed with water "" ) .",Experiments,Memory Access and Compositionality,question-answering,7,,,,130,0.9285714285714286,252,0.9163636363636364,8,0.4444444444444444,1,0,
254,Another interesting property is that the model tends to perform sensible compositions while processing the input sentence .,Experiments,Memory Access and Compositionality,question-answering,7,,,,131,0.9357142857142857,253,0.92,9,0.5,1,0,
255,"For example , NSE retrieved the memory slot corresponding to "" wall "" or "" Three "" when reading the input "" rock "" or "" are "" .",Experiments,Memory Access and Compositionality,question-answering,7,,,,132,0.9428571428571428,254,0.9236363636363636,10,0.5555555555555556,1,0,
256,In Appendix,Experiments,,question-answering,7,,,,133,0.95,255,0.9272727272727272,11,0.6111111111111112,1,0,
257,"A , we show a step - by - step visualization of NSE memory states for the first sentence .",Experiments,In Appendix,question-answering,7,,,,134,0.9571428571428572,256,0.9309090909090909,12,0.6666666666666666,1,0,
258,Note how the encoding memory is evolved overtime .,Experiments,,question-answering,7,,,,135,0.9642857142857143,257,0.9345454545454546,13,0.7222222222222222,1,0,
259,"In time step four ( t = 4 ) , the memory slot for "" quietly "" encodes information about "" quiet ( ly ) little child "" .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,,,,136,0.9714285714285714,258,0.9381818181818182,14,0.7777777777777778,1,0,
260,"When t = 6 , the model forms another composition involving "" quietly "" , "" quietly sits "" .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,,,,137,0.9785714285714285,259,0.9418181818181818,15,0.8333333333333334,1,0,
261,"In the last time step , we are able to find the most or the least frequently accessed slots in the memory .",Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,,,,138,0.9857142857142858,260,0.9454545454545454,16,0.8888888888888888,1,0,
262,The least accessed slots correspond to function words while the frequently accessed slots are content words and tend to carry out rich semantics and intrinsic compositions found in the input sentence .,Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,,,,139,0.9928571428571429,261,0.9490909090909091,17,0.9444444444444444,1,0,
263,Overall the model is less constrained and is able to compose multiword expressions .,Experiments,Note how the encoding memory is evolved overtime .,question-answering,7,,,,140,1.0,262,0.9527272727272728,18,1.0,1,0,
264,Conclusion,,,question-answering,7,,,,0,0.0,263,0.9563636363636364,0,0.0,1,0,
265,Our proposed memory augmented neural networks have achieved the state - of - the - art results when evaluated on five representative NLP tasks .,Conclusion,Conclusion,question-answering,7,,,,1,0.09090909090909091,264,0.96,1,0.09090909090909091,0,0,
266,"NSE is capable of building an efficient architecture of the single , shared and multiple memory accesses for a specific NLP task .",Conclusion,Conclusion,question-answering,7,,,,2,0.18181818181818182,265,0.9636363636363636,2,0.18181818181818182,0,0,
267,"For example , for the NLI task NSE accesses premise encoded memory when processing hypothesis .",Conclusion,Conclusion,question-answering,7,,,,3,0.2727272727272727,266,0.9672727272727273,3,0.2727272727272727,0,0,
268,"For the QA task , NSE accesses answer encoded memory when reading question for QA .",Conclusion,Conclusion,question-answering,7,,,,4,0.36363636363636365,267,0.9709090909090909,4,0.36363636363636365,0,0,
269,"In machine translation , NSE shares a single encoded memory between encoder and decoder .",Conclusion,Conclusion,question-answering,7,,,,5,0.45454545454545453,268,0.9745454545454545,5,0.45454545454545453,0,0,
270,Such flexibility in the architectural choice of the NSE memory access allows for the robust models for a better performance .,Conclusion,Conclusion,question-answering,7,,,,6,0.5454545454545454,269,0.9781818181818182,6,0.5454545454545454,0,0,
271,The initial state of the NSE memory stores information about each word in the input sequence .,Conclusion,Conclusion,question-answering,7,,,,7,0.6363636363636364,270,0.9818181818181818,7,0.6363636363636364,0,0,
272,We in this paper used word embeddings to represent the words in the memory .,Conclusion,Conclusion,question-answering,7,,,,8,0.7272727272727273,271,0.9854545454545455,8,0.7272727272727273,0,0,
273,Different variations of word representations such as character - based models are left to be evaluated for memory initialization in the future .,Conclusion,Conclusion,question-answering,7,,,,9,0.8181818181818182,272,0.9890909090909091,9,0.8181818181818182,0,0,
274,We plan to extend NSE so that it learns to select and access a relevant subset from a memory set .,Conclusion,Conclusion,question-answering,7,,,,10,0.9090909090909091,273,0.9927272727272727,10,0.9090909090909091,0,0,
275,"One could also explore unsupervised variations of NSE , for example , to train them to produce encoding memory and representation vector of entire sentences or documents using either new or existing models such as the skip - gram model .",Conclusion,Conclusion,question-answering,7,,,,11,1.0,274,0.9963636363636363,11,1.0,0,0,
1,title,,,relation-classification,9,,,,0,0.0,0,0.0,0,0.0,1,0,
2,SCIBERT : A Pretrained Language Model for Scientific Text,title,,relation-classification,9,"['O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O']",1,0.0,1,0.006802721088435374,1,0.0,1,1,research-problem
3,abstract,,,relation-classification,9,,,,0,0.0,2,0.013605442176870748,0,0.0,1,0,
4,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,abstract,relation-classification,9,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O']",1,0.16666666666666666,3,0.02040816326530612,1,0.16666666666666666,1,1,research-problem
5,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",abstract,abstract,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.3333333333333333,4,0.027210884353741496,2,0.3333333333333333,1,1,research-problem
6,SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks .,abstract,abstract,relation-classification,9,,,,3,0.5,5,0.034013605442176874,3,0.5,1,0,
7,"We evaluate on a suite of tasks including sequence tagging , sentence classification and dependency parsing , with datasets from a variety of scientific domains .",abstract,abstract,relation-classification,9,,,,4,0.6666666666666666,6,0.04081632653061224,4,0.6666666666666666,1,0,
8,We demonstrate statistically significant improvements over BERT and achieve new state - of - theart results on several of these tasks .,abstract,abstract,relation-classification,9,,,,5,0.8333333333333334,7,0.047619047619047616,5,0.8333333333333334,1,0,
9,The code and pretrained models are available at https://github.com/allenai/scibert/.,abstract,abstract,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,1.0,8,0.05442176870748299,6,1.0,1,1,code
10,Introduction,,,relation-classification,9,,,,0,0.0,9,0.061224489795918366,0,0.0,1,0,
11,The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these documents .,Introduction,Introduction,relation-classification,9,,,,1,0.08333333333333333,10,0.06802721088435375,1,0.08333333333333333,1,0,
12,"Recent progress in NLP has been driven by the adoption of deep neural models , but training such models often requires large amounts of labeled data .",Introduction,Introduction,relation-classification,9,,,,2,0.16666666666666666,11,0.07482993197278912,2,0.16666666666666666,1,0,
13,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",Introduction,Introduction,relation-classification,9,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.25,12,0.08163265306122448,3,0.25,1,1,research-problem
14,"As shown through ELMo , and BERT , unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks .",Introduction,Introduction,relation-classification,9,,,,4,0.3333333333333333,13,0.08843537414965986,4,0.3333333333333333,1,0,
15,These models return contextualized embeddings for each token which can be passed into minimal task - specific neural architectures .,Introduction,Introduction,relation-classification,9,,,,5,0.4166666666666667,14,0.09523809523809523,5,0.4166666666666667,1,0,
16,"Leveraging the success of unsupervised pretraining has become especially important especially when task - specific annotations are difficult to obtain , like in scientific NLP .",Introduction,Introduction,relation-classification,9,,,,6,0.5,15,0.10204081632653061,6,0.5,1,0,
17,"Yet while both BERT and ELMo have released pretrained models , they are still trained on general domain corpora such as news articles and Wikipedia .",Introduction,Introduction,relation-classification,9,,,,7,0.5833333333333334,16,0.10884353741496598,7,0.5833333333333334,1,0,
18,"In this work , we make the following contributions :",Introduction,Introduction,relation-classification,9,,,,8,0.6666666666666666,17,0.11564625850340136,8,0.6666666666666666,1,0,
19,"( i ) We release SCIBERT , a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain .",Introduction,Introduction,relation-classification,9,,,,9,0.75,18,0.12244897959183673,9,0.75,1,0,
20,SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text .,Introduction,Introduction,relation-classification,9,,,,10,0.8333333333333334,19,0.1292517006802721,10,0.8333333333333334,1,0,
21,"( ii ) We perform extensive experimentation to investigate the performance of finetuning versus task - specific architectures atop frozen embeddings , and the effect of having an in - domain vocabulary .",Introduction,Introduction,relation-classification,9,,,,11,0.9166666666666666,20,0.1360544217687075,11,0.9166666666666666,1,0,
22,"( iii ) We evaluate SCIBERT on a suite of tasks in the scientific domain , and achieve new state - of the - art ( SOTA ) results on many of these tasks .",Introduction,Introduction,relation-classification,9,,,,12,1.0,21,0.14285714285714285,12,1.0,1,0,
23,Methods,,,relation-classification,9,,,,0,0.0,22,0.14965986394557823,0,0.0,1,0,
24,Background,,,relation-classification,9,,,,0,0.0,23,0.1564625850340136,1,0.1,1,0,
25,The BERT model architecture is based on a multilayer bidirectional Transformer .,Background,Background,relation-classification,9,,,,1,0.0125,24,0.16326530612244897,2,0.2,0,0,
26,"Instead of the traditional left - to - right language modeling objective , BERT is trained on two tasks : predicting randomly masked tokens and predicting whether two sentences follow each other .",Background,Background,relation-classification,9,,,,2,0.025,25,0.17006802721088435,3,0.3,0,0,
27,SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientific text .,Background,Background,relation-classification,9,"['B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",3,0.0375,26,0.17687074829931973,4,0.4,0,1,approach
28,Vocabulary BERT uses WordPiece for unsupervised tokenization of the input text .,Background,Background,relation-classification,9,,,,4,0.05,27,0.1836734693877551,5,0.5,0,0,
29,The vocabulary is built such that it contains the most frequently used words or subword units .,Background,Background,relation-classification,9,,,,5,0.0625,28,0.19047619047619047,6,0.6,0,0,
30,We refer to the original vocabulary released with BERT as BASEVOCAB .,Background,Background,relation-classification,9,,,,6,0.075,29,0.19727891156462585,7,0.7,0,0,
31,"We construct SCIVOCAB , a new WordPiece vocabulary on our scientific corpus using the Sen - tencePiece 1 library .",Background,Background,relation-classification,9,"['O', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.0875,30,0.20408163265306123,8,0.8,0,1,approach
32,We produce both cased and uncased vocabularies and set the vocabulary size to 30 K to match the size of BASEVOCAB .,Background,Background,relation-classification,9,,,,8,0.1,31,0.2108843537414966,9,0.9,0,0,
33,"The resulting token overlap between BASEVOCAB and SCIVOCAB is 42 % , illustrating a substantial difference in frequently used words between scientific and general domain texts .",Background,Background,relation-classification,9,,,,9,0.1125,32,0.21768707482993196,10,1.0,0,0,
34,Corpus,Background,,relation-classification,9,['B'],['B-n'],['B-b'],10,0.125,33,0.22448979591836735,0,0.0,0,1,approach
35,We train SCIBERT on a random sample of 1.14 M papers from Semantic Scholar .,Background,Corpus,relation-classification,9,"['O', 'B', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",11,0.1375,34,0.23129251700680273,1,0.16666666666666666,0,1,approach
36,This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain .,Background,Corpus,relation-classification,9,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']",,,12,0.15,35,0.23809523809523808,2,0.3333333333333333,0,1,approach
37,"We use the full text of the papers , not just the abstracts .",Background,Corpus,relation-classification,9,,,,13,0.1625,36,0.24489795918367346,3,0.5,0,0,
38,"The average paper length is 154 sentences ( 2,769 tokens ) resulting in a corpus size of 3.17B tokens , similar to the 3.3B tokens on which BERT was trained .",Background,Corpus,relation-classification,9,,,,14,0.175,37,0.25170068027210885,4,0.6666666666666666,0,0,
39,"We split sentences using Scispa Cy , 2 which is optimized for scientific text .",Background,Corpus,relation-classification,9,,,,15,0.1875,38,0.2585034013605442,5,0.8333333333333334,0,0,
40,3 Experimental Setup,Background,Corpus,relation-classification,9,,,,16,0.2,39,0.2653061224489796,6,1.0,0,0,
41,Tasks,Background,,relation-classification,9,,,,17,0.2125,40,0.272108843537415,0,0.0,0,0,
42,We experiment on the following core NLP tasks :,Background,Tasks,relation-classification,9,,,,18,0.225,41,0.2789115646258503,1,0.1111111111111111,0,0,
43,1 .,Background,Tasks,relation-classification,9,,,,19,0.2375,42,0.2857142857142857,2,0.2222222222222222,0,0,
44,Named Entity Recognition ( NER ),Background,Tasks,relation-classification,9,"['B', 'I', 'I', 'O', 'B', 'O']","['B-n', 'I-n', 'I-n', 'O', 'B-n', 'O']","['B-b', 'I-b', 'I-b', 'O', 'B-ob', 'O']",20,0.25,43,0.2925170068027211,3,0.3333333333333333,0,1,tasks
45,2 . PICO Extraction ( PICO ),Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O']",21,0.2625,44,0.29931972789115646,4,0.4444444444444444,0,1,tasks
46,3 . Text Classification ( CLS ),Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O']",22,0.275,45,0.30612244897959184,5,0.5555555555555556,0,1,tasks
47,4 . Relation Classification ( REL ),Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'B-n', 'O']","['O', 'O', 'B-b', 'I-b', 'O', 'B-ob', 'O']",23,0.2875,46,0.3129251700680272,6,0.6666666666666666,0,1,tasks
48,5 . Dependency Parsing ( DEP ) ,Background,Tasks,relation-classification,9,"['O', 'O', 'B', 'I', 'O', 'B', 'O', 'O']",,,24,0.3,47,0.3197278911564626,7,0.7777777777777778,0,1,
49,"PICO , like NER , is a sequence labeling task where the model extracts spans describing the Participants , Interventions , Comparisons , and Outcomes in a clinical trial paper .",Background,Tasks,relation-classification,9,,,,25,0.3125,48,0.32653061224489793,8,0.8888888888888888,0,0,
50,"REL is a special case of text classification where the model predicts the type of relation expressed between two entities , which are encapsulated in the sentence by inserted special tokens .",Background,Tasks,relation-classification,9,,,,26,0.325,49,0.3333333333333333,9,1.0,0,0,
51,Datasets,Background,,relation-classification,9,,,,27,0.3375,50,0.3401360544217687,0,0.0,0,0,
52,"For brevity , we only describe the newer datasets here , and refer the reader to the references in Table 1 for the older datasets .",Background,Datasets,relation-classification,9,,,,28,0.35,51,0.3469387755102041,1,0.08333333333333333,0,0,
53,EBM - NLP annotates PICO spans in clinical trial abstracts .,Background,Datasets,relation-classification,9,,,,29,0.3625,52,0.35374149659863946,2,0.16666666666666666,0,0,
54,SciERC annotates entities and relations from computer science ab - 1 https://github.com/google/sentencepiece,Background,Datasets,relation-classification,9,,,,30,0.375,53,0.36054421768707484,3,0.25,0,0,
55,"2 https://github.com/allenai/SciSpaCy stracts . ACL - ARC and Sci -Cite assign intent labels ( e.g. Comparison , Extension , etc. ) to sentences from scientific papers that cite other papers .",Background,Datasets,relation-classification,9,,,,31,0.3875,54,0.3673469387755102,4,0.3333333333333333,0,0,
56,The Paper Field dataset is built from the Microsoft Academic Graph 3 and maps paper titles to one of 7 fields of study .,Background,Datasets,relation-classification,9,,,,32,0.4,55,0.3741496598639456,5,0.4166666666666667,0,0,
57,"Each field of study ( i.e. geography , politics , economics , business , sociology , medicine , and psychology ) has approximately 12 K training examples .",Background,Datasets,relation-classification,9,,,,33,0.4125,56,0.38095238095238093,6,0.5,0,0,
58,Pretrained BERT,Background,,relation-classification,9,,,,34,0.425,57,0.3877551020408163,7,0.5833333333333334,0,0,
59,Variants,Background,,relation-classification,9,,,,35,0.4375,58,0.3945578231292517,8,0.6666666666666666,0,0,
60,BERT - Base,Background,Variants,relation-classification,9,,,,36,0.45,59,0.4013605442176871,9,0.75,0,0,
61,We use the pretrained weights for BERT - Base released with the original BERT code .,Background,Variants,relation-classification,9,,,,37,0.4625,60,0.40816326530612246,10,0.8333333333333334,0,0,
62,The vocabulary is BASE - VOCAB .,Background,Variants,relation-classification,9,,,,38,0.475,61,0.41496598639455784,11,0.9166666666666666,0,0,
63,We evaluate both cased and uncased versions of this model .,Background,Variants,relation-classification,9,,,,39,0.4875,62,0.4217687074829932,12,1.0,0,0,
64,SCIBERT,Background,,relation-classification,9,,,,40,0.5,63,0.42857142857142855,0,0.0,0,0,
65,We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT - Base .,Background,SCIBERT,relation-classification,9,,,,41,0.5125,64,0.43537414965986393,1,0.0625,0,0,
66,We train 4 different versions of SCIBERT : ( i ) cased or uncased and ( ii ) BASEVOCAB or SCIVOCAB .,Background,SCIBERT,relation-classification,9,,,,42,0.525,65,0.4421768707482993,2,0.125,0,0,
67,The two models that use BASEVOCAB are finetuned from the corresponding BERT - Base models .,Background,SCIBERT,relation-classification,9,,,,43,0.5375,66,0.4489795918367347,3,0.1875,0,0,
68,The other two models that use the new SCIVOCAB are trained from scratch .,Background,SCIBERT,relation-classification,9,,,,44,0.55,67,0.4557823129251701,4,0.25,0,0,
69,Pretraining BERT for long sentences can be slow .,Background,SCIBERT,relation-classification,9,,,,45,0.5625,68,0.46258503401360546,5,0.3125,0,0,
70,"Following the original BERT code , we set a maximum sentence length of 128 tokens , and train the model until the training loss stops decreasing .",Background,SCIBERT,relation-classification,9,,,,46,0.575,69,0.46938775510204084,6,0.375,0,0,
71,We then continue training the model allowing sentence lengths up to 512 tokens .,Background,SCIBERT,relation-classification,9,,,,47,0.5875,70,0.47619047619047616,7,0.4375,0,0,
72,We use a single TPU v 3 with 8 cores .,Background,SCIBERT,relation-classification,9,,,,48,0.6,71,0.48299319727891155,8,0.5,0,0,
73,"Training the SCIVOCAB models from scratch on our corpus takes 1 week 5 ( 5 days with max length 128 , then 2 days with max length 512 ) .",Background,SCIBERT,relation-classification,9,,,,49,0.6125,72,0.4897959183673469,9,0.5625,0,0,
74,The BASEVOCAB models take 2 fewer days of training because they are n't trained from scratch .,Background,SCIBERT,relation-classification,9,,,,50,0.625,73,0.4965986394557823,10,0.625,0,0,
75,All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library .,Background,SCIBERT,relation-classification,9,,,,51,0.6375,74,0.5034013605442177,11,0.6875,0,0,
76,All our models ( Sections 3.4 and 3.5 ) are implemented in PyTorch using AllenNLP .,Background,SCIBERT,relation-classification,9,,,,52,0.65,75,0.5102040816326531,12,0.75,0,0,
77,Casing,Background,,relation-classification,9,,,,53,0.6625,76,0.5170068027210885,13,0.8125,0,0,
78,We follow in using the cased models for NER and the uncased models for all other tasks .,Background,Casing,relation-classification,9,,,,54,0.675,77,0.5238095238095238,14,0.875,0,0,
79,We also use the cased models for parsing .,Background,Casing,relation-classification,9,,,,55,0.6875,78,0.5306122448979592,15,0.9375,0,0,
80,Some light experimentation showed that the uncased models perform slightly better ( even sometimes on NER ) than cased models .,Background,Casing,relation-classification,9,,,,56,0.7,79,0.5374149659863946,16,1.0,0,0,
81,Finetuning BERT,Background,,relation-classification,9,,,,57,0.7125,80,0.54421768707483,0,0.0,0,0,
82,"We mostly follow the same architecture , optimization , and hyperparameter choices used in .",Background,Finetuning BERT,relation-classification,9,,,,58,0.725,81,0.5510204081632653,1,0.043478260869565216,0,0,
83,"For text classification ( i.e. CLS and REL ) , we feed the final BERT vector for the [ CLS ] token into a linear classification layer .",Background,Finetuning BERT,relation-classification,9,,,,59,0.7375,82,0.5578231292517006,2,0.08695652173913043,0,0,
84,"For sequence labeling ( i.e. NER and PICO ) , we feed the final BERT vector for each token into a linear classification layer with softmax output .",Background,Finetuning BERT,relation-classification,9,,,,60,0.75,83,0.564625850340136,3,0.13043478260869565,0,0,
85,"We differ slightly in using an additional conditional random field , which made evaluation easier by guaranteeing well - formed entities .",Background,Finetuning BERT,relation-classification,9,,,,61,0.7625,84,0.5714285714285714,4,0.17391304347826086,0,0,
86,"For DEP , we use the model from with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs .",Background,Finetuning BERT,relation-classification,9,,,,62,0.775,85,0.5782312925170068,5,0.21739130434782608,0,0,
87,"In all settings , we apply a dropout of 0.1 and optimize cross entropy loss using Adam .",Background,Finetuning BERT,relation-classification,9,,,,63,0.7875,86,0.5850340136054422,6,0.2608695652173913,0,0,
88,"We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5 e - 6 , 1 e - 5 , 2 e - 5 , or 5 e - 5 with a slanted triangular schedule which is equivalent to the linear warmup followed by linear decay .",Background,Finetuning BERT,relation-classification,9,,,,64,0.8,87,0.5918367346938775,7,0.30434782608695654,0,0,
89,"For each dataset and BERT variant , we pick the best learning rate and number of epochs on the development set and report the corresponding test results .",Background,Finetuning BERT,relation-classification,9,,,,65,0.8125,88,0.5986394557823129,8,0.34782608695652173,0,0,
90,We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2 e - 5 .,Background,Finetuning BERT,relation-classification,9,,,,66,0.825,89,0.6054421768707483,9,0.391304347826087,0,0,
91,"While task - dependent , optimal hyperparameters for each task are often the same across BERT variants .",Background,Finetuning BERT,relation-classification,9,,,,67,0.8375,90,0.6122448979591837,10,0.43478260869565216,0,0,
92,Frozen BERT,Background,,relation-classification,9,,,,68,0.85,91,0.6190476190476191,11,0.4782608695652174,0,0,
93,Embeddings,Background,,relation-classification,9,,,,69,0.8625,92,0.6258503401360545,12,0.5217391304347826,0,0,
94,"We also explore the usage of BERT as pretrained contextualized word embeddings , like ELMo ) , by training simple task - specific models atop frozen BERT embeddings .",Background,Embeddings,relation-classification,9,,,,70,0.875,93,0.6326530612244898,13,0.5652173913043478,0,0,
95,"For text classification , we feed each sentence of BERT vectors into a 2 - layer BiLSTM of size 200 and apply a multilayer perceptron ( with hidden size 200 ) on the concatenated first and last BiLSTM vectors .",Background,Embeddings,relation-classification,9,,,,71,0.8875,94,0.6394557823129252,14,0.6086956521739131,0,0,
96,"For sequence labeling , we use the same BiLSTM layers and use a conditional random field to guarantee well - formed predictions .",Background,Embeddings,relation-classification,9,,,,72,0.9,95,0.6462585034013606,15,0.6521739130434783,0,0,
97,"For DEP , we use the full model from with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks .",Background,Embeddings,relation-classification,9,,,,73,0.9125,96,0.6530612244897959,16,0.6956521739130435,0,0,
98,We did not find changing the depth or size of the BiLSTMs to significantly impact results .,Background,Embeddings,relation-classification,9,,,,74,0.925,97,0.6598639455782312,17,0.7391304347826086,0,0,
99,"We optimize cross entropy loss using Adam , but holding BERT weights frozen and applying a dropout of 0.5 .",Background,Embeddings,relation-classification,9,,,,75,0.9375,98,0.6666666666666666,18,0.782608695652174,0,0,
100,We train with early stopping on the development set ( patience of 10 ) using a batch size of 32 and a learning rate of 0.001 .,Background,Embeddings,relation-classification,9,,,,76,0.95,99,0.673469387755102,19,0.8260869565217391,0,0,
101,"We did not perform extensive hyperparameter search , but while optimal hyperparameters are going to be task - dependent , some light experimentation showed these settings work fairly well across most tasks and BERT variants .",Background,Embeddings,relation-classification,9,,,,77,0.9625,100,0.6802721088435374,20,0.8695652173913043,0,0,
102,summarizes the experimental results .,Background,Embeddings,relation-classification,9,,,,78,0.975,101,0.6870748299319728,21,0.9130434782608695,0,0,
103,We observe that SCIBERT outperforms BERT - Base on scientific tasks ( + 2.11 F1 with finetuning and + 2.43 F1 without ),Background,Embeddings,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I']","['O', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['O', 'B-p', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob']",79,0.9875,102,0.6938775510204082,22,0.9565217391304348,0,1,results
104,8 . We also achieve new SOTA results on many of these tasks using SCIBERT .,Background,Embeddings,relation-classification,9,,,,80,1.0,103,0.7006802721088435,23,1.0,0,0,
105,Results,,,relation-classification,9,,,,0,0.0,104,0.7074829931972789,0,0.0,1,0,
106,Biomedical Domain,Results,,relation-classification,9,"['B', 'I']","['B-n', 'I-n']","['B-b', 'I-b']",1,0.03225806451612903,105,0.7142857142857143,0,0.0,1,1,results
107,We observe that SCIBERT outperforms BERT - Base on biomedical tasks ( + 1.92 F1 with finetuning and + 3.59 F1 without ) .,Results,Biomedical Domain,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O']",,,2,0.06451612903225806,106,0.7210884353741497,1,0.09090909090909091,1,1,results
108,"In addition , SCIB - ERT achieves new SOTA results on BC5 CDR and ChemProt , and EBM - NLP .",Results,Biomedical Domain,relation-classification,9,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",3,0.0967741935483871,107,0.7278911564625851,2,0.18181818181818182,1,1,results
109,SCIBERT performs slightly worse than SOTA on 3 datasets .,Results,Biomedical Domain,relation-classification,9,,,,4,0.12903225806451613,108,0.7346938775510204,3,0.2727272727272727,1,0,
110,The SOTA model for JNLPBA is a BiLSTM - CRF ensemble trained on multiple NER datasets not just JNLPBA .,Results,Biomedical Domain,relation-classification,9,,,,5,0.16129032258064516,109,0.7414965986394558,4,0.36363636363636365,1,0,
111,"The SOTA model for NCBI - disease is BIOBERT , which is BERT - Base finetuned on 18B tokens from biomedical papers .",Results,Biomedical Domain,relation-classification,9,,,,6,0.1935483870967742,110,0.7482993197278912,5,0.45454545454545453,1,0,
112,"The SOTA result for GENIA is in Nguyen and Verspoor ( 2019 ) which uses the model from with partof - speech ( POS ) features , which we do not use .",Results,Biomedical Domain,relation-classification,9,,,,7,0.22580645161290322,111,0.7551020408163265,6,0.5454545454545454,1,0,
113,"In , we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in .",Results,Biomedical Domain,relation-classification,9,,,,8,0.25806451612903225,112,0.7619047619047619,7,0.6363636363636364,1,0,
114,"Interesting , SCIBERT outperforms BIOBERT results on 7 The SOTA paper did not report a single score .",Results,Biomedical Domain,relation-classification,9,,,,9,0.2903225806451613,113,0.7687074829931972,8,0.7272727272727273,1,0,
115,We compute the average of the reported results for each class weighted by number of examples in each class .,Results,Biomedical Domain,relation-classification,9,,,,10,0.3225806451612903,114,0.7755102040816326,9,0.8181818181818182,1,0,
116,"8 Forrest of this paper , all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS .",Results,Biomedical Domain,relation-classification,9,,,,11,0.3548387096774194,115,0.782312925170068,10,0.9090909090909091,1,0,
117,"BC5 CDR and ChemProt , and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus .",Results,Biomedical Domain,relation-classification,9,,,,12,0.3870967741935484,116,0.7891156462585034,11,1.0,1,0,
118,Computer Science Domain,Results,,relation-classification,9,"['B', 'I', 'I']","['B-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b']",13,0.41935483870967744,117,0.7959183673469388,0,0.0,1,1,results
119,We observe that SCIBERT outperforms BERT - Base on computer science tasks ( + 3.55 F1 with finetuning and + 1.13 F1 without ) .,Results,Computer Science Domain,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O']",,,14,0.45161290322580644,118,0.8027210884353742,1,0.3333333333333333,1,1,results
120,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .",Results,Computer Science Domain,relation-classification,9,"['O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",15,0.4838709677419355,119,0.8095238095238095,2,0.6666666666666666,1,1,results
121,"For relations in Sci - ERC , our results are not comparable with those in because we are performing relation classification given gold entities , while they perform joint entity and relation extraction .",Results,Computer Science Domain,relation-classification,9,,,,16,0.5161290322580645,120,0.8163265306122449,3,1.0,1,0,
122,Multiple Domains,Results,,relation-classification,9,"['B', 'I']","['B-n', 'I-n']","['B-b', 'I-b']",17,0.5483870967741935,121,0.8231292517006803,0,0.0,1,1,results
123,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,Results,Multiple Domains,relation-classification,9,"['O', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'O']",,,18,0.5806451612903226,122,0.8299319727891157,1,0.3333333333333333,1,1,results
124,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .",Results,Multiple Domains,relation-classification,9,"['O', 'O', 'O', 'B', 'B', 'O', 'B', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'B-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'B-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",19,0.6129032258064516,123,0.8367346938775511,2,0.6666666666666666,1,1,results
125,No prior published SOTA results exist for the Paper Field dataset .,Results,Multiple Domains,relation-classification,9,,,,20,0.6451612903225806,124,0.8435374149659864,3,1.0,1,0,
126,Discussion,Results,,relation-classification,9,,,,21,0.6774193548387096,125,0.8503401360544217,0,0.0,1,0,
127,Effect of Finetuning,Results,,relation-classification,9,,,,22,0.7096774193548387,126,0.8571428571428571,0,0.0,1,0,
128,"We observe improved results via BERT finetuning rather than task - specific architectures atop frozen embeddings ( + 3.25 F1 with SCIBERT and + 3.58 with BERT - Base , on average ) .",Results,Effect of Finetuning,relation-classification,9,,,,23,0.7419354838709677,127,0.8639455782312925,1,0.3333333333333333,1,0,
129,"For each scientific domain , we observe the largest effects of finetuning on the computer science ( + 5.59 F1 with SCIB - ERT and + 3.17 F1 with BERT - Base ) and biomedical tasks ( + 2.94 F1 with SCIBERT and + 4.61 F1 with BERT - Base ) , and the smallest effect on multidomain tasks ( + 0.7 F1 with SCIBERT and + 1.14 F1 with BERT - Base ) .",Results,Effect of Finetuning,relation-classification,9,,,,24,0.7741935483870968,128,0.8707482993197279,2,0.6666666666666666,1,0,
130,"On every dataset except BC5 CDR and SciCite , BERT - Base with finetuning outperforms ( or performs similarly to ) a model using frozen SCIBERT embeddings .",Results,Effect of Finetuning,relation-classification,9,,,,25,0.8064516129032258,129,0.8775510204081632,3,1.0,1,0,
131,Effect of SCIVOCAB,Results,,relation-classification,9,,,,26,0.8387096774193549,130,0.8843537414965986,0,0.0,1,0,
132,We assess the importance of an in - domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB .,Results,Effect of SCIVOCAB,relation-classification,9,,,,27,0.8709677419354839,131,0.891156462585034,1,0.2,1,0,
133,We find the optimal hyperparameters for SCIBERT - BASEVOCAB often coincide with those of SCIB - ERT - SCIVOCAB .,Results,Effect of SCIVOCAB,relation-classification,9,,,,28,0.9032258064516129,132,0.8979591836734694,2,0.4,1,0,
134,"Averaged across datasets , we observe + 0.60 F1 when using SCIVOCAB .",Results,Effect of SCIVOCAB,relation-classification,9,,,,29,0.9354838709677419,133,0.9047619047619048,3,0.6,1,0,
135,"For each scientific do - main , we observe + 0.76 F1 for biomedical tasks , + 0.61 F1 for computer science tasks , and + 0.11 F1 for multidomain tasks .",Results,Effect of SCIVOCAB,relation-classification,9,,,,30,0.967741935483871,134,0.9115646258503401,4,0.8,1,0,
136,"Given the disjoint vocabularies ( Section 2 ) and the magnitude of improvement over BERT - Base ( Section 4 ) , we suspect that while an in - domain vocabulary is helpful , SCIBERT benefits most from the scientific corpus pretraining .",Results,Effect of SCIVOCAB,relation-classification,9,,,,31,1.0,135,0.9183673469387755,5,1.0,1,0,
137,Related Work,,,relation-classification,9,,,,0,0.0,136,0.9251700680272109,0,0.0,1,0,
138,Recent work on domain adaptation of BERT includes BIOBERT and CLIN - ICALBERT .,Related Work,Related Work,relation-classification,9,,,,1,0.25,137,0.9319727891156463,1,0.25,0,0,
139,"BIOBERT is trained on PubMed abstracts and PMC full text articles , and CLIN - ICALBERT is trained on clinical text from the MIMIC - III data base .",Related Work,Related Work,relation-classification,9,,,,2,0.5,138,0.9387755102040817,2,0.5,0,0,
140,"In contrast , SCIBERT is trained on the full text of 1.14 M biomedical and computer science papers from the Semantic Scholar corpus .",Related Work,Related Work,relation-classification,9,,,,3,0.75,139,0.9455782312925171,3,0.75,0,0,
141,"Furthermore , SCIBERT uses an in - domain vocabulary ( SCIVOCAB ) while the other abovementioned models use the original BERT vocabulary ( BASEVOCAB ) .",Related Work,Related Work,relation-classification,9,,,,4,1.0,140,0.9523809523809523,4,1.0,0,0,
142,Conclusion and Future Work,,,relation-classification,9,,,,0,0.0,141,0.9591836734693877,0,0.0,1,0,
143,"We released SCIBERT , a pretrained language model for scientific text based on BERT .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,,,,1,0.2,142,0.9659863945578231,1,0.2,0,0,
144,We evaluated SCIBERT on a suite of tasks and datasets from scientific domains .,Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,,,,2,0.4,143,0.9727891156462585,2,0.4,0,0,
145,"SCIBERT significantly outperformed BERT - Base and achieves new SOTA results on several of these tasks , even compared to some reported BIOBERT ) results on biomedical tasks .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,,,,3,0.6,144,0.9795918367346939,3,0.6,0,0,
146,"For future work , we will release a version of SCIBERT analogous to BERT - Large , as well as experiment with different proportions of papers from each domain .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,,,,4,0.8,145,0.9863945578231292,4,0.8,0,0,
147,"Because these language models are costly to train , we aim to build a single resource that 's useful across multiple domains .",Conclusion and Future Work,Conclusion and Future Work,relation-classification,9,,,,5,1.0,146,0.9931972789115646,5,1.0,0,0,
1,title,,,machine-translation,7,,,,0,0.0,0,0.0,0,0.0,1,0,
2,OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,title,title,machine-translation,7,,,,1,0.0,1,0.002680965147453083,1,0.0,1,0,
3,abstract,,,machine-translation,7,,,,0,0.0,2,0.005361930294906166,0,0.0,1,0,
4,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,abstract,machine-translation,7,,,,1,0.010869565217391304,3,0.00804289544235925,1,0.02040816326530612,1,0,
5,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract,abstract,machine-translation,7,"['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",2,0.021739130434782608,4,0.010723860589812333,2,0.04081632653061224,1,1,research-problem
6,"In practice , however , there are significant algorithmic and performance challenges .",abstract,abstract,machine-translation,7,,,,3,0.03260869565217391,5,0.013404825737265416,3,0.061224489795918366,1,0,
7,"In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters .",abstract,abstract,machine-translation,7,,,,4,0.043478260869565216,6,0.0160857908847185,4,0.08163265306122448,1,0,
8,"We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks .",abstract,abstract,machine-translation,7,,,,5,0.05434782608695652,7,0.01876675603217158,5,0.10204081632653061,1,0,
9,A trainable gating network determines a sparse combination of these experts to use for each example .,abstract,abstract,machine-translation,7,,,,6,0.06521739130434782,8,0.021447721179624665,6,0.12244897959183673,1,0,
10,"We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .",abstract,abstract,machine-translation,7,,,,7,0.07608695652173914,9,0.024128686327077747,7,0.14285714285714285,1,0,
11,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .,abstract,abstract,machine-translation,7,,,,8,0.08695652173913043,10,0.02680965147453083,8,0.16326530612244897,1,0,
12,"On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",abstract,abstract,machine-translation,7,,,,9,0.09782608695652174,11,0.029490616621983913,9,0.1836734693877551,1,0,
13,* Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency ),abstract,abstract,machine-translation,7,,,,10,0.10869565217391304,12,0.032171581769437,10,0.20408163265306123,1,0,
14,INTRODUCTION AND RELATED WORK 1 .,abstract,abstract,machine-translation,7,,,,11,0.11956521739130435,13,0.03485254691689008,11,0.22448979591836735,1,0,
15,CONDITIONAL COMPUTATION,abstract,abstract,machine-translation,7,,,,12,0.13043478260869565,14,0.03753351206434316,12,0.24489795918367346,1,0,
16,Exploiting scale in both training data and model size has been central to the success of deep learning .,abstract,abstract,machine-translation,7,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",13,0.14130434782608695,15,0.040214477211796246,13,0.2653061224489796,1,1,research-problem
17,"When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy .",abstract,abstract,machine-translation,7,,,,14,0.15217391304347827,16,0.04289544235924933,14,0.2857142857142857,1,0,
18,"This has been shown in domains such as text , images , and audio .",abstract,abstract,machine-translation,7,,,,15,0.16304347826086957,17,0.045576407506702415,15,0.30612244897959184,1,0,
19,"For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .",abstract,abstract,machine-translation,7,,,,16,0.17391304347826086,18,0.04825737265415549,16,0.32653061224489793,1,0,
20,"Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand .",abstract,abstract,machine-translation,7,,,,17,0.18478260869565216,19,0.05093833780160858,17,0.3469387755102041,1,0,
21,Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .,abstract,abstract,machine-translation,7,,,,18,0.1956521739130435,20,0.05361930294906166,18,0.3673469387755102,1,0,
22,"In these schemes , large parts of a network are active or inactive on a per-example basis .",abstract,abstract,machine-translation,7,,,,19,0.20652173913043478,21,0.05630026809651475,19,0.3877551020408163,1,0,
23,"The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .",abstract,abstract,machine-translation,7,,,,20,0.21739130434782608,22,0.058981233243967826,20,0.40816326530612246,1,0,
24,Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions .,abstract,abstract,machine-translation,7,,,,21,0.22826086956521738,23,0.06166219839142091,21,0.42857142857142855,1,0,
25,"While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .",abstract,abstract,machine-translation,7,,,,22,0.2391304347826087,24,0.064343163538874,22,0.4489795918367347,1,0,
26,We blame this on a combination of the following challenges :,abstract,abstract,machine-translation,7,,,,23,0.25,25,0.06702412868632708,23,0.46938775510204084,1,0,
27,"Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .",abstract,abstract,machine-translation,7,,,,24,0.2608695652173913,26,0.06970509383378017,24,0.4897959183673469,1,0,
28,Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .,abstract,abstract,machine-translation,7,,,,25,0.2717391304347826,27,0.07238605898123325,25,0.5102040816326531,1,0,
29,"Large batch sizes are critical for performance , as they amortize the costs of parameter transfers and updates .",abstract,abstract,machine-translation,7,,,,26,0.2826086956521739,28,0.07506702412868632,26,0.5306122448979592,1,0,
30,Conditional computation reduces the batch sizes for the conditionally active chunks of the network .,abstract,abstract,machine-translation,7,,,,27,0.29347826086956524,29,0.0777479892761394,27,0.5510204081632653,1,0,
31,Network bandwidth can be a bottleneck .,abstract,abstract,machine-translation,7,,,,28,0.30434782608695654,30,0.08042895442359249,28,0.5714285714285714,1,0,
32,A cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .,abstract,abstract,machine-translation,7,,,,29,0.31521739130434784,31,0.08310991957104558,29,0.5918367346938775,1,0,
33,"To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .",abstract,abstract,machine-translation,7,,,,30,0.32608695652173914,32,0.08579088471849866,30,0.6122448979591837,1,0,
34,"Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .",abstract,abstract,machine-translation,7,,,,31,0.33695652173913043,33,0.08847184986595175,31,0.6326530612244898,1,0,
35,"Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .",abstract,abstract,machine-translation,7,,,,32,0.34782608695652173,34,0.09115281501340483,32,0.6530612244897959,1,0,
36,"Depending on the scheme , loss terms maybe necessary to achieve the desired level of sparsity per-chunk and / or per example .",abstract,abstract,machine-translation,7,,,,33,0.358695652173913,35,0.0938337801608579,33,0.673469387755102,1,0,
37,use three such terms .,abstract,abstract,machine-translation,7,,,,34,0.3695652173913043,36,0.09651474530831099,34,0.6938775510204082,1,0,
38,These issues can affect both model quality and load - balancing .,abstract,abstract,machine-translation,7,,,,35,0.3804347826086957,37,0.09919571045576407,35,0.7142857142857143,1,0,
39,Model capacity is most critical for very large data sets .,abstract,abstract,machine-translation,7,,,,36,0.391304347826087,38,0.10187667560321716,36,0.7346938775510204,1,0,
40,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",abstract,abstract,machine-translation,7,,,,37,0.40217391304347827,39,0.10455764075067024,37,0.7551020408163265,1,0,
41,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",abstract,abstract,machine-translation,7,,,,38,0.41304347826086957,40,0.10723860589812333,38,0.7755102040816326,1,0,
42,"In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation .",abstract,abstract,machine-translation,7,,,,39,0.42391304347826086,41,0.10991957104557641,39,0.7959183673469388,1,0,
43,We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets .,abstract,abstract,machine-translation,7,,,,40,0.43478260869565216,42,0.1126005361930295,40,0.8163265306122449,1,0,
44,OUR APPROACH : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER,abstract,abstract,machine-translation,7,,,,41,0.44565217391304346,43,0.11528150134048257,41,0.8367346938775511,1,0,
45,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,abstract,abstract,machine-translation,7,"['O', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",42,0.45652173913043476,44,0.11796246648793565,42,0.8571428571428571,1,1,approach
46,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",abstract,abstract,machine-translation,7,"['O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",43,0.4673913043478261,45,0.12064343163538874,43,0.8775510204081632,1,1,approach
47,All parts of the network are trained jointly by back - propagation .,abstract,abstract,machine-translation,7,"['B', 'I', 'I', 'O', 'B', 'O', 'B', 'B', 'B', 'B', 'I', 'I', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-n', 'O', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O']",44,0.4782608695652174,46,0.12332439678284182,44,0.8979591836734694,1,1,approach
48,"While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models .",abstract,abstract,machine-translation,7,,,,45,0.4891304347826087,47,0.1260053619302949,45,0.9183673469387755,1,0,
49,"In particular , we apply a MoE convolutionally between stacked LSTM layers , as in .",abstract,abstract,machine-translation,7,,,,46,0.5,48,0.128686327077748,46,0.9387755102040817,1,0,
50,"The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position .",abstract,abstract,machine-translation,7,,,,47,0.5108695652173914,49,0.13136729222520108,47,0.9591836734693877,1,0,
51,The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .,abstract,abstract,machine-translation,7,,,,48,0.5217391304347826,50,0.13404825737265416,48,0.9795918367346939,1,0,
52,"On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost .",abstract,abstract,machine-translation,7,,,,49,0.532608695652174,51,0.13672922252010725,49,1.0,1,0,
53,RELATED WORK ON MIXTURES OF EXPERTS,abstract,abstract,machine-translation,7,,,,50,0.5434782608695652,52,0.13941018766756033,0,0.0,1,0,
54,"Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .",abstract,abstract,machine-translation,7,,,,51,0.5543478260869565,53,0.14209115281501342,1,0.03333333333333333,1,0,
55,"Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .",abstract,abstract,machine-translation,7,,,,52,0.5652173913043478,54,0.1447721179624665,2,0.06666666666666667,1,0,
56,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",abstract,abstract,machine-translation,7,,,,53,0.5760869565217391,55,0.14745308310991956,3,0.1,1,0,
57,suggest an ensemble model in the format of mixture of experts for machine translation .,abstract,abstract,machine-translation,7,,,,54,0.5869565217391305,56,0.15013404825737264,4,0.13333333333333333,1,0,
58,The gating network is trained on a pre-trained ensemble NMT model .,abstract,abstract,machine-translation,7,,,,55,0.5978260869565217,57,0.15281501340482573,5,0.16666666666666666,1,0,
59,The works above concern top - level mixtures of experts .,abstract,abstract,machine-translation,7,,,,56,0.6086956521739131,58,0.1554959785522788,6,0.2,1,0,
60,The mixture of experts is the whole model .,abstract,abstract,machine-translation,7,,,,57,0.6195652173913043,59,0.1581769436997319,7,0.23333333333333334,1,0,
61,introduce the idea of using multiple,abstract,abstract,machine-translation,7,,,,58,0.6304347826086957,60,0.16085790884718498,8,0.26666666666666666,1,0,
62,MoEs with their own gating networks as parts of a deep model .,abstract,abstract,machine-translation,7,,,,59,0.6413043478260869,61,0.16353887399463807,9,0.3,1,0,
63,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",abstract,abstract,machine-translation,7,,,,60,0.6521739130434783,62,0.16621983914209115,10,0.3333333333333333,1,0,
64,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",abstract,abstract,machine-translation,7,,,,61,0.6630434782608695,63,0.16890080428954424,11,0.36666666666666664,1,0,
65,Our work builds on this use of MoEs as a general purpose neural network component .,abstract,abstract,machine-translation,7,,,,62,0.6739130434782609,64,0.17158176943699732,12,0.4,1,0,
66,"While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",abstract,abstract,machine-translation,7,,,,63,0.6847826086956522,65,0.1742627345844504,13,0.43333333333333335,1,0,
67,We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity .,abstract,abstract,machine-translation,7,,,,64,0.6956521739130435,66,0.1769436997319035,14,0.4666666666666667,1,0,
68,THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER,abstract,abstract,machine-translation,7,,,,65,0.7065217391304348,67,0.17962466487935658,15,0.5,1,0,
69,"The Mixture - of - Experts ( MoE ) layer consists of a set of n "" expert networks "" E 1 , , E n , and a "" gating network "" G whose output is a sparse n-dimensional vector .",abstract,abstract,machine-translation,7,,,,66,0.717391304347826,68,0.18230563002680966,16,0.5333333333333333,1,0,
70,shows an overview of the MoE module .,abstract,abstract,machine-translation,7,,,,67,0.7282608695652174,69,0.18498659517426275,17,0.5666666666666667,1,0,
71,"The experts are themselves neural networks , each with their own parameters .",abstract,abstract,machine-translation,7,,,,68,0.7391304347826086,70,0.1876675603217158,18,0.6,1,0,
72,"Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .",abstract,abstract,machine-translation,7,,,,69,0.75,71,0.1903485254691689,19,0.6333333333333333,1,0,
73,Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network for a given input x .,abstract,abstract,machine-translation,7,,,,70,0.7608695652173914,72,0.19302949061662197,20,0.6666666666666666,1,0,
74,The output y of the MoE module can be written as follows :,abstract,abstract,machine-translation,7,,,,71,0.7717391304347826,73,0.19571045576407506,21,0.7,1,0,
75,We save computation based on the sparsity of the output of G ( x ) .,abstract,abstract,machine-translation,7,,,,72,0.782608695652174,74,0.19839142091152814,22,0.7333333333333333,1,0,
76,"Wherever G (x ) i = 0 , we need not compute E i ( x ) .",abstract,abstract,machine-translation,7,,,,73,0.7934782608695652,75,0.20107238605898123,23,0.7666666666666667,1,0,
77,"In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example .",abstract,abstract,machine-translation,7,,,,74,0.8043478260869565,76,0.2037533512064343,24,0.8,1,0,
78,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",abstract,abstract,machine-translation,7,,,,75,0.8152173913043478,77,0.2064343163538874,25,0.8333333333333334,1,0,
79,In the following we focus on ordinary MoEs .,abstract,abstract,machine-translation,7,,,,76,0.8260869565217391,78,0.20911528150134048,26,0.8666666666666667,1,0,
80,We provide more details on hierarchical MoEs in Appendix B.,abstract,abstract,machine-translation,7,,,,77,0.8369565217391305,79,0.21179624664879357,27,0.9,1,0,
81,Our implementation is related to other models of conditional computation .,abstract,abstract,machine-translation,7,,,,78,0.8478260869565217,80,0.21447721179624665,28,0.9333333333333333,1,0,
82,A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .,abstract,abstract,machine-translation,7,,,,79,0.8586956521739131,81,0.21715817694369974,29,0.9666666666666667,1,0,
83,"A MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .",abstract,abstract,machine-translation,7,,,,80,0.8695652173913043,82,0.21983914209115282,30,1.0,1,0,
84,GATING NETWORK,abstract,abstract,machine-translation,7,,,,81,0.8804347826086957,83,0.2225201072386059,0,0.0,1,0,
85,Softmax Gating :,abstract,abstract,machine-translation,7,,,,82,0.8913043478260869,84,0.225201072386059,1,0.05263157894736842,1,0,
86,A simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .,abstract,abstract,machine-translation,7,,,,83,0.9021739130434783,85,0.22788203753351208,2,0.10526315789473684,1,0,
87,Noisy Top - K,abstract,,machine-translation,7,,,,84,0.9130434782608695,86,0.23056300268096513,3,0.15789473684210525,1,0,
88,Gating :,abstract,Noisy Top - K,machine-translation,7,,,,85,0.9239130434782609,87,0.23324396782841822,4,0.21052631578947367,1,0,
89,We add two components to the Softmax gating network : sparsity and noise .,abstract,Noisy Top - K,machine-translation,7,,,,86,0.9347826086956522,88,0.2359249329758713,5,0.2631578947368421,1,0,
90,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ??",abstract,Noisy Top - K,machine-translation,7,,,,87,0.9456521739130435,89,0.2386058981233244,6,0.3157894736842105,1,0,
91,( which causes the corresponding gate values to equal 0 ) .,abstract,Noisy Top - K,machine-translation,7,,,,88,0.9565217391304348,90,0.24128686327077747,7,0.3684210526315789,1,0,
92,"The sparsity serves to save computation , as described above .",abstract,Noisy Top - K,machine-translation,7,,,,89,0.967391304347826,91,0.24396782841823056,8,0.42105263157894735,1,0,
93,"While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .",abstract,Noisy Top - K,machine-translation,7,,,,90,0.9782608695652174,92,0.24664879356568364,9,0.47368421052631576,1,0,
94,"The noise term helps with load balancing , as will be discussed in Appendix A .",abstract,Noisy Top - K,machine-translation,7,,,,91,0.9891304347826086,93,0.24932975871313673,10,0.5263157894736842,1,0,
95,The amount of noise per component is controlled by a second trainable weight matrix W noise .,abstract,Noisy Top - K,machine-translation,7,,,,92,1.0,94,0.2520107238605898,11,0.5789473684210527,1,0,
96,Training the Gating Network,,,machine-translation,7,,,,0,0.0,95,0.2546916890080429,12,0.631578947368421,1,0,
97,"We train the gating network by simple back - propagation , along with the rest of the model .",Training the Gating Network,Training the Gating Network,machine-translation,7,,,,1,0.008264462809917356,96,0.257372654155496,13,0.6842105263157895,1,0,
98,"If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .",Training the Gating Network,Training the Gating Network,machine-translation,7,,,,2,0.01652892561983471,97,0.26005361930294907,14,0.7368421052631579,1,0,
99,This type of occasionally - sensitive behavior is described in with respect to noisy rectifiers .,Training the Gating Network,Training the Gating Network,machine-translation,7,,,,3,0.024793388429752067,98,0.26273458445040215,15,0.7894736842105263,1,0,
100,Gradients also backpropagate through the gating network to its inputs .,Training the Gating Network,Training the Gating Network,machine-translation,7,,,,4,0.03305785123966942,99,0.26541554959785524,16,0.8421052631578947,1,0,
101,Our method differs here from who use boolean gates and a REINFORCE - style approach to train the gating network .,Training the Gating Network,Training the Gating Network,machine-translation,7,,,,5,0.04132231404958678,100,0.2680965147453083,17,0.8947368421052632,1,0,
102,ADDRESSING PERFORMANCE,Training the Gating Network,Training the Gating Network,machine-translation,7,,,,6,0.049586776859504134,101,0.2707774798927614,18,0.9473684210526315,1,0,
103,CHALLENGES,Training the Gating Network,,machine-translation,7,,,,7,0.05785123966942149,102,0.2734584450402145,19,1.0,1,0,
104,THE SHRINKING BATCH PROBLEM,Training the Gating Network,CHALLENGES,machine-translation,7,,,,8,0.06611570247933884,103,0.2761394101876676,0,0.0,1,0,
105,"On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,9,0.0743801652892562,104,0.27882037533512066,1,0.03225806451612903,1,0,
106,"If the gating network chooses k out of n experts for each example , then for a batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,10,0.08264462809917356,105,0.28150134048257375,2,0.06451612903225806,1,0,
107,This causes a naive MoE implementation to become very inefficient as the number of experts increases .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,11,0.09090909090909091,106,0.28418230563002683,3,0.0967741935483871,1,0,
108,The solution to this shrinking batch problem is to make the original batch size as large as possible .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,12,0.09917355371900827,107,0.2868632707774799,4,0.12903225806451613,1,0,
109,"However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,13,0.10743801652892562,108,0.289544235924933,5,0.16129032258064516,1,0,
110,We propose the following techniques for increasing the batch size :,Training the Gating Network,CHALLENGES,machine-translation,7,,,,14,0.11570247933884298,109,0.29222520107238603,6,0.1935483870967742,1,0,
111,Mixing Data Parallelism and Model Parallelism :,Training the Gating Network,CHALLENGES,machine-translation,7,,,,15,0.12396694214876033,110,0.2949061662198391,7,0.22580645161290322,1,0,
112,"In a conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,16,0.1322314049586777,111,0.2975871313672922,8,0.25806451612903225,1,0,
113,"In our technique , these different batches run synchronously so that they can be combined for the MoE layer .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,17,0.14049586776859505,112,0.3002680965147453,9,0.2903225806451613,1,0,
114,"We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,18,0.1487603305785124,113,0.30294906166219837,10,0.3225806451612903,1,0,
115,Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,19,0.15702479338842976,114,0.30563002680965146,11,0.3548387096774194,1,0,
116,The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,20,0.1652892561983471,115,0.30831099195710454,12,0.3870967741935484,1,0,
117,"If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,21,0.17355371900826447,116,0.3109919571045576,13,0.41935483870967744,1,0,
118,"Thus , we achieve a factor of d improvement inexpert batch size .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,22,0.18181818181818182,117,0.3136729222520107,14,0.45161290322580644,1,0,
119,"In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,23,0.19008264462809918,118,0.3163538873994638,15,0.4838709677419355,1,0,
120,Each secondary MoE resides on one device .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,24,0.19834710743801653,119,0.3190348525469169,16,0.5161290322580645,1,0,
121,This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,25,0.2066115702479339,120,0.32171581769436997,17,0.5483870967741935,1,0,
122,"The total batch size increases , keeping the batch size per expert constant .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,26,0.21487603305785125,121,0.32439678284182305,18,0.5806451612903226,1,0,
123,"The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,27,0.2231404958677686,122,0.32707774798927614,19,0.6129032258064516,1,0,
124,It is our goal to train a trillionparameter model on a trillion - word corpus .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,28,0.23140495867768596,123,0.3297587131367292,20,0.6451612903225806,1,0,
125,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,29,0.2396694214876033,124,0.3324396782841823,21,0.6774193548387096,1,0,
126,Taking Advantage of Convolutionality :,Training the Gating Network,CHALLENGES,machine-translation,7,,,,30,0.24793388429752067,125,0.3351206434316354,22,0.7096774193548387,1,0,
127,"In our language models , we apply the same MoE to each time step of the previous layer .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,31,0.256198347107438,126,0.3378016085790885,23,0.7419354838709677,1,0,
128,"If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch .",Training the Gating Network,CHALLENGES,machine-translation,7,,,,32,0.2644628099173554,127,0.34048257372654156,24,0.7741935483870968,1,0,
129,Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps .,Training the Gating Network,CHALLENGES,machine-translation,7,,,,33,0.2727272727272727,128,0.34316353887399464,25,0.8064516129032258,1,0,
130,Increasing Batch Size for a,Training the Gating Network,,machine-translation,7,,,,34,0.2809917355371901,129,0.34584450402144773,26,0.8387096774193549,1,0,
131,Recurrent MoE :,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,35,0.2892561983471074,130,0.3485254691689008,27,0.8709677419354839,1,0,
132,We suspect that even more powerful models may involve applying a MoE recurrently .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,36,0.2975206611570248,131,0.3512064343163539,28,0.9032258064516129,1,0,
133,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,37,0.30578512396694213,132,0.353887399463807,29,0.9354838709677419,1,0,
134,"Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,38,0.3140495867768595,133,0.35656836461126007,30,0.967741935483871,1,0,
135,This would allow for a large increase in batch size .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,39,0.32231404958677684,134,0.35924932975871315,31,1.0,1,0,
136,NETWORK BANDWIDTH,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,40,0.3305785123966942,135,0.36193029490616624,0,0.0,1,0,
137,Another major performance concern in distributed computing is network bandwidth .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,41,0.33884297520661155,136,0.3646112600536193,1,0.14285714285714285,1,0,
138,"Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,42,0.34710743801652894,137,0.3672922252010724,2,0.2857142857142857,1,0,
139,"To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,43,0.35537190082644626,138,0.3699731903485255,3,0.42857142857142855,1,0,
140,"For GPUs , this maybe thousands to one .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,44,0.36363636363636365,139,0.3726541554959786,4,0.5714285714285714,1,0,
141,"In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,45,0.371900826446281,140,0.3753351206434316,5,0.7142857142857143,1,0,
142,"Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,46,0.38016528925619836,141,0.3780160857908847,6,0.8571428571428571,1,0,
143,"Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,47,0.3884297520661157,142,0.3806970509383378,7,1.0,1,0,
144,BALANCING EXPERT UTILIZATION,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,48,0.39669421487603307,143,0.38337801608579086,0,0.0,1,0,
145,We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,49,0.4049586776859504,144,0.38605898123324395,1,0.02631578947368421,1,0,
146,"This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,50,0.4132231404958678,145,0.38873994638069703,2,0.05263157894736842,1,0,
147,"describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,51,0.4214876033057851,146,0.3914209115281501,3,0.07894736842105263,1,0,
148,include a soft constraint on the batch - wise average of each gate .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,52,0.4297520661157025,147,0.3941018766756032,4,0.10526315789473684,1,0,
149,We take a soft constraint approach .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,53,0.4380165289256198,148,0.3967828418230563,5,0.13157894736842105,1,0,
150,We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,54,0.4462809917355372,149,0.39946380697050937,6,0.15789473684210525,1,0,
151,"We define an additional loss L importance , which is added to the over all loss function for the model .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,55,0.45454545454545453,150,0.40214477211796246,7,0.18421052631578946,1,0,
152,"This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,56,0.4628099173553719,151,0.40482573726541554,8,0.21052631578947367,1,0,
153,This additional loss encourages all experts to have equal importance .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,57,0.47107438016528924,152,0.4075067024128686,9,0.23684210526315788,1,0,
154,L importance ( X ) = w importance CV ( Importance ( X ) ),Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,58,0.4793388429752066,153,0.4101876675603217,10,0.2631578947368421,1,0,
155,2 . The number of parameters in the LSTM layers of these models vary from 2 million to 151 million .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,59,0.48760330578512395,154,0.4128686327077748,11,0.2894736842105263,1,0,
156,"Quality increases greatly with parameter count , as do computational costs .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,60,0.49586776859504134,155,0.4155495978552279,12,0.3157894736842105,1,0,
157,Results for these models form the top line of - right .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,61,0.5041322314049587,156,0.41823056300268097,13,0.34210526315789475,1,0,
158,MoE Models :,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,62,0.512396694214876,157,0.42091152815013405,14,0.3684210526315789,1,0,
159,Our models consist of two stacked LSTM layers with a MoE layer between them ( see ) .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,63,0.5206611570247934,158,0.42359249329758714,15,0.39473684210526316,1,0,
160,We vary the sizes of the layers and the number of experts .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,64,0.5289256198347108,159,0.4262734584450402,16,0.42105263157894735,1,0,
161,"For full details on model architecture , training regimen , additional baselines and results , see Appendix C .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,65,0.5371900826446281,160,0.4289544235924933,17,0.4473684210526316,1,0,
162,The results of these models are shown in - left .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,66,0.5454545454545454,161,0.4316353887399464,18,0.47368421052631576,1,0,
163,"The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,67,0.5537190082644629,162,0.4343163538873995,19,0.5,1,0,
164,"Varied Computation , High Capacity :",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,68,0.5619834710743802,163,0.43699731903485256,20,0.5263157894736842,1,0,
165,"In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,69,0.5702479338842975,164,0.43967828418230565,21,0.5526315789473685,1,0,
166,"These models had larger LSTMs , and fewer but larger and experts .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,70,0.5785123966942148,165,0.44235924932975873,22,0.5789473684210527,1,0,
167,Details can be found in Appendix C.2 .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,71,0.5867768595041323,166,0.4450402144772118,23,0.6052631578947368,1,0,
168,Results of these three models form the bottom line of - right .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,72,0.5950413223140496,167,0.4477211796246649,24,0.631578947368421,1,0,
169,compares the results of these models to the best previously - published result on this dataset .,Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,73,0.6033057851239669,168,0.450402144772118,25,0.6578947368421053,1,0,
170,"Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation .",Training the Gating Network,Increasing Batch Size for a,machine-translation,7,,,,74,0.6115702479338843,169,0.45308310991957107,26,0.6842105263157895,1,0,
171,Computational,Training the Gating Network,,machine-translation,7,,,,75,0.6198347107438017,170,0.45576407506702415,27,0.7105263157894737,1,0,
172,Efficiency : We trained our models using TensorFlow on clusters containing 16 - 32 Tesla K40 GPUs .,Training the Gating Network,Computational,machine-translation,7,,,,76,0.628099173553719,171,0.4584450402144772,28,0.7368421052631579,1,0,
173,"For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster .",Training the Gating Network,Computational,machine-translation,7,,,,77,0.6363636363636364,172,0.46112600536193027,29,0.7631578947368421,1,0,
174,"The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .",Training the Gating Network,Computational,machine-translation,7,,,,78,0.6446280991735537,173,0.46380697050938335,30,0.7894736842105263,1,0,
175,"For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total .",Training the Gating Network,Computational,machine-translation,7,,,,79,0.6528925619834711,174,0.46648793565683644,31,0.8157894736842105,1,0,
176,"For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU .",Training the Gating Network,Computational,machine-translation,7,,,,80,0.6611570247933884,175,0.4691689008042895,32,0.8421052631578947,1,0,
177,"For our low-computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism .",Training the Gating Network,Computational,machine-translation,7,,,,81,0.6694214876033058,176,0.4718498659517426,33,0.868421052631579,1,0,
178,"Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices .",Training the Gating Network,Computational,machine-translation,7,,,,82,0.6776859504132231,177,0.4745308310991957,34,0.8947368421052632,1,0,
179,These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .,Training the Gating Network,Computational,machine-translation,7,,,,83,0.6859504132231405,178,0.4772117962466488,35,0.9210526315789473,1,0,
180,"Detailed results are in Appendix C , .",Training the Gating Network,Computational,machine-translation,7,,,,84,0.6942148760330579,179,0.47989276139410186,36,0.9473684210526315,1,0,
181,"On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in - left .",Training the Gating Network,Computational,machine-translation,7,,,,85,0.7024793388429752,180,0.48257372654155495,37,0.9736842105263158,1,0,
182,"We hypothesized that for a larger training set , even higher capacities would produce significant quality improvements .",Training the Gating Network,Computational,machine-translation,7,,,,86,0.7107438016528925,181,0.48525469168900803,38,1.0,1,0,
183,100 BILLION WORD GOOGLE NEWS CORPUS,Training the Gating Network,Computational,machine-translation,7,"['B', 'I', 'I', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b']",87,0.71900826446281,182,0.4879356568364611,0,0.0,1,1,tasks
184,"We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .",Training the Gating Network,Computational,machine-translation,7,,,,88,0.7272727272727273,183,0.4906166219839142,1,0.037037037037037035,1,0,
185,"Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .",Training the Gating Network,Computational,machine-translation,7,,,,89,0.7355371900826446,184,0.4932975871313673,2,0.07407407407407407,1,0,
186,"In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .",Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",90,0.743801652892562,185,0.4959785522788204,3,0.1111111111111111,1,1,tasks
187,This corresponds to up to 137 billion parameters in the MoE layer .,Training the Gating Network,Computational,machine-translation,7,,,,91,0.7520661157024794,186,0.49865951742627346,4,0.14814814814814814,1,0,
188,"Details on architecture , training , and results are given in Appendix D.",Training the Gating Network,Computational,machine-translation,7,,,,92,0.7603305785123967,187,0.5013404825737265,5,0.18518518518518517,1,0,
189,Results : shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) .,Training the Gating Network,Computational,machine-translation,7,,,,93,0.768595041322314,188,0.5040214477211796,6,0.2222222222222222,1,0,
190,"When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .",Training the Gating Network,Computational,machine-translation,7,"['B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",94,0.7768595041322314,189,0.5067024128686327,7,0.25925925925925924,1,1,tasks
191,The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets .,Training the Gating Network,Computational,machine-translation,7,,,,95,0.7851239669421488,190,0.5093833780160858,8,0.2962962962962963,1,0,
192,"Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU .",Training the Gating Network,Computational,machine-translation,7,,,,96,0.7933884297520661,191,0.5120643431635389,9,0.3333333333333333,1,0,
193,MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ),Training the Gating Network,Computational,machine-translation,7,"['B', 'I', 'I', 'I', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b']",97,0.8016528925619835,192,0.514745308310992,10,0.37037037037037035,1,1,tasks
194,Model Architecture :,Training the Gating Network,Computational,machine-translation,7,,,,98,0.8099173553719008,193,0.517426273458445,11,0.4074074074074074,1,0,
195,Our model was a modified version of the GNMT model described in .,Training the Gating Network,Computational,machine-translation,7,"['O', 'B', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O']","['O', 'B-p', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O']","['O', 'B-p', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O']",99,0.8181818181818182,194,0.5201072386058981,12,0.4444444444444444,1,1,tasks
196,"To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",Training the Gating Network,Computational,machine-translation,7,"['B', 'I', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'B-n', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'B-ob', 'O', 'O', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",100,0.8264462809917356,195,0.5227882037533512,13,0.48148148148148145,1,1,tasks
197,We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,Training the Gating Network,Computational,machine-translation,7,"['O', 'B', 'B', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O']",,,101,0.8347107438016529,196,0.5254691689008043,14,0.5185185185185185,1,1,tasks
198,"Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .",Training the Gating Network,Computational,machine-translation,7,"['B', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",102,0.8429752066115702,197,0.5281501340482574,15,0.5555555555555556,1,1,tasks
199,"Further details on model architecture , testing procedure and results can be found in Appendix E.",Training the Gating Network,Computational,machine-translation,7,,,,103,0.8512396694214877,198,0.5308310991957105,16,0.5925925925925926,1,0,
200,Datasets :,Training the Gating Network,Computational,machine-translation,7,,,,104,0.859504132231405,199,0.5335120643431636,17,0.6296296296296297,1,0,
201,We benchmarked our method on the WMT ' 14 En? Fr and En ?,Training the Gating Network,Computational,machine-translation,7,,,,105,0.8677685950413223,200,0.5361930294906166,18,0.6666666666666666,1,0,
202,"De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .",Training the Gating Network,Computational,machine-translation,7,,,,106,0.8760330578512396,201,0.5388739946380697,19,0.7037037037037037,1,0,
203,"The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .",Training the Gating Network,Computational,machine-translation,7,,,,107,0.8842975206611571,202,0.5415549597855228,20,0.7407407407407407,1,0,
204,We also tested the same model on a Google 's Production English to French data . 2.79 39.22 214M 278M 6 days/96 k 80s GNMT+RL 2.96 39.92 214M 278M 6 days/96 k80s PBMT 37.0 LSTM ( 6-layer ) 31.5 LSTM ( 6-layer + PosUnk ) 33.1 DeepAtt 37.7 DeepAtt+PosUnk 39.2 5.25 24.91 214M 278M 1 day/96 k80s GNMT + RL 8.08 24.66 214M 278M 1 day/96 k80s PBMT 20.7 DeepAtt 20.6,Training the Gating Network,Computational,machine-translation,7,,,,108,0.8925619834710744,203,0.5442359249329759,21,0.7777777777777778,1,0,
205,"Results : show the results of our largest models , compared with published results .",Training the Gating Network,Computational,machine-translation,7,,,,109,0.9008264462809917,204,0.546916890080429,22,0.8148148148148148,1,0,
206,Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",110,0.9090909090909091,205,0.5495978552278821,23,0.8518518518518519,1,1,tasks
207,"As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in .",Training the Gating Network,Computational,machine-translation,7,,,,111,0.9173553719008265,206,0.5522788203753352,24,0.8888888888888888,1,0,
208,The perplexity scores are also better .,Training the Gating Network,Computational,machine-translation,7,,,,112,0.9256198347107438,207,0.5549597855227882,25,0.9259259259259259,1,0,
209,2,Training the Gating Network,Computational,machine-translation,7,,,,113,0.9338842975206612,208,0.5576407506702413,26,0.9629629629629629,1,0,
210,"On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .",Training the Gating Network,Computational,machine-translation,7,"['B', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",114,0.9421487603305785,209,0.5603217158176944,27,1.0,1,1,tasks
211,MULTILINGUAL MACHINE TRANSLATION,Training the Gating Network,Computational,machine-translation,7,"['B', 'I', 'I']","['B-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b']",115,0.9504132231404959,210,0.5630026809651475,0,0.0,1,1,tasks
212,Results :,Training the Gating Network,Computational,machine-translation,7,,,,116,0.9586776859504132,211,0.5656836461126006,1,0.16666666666666666,1,0,
213,"Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in .",Training the Gating Network,Computational,machine-translation,7,,,,117,0.9669421487603306,212,0.5683646112600537,2,0.3333333333333333,1,0,
214,The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .,Training the Gating Network,Computational,machine-translation,7,"['O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",118,0.9752066115702479,213,0.5710455764075067,3,0.5,1,1,tasks
215,"On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .",Training the Gating Network,Computational,machine-translation,7,"['B', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']",,,119,0.9834710743801653,214,0.5737265415549598,4,0.6666666666666666,1,1,tasks
216,The poor performance on English ?,Training the Gating Network,Computational,machine-translation,7,,,,120,0.9917355371900827,215,0.5764075067024129,5,0.8333333333333334,1,0,
217,"Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .",Training the Gating Network,Computational,machine-translation,7,,,,121,1.0,216,0.579088471849866,6,1.0,1,0,
218,CONCLUSION,,,machine-translation,7,,,,0,0.0,217,0.5817694369973191,0,0.0,1,0,
219,This work is the first to demonstrate major wins from conditional computation in deep networks .,CONCLUSION,CONCLUSION,machine-translation,7,,,,1,0.0064516129032258064,218,0.5844504021447721,1,0.03125,0,0,
220,We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions .,CONCLUSION,CONCLUSION,machine-translation,7,,,,2,0.012903225806451613,219,0.5871313672922251,2,0.0625,0,0,
221,"While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets .",CONCLUSION,CONCLUSION,machine-translation,7,,,,3,0.01935483870967742,220,0.5898123324396782,3,0.09375,0,0,
222,We look forward to seeing many novel implementations and applications of conditional computation in the years to come .,CONCLUSION,CONCLUSION,machine-translation,7,,,,4,0.025806451612903226,221,0.5924932975871313,4,0.125,0,0,
223,APPENDICES A LOAD - BALANCING LOSS,CONCLUSION,CONCLUSION,machine-translation,7,,,,5,0.03225806451612903,222,0.5951742627345844,5,0.15625,0,0,
224,"As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples .",CONCLUSION,CONCLUSION,machine-translation,7,,,,6,0.03870967741935484,223,0.5978552278820375,6,0.1875,0,0,
225,"Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation .",CONCLUSION,CONCLUSION,machine-translation,7,,,,7,0.04516129032258064,224,0.6005361930294906,7,0.21875,0,0,
226,"Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert for a batch X of inputs .",CONCLUSION,CONCLUSION,machine-translation,7,,,,8,0.05161290322580645,225,0.6032171581769437,8,0.25,0,0,
227,The smoothness allows us to back - propagate gradients through the estimator .,CONCLUSION,CONCLUSION,machine-translation,7,,,,9,0.05806451612903226,226,0.6058981233243967,9,0.28125,0,0,
228,This is the purpose of the noise term in the gating function .,CONCLUSION,CONCLUSION,machine-translation,7,,,,10,0.06451612903225806,227,0.6085790884718498,10,0.3125,0,0,
229,"We define P ( x , i ) as the probability that G (x ) i is nonzero , given a new random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements .",CONCLUSION,CONCLUSION,machine-translation,7,,,,11,0.07096774193548387,228,0.6112600536193029,11,0.34375,0,0,
230,"To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .",CONCLUSION,CONCLUSION,machine-translation,7,,,,12,0.07741935483870968,229,0.613941018766756,12,0.375,0,0,
231,The probability works out to be :,CONCLUSION,CONCLUSION,machine-translation,7,,,,13,0.08387096774193549,230,0.6166219839142091,13,0.40625,0,0,
232,"Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .",CONCLUSION,CONCLUSION,machine-translation,7,,,,14,0.09032258064516129,231,0.6193029490616622,14,0.4375,0,0,
233,"Simplifying , we get :",CONCLUSION,CONCLUSION,machine-translation,7,,,,15,0.0967741935483871,232,0.6219839142091153,15,0.46875,0,0,
234,Where ?,CONCLUSION,CONCLUSION,machine-translation,7,,,,16,0.1032258064516129,233,0.6246648793565683,16,0.5,0,0,
235,is the CDF of the standard normal distribution .,CONCLUSION,CONCLUSION,machine-translation,7,,,,17,0.10967741935483871,234,0.6273458445040214,17,0.53125,0,0,
236,"We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load .",CONCLUSION,CONCLUSION,machine-translation,7,,,,18,0.11612903225806452,235,0.6300268096514745,18,0.5625,0,0,
237,L load ( X ) = w load CV ( Load ( X ) ),CONCLUSION,CONCLUSION,machine-translation,7,,,,19,0.12258064516129032,236,0.6327077747989276,19,0.59375,0,0,
238,2 ( 11 ) Initial Load Imbalance :,CONCLUSION,CONCLUSION,machine-translation,7,,,,20,0.12903225806451613,237,0.6353887399463807,20,0.625,0,0,
239,"To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need sometime to work ) .",CONCLUSION,CONCLUSION,machine-translation,7,,,,21,0.13548387096774195,238,0.6380697050938338,21,0.65625,0,0,
240,"To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise .",CONCLUSION,CONCLUSION,machine-translation,7,,,,22,0.14193548387096774,239,0.6407506702412868,22,0.6875,0,0,
241,Experiments :,CONCLUSION,CONCLUSION,machine-translation,7,,,,23,0.14838709677419354,240,0.6434316353887399,23,0.71875,0,0,
242,"We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load .",CONCLUSION,CONCLUSION,machine-translation,7,,,,24,0.15483870967741936,241,0.646112600536193,24,0.75,0,0,
243,"We trained each model for 10 epochs , then measured perplexity on the test set .",CONCLUSION,CONCLUSION,machine-translation,7,,,,25,0.16129032258064516,242,0.6487935656836461,25,0.78125,0,0,
244,"We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load .",CONCLUSION,CONCLUSION,machine-translation,7,,,,26,0.16774193548387098,243,0.6514745308310992,26,0.8125,0,0,
245,This last value is significant for load balancing purposes on distributed hardware .,CONCLUSION,CONCLUSION,machine-translation,7,,,,27,0.17419354838709677,244,0.6541554959785523,27,0.84375,0,0,
246,All of these metrics were averaged over several training batches .,CONCLUSION,CONCLUSION,machine-translation,7,,,,28,0.18064516129032257,245,0.6568364611260054,28,0.875,0,0,
247,Results :,CONCLUSION,CONCLUSION,machine-translation,7,,,,29,0.1870967741935484,246,0.6595174262734584,29,0.90625,0,0,
248,Results are reported in .,CONCLUSION,CONCLUSION,machine-translation,7,,,,30,0.1935483870967742,247,0.6621983914209115,30,0.9375,0,0,
249,"All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse .",CONCLUSION,CONCLUSION,machine-translation,7,,,,31,0.2,248,0.6648793565683646,31,0.96875,0,0,
250,Models with higher values of w load had lower loads on the most overloaded expert .,CONCLUSION,CONCLUSION,machine-translation,7,,,,32,0.2064516129032258,249,0.6675603217158177,32,1.0,0,0,
251,B HIERACHICAL MIXTURE OF EXPERTS,CONCLUSION,CONCLUSION,machine-translation,7,,,,33,0.2129032258064516,250,0.6702412868632708,0,0.0,0,0,
252,"If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of "" experts "" , each of which is itself a secondary mixture - of - experts with its own gating network .",CONCLUSION,CONCLUSION,machine-translation,7,,,,34,0.21935483870967742,251,0.6729222520107239,1,0.01,0,0,
253,3,CONCLUSION,CONCLUSION,machine-translation,7,,,,35,0.22580645161290322,252,0.675603217158177,2,0.02,0,0,
254,"If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by , and the expert networks by ( E 0 , 0 , E 0 , 1 ..E a , b ) .",CONCLUSION,CONCLUSION,machine-translation,7,,,,36,0.23225806451612904,253,0.67828418230563,3,0.03,0,0,
255,The output of the MoE is given by :,CONCLUSION,CONCLUSION,machine-translation,7,,,,37,0.23870967741935484,254,0.6809651474530831,4,0.04,0,0,
256,Our metrics of expert utilization change to the following :,CONCLUSION,CONCLUSION,machine-translation,7,,,,38,0.24516129032258063,255,0.6836461126005362,5,0.05,0,0,
257,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,CONCLUSION,CONCLUSION,machine-translation,7,,,,39,0.25161290322580643,256,0.6863270777479893,6,0.06,0,0,
258,X ( i ) denotes the subset of X for which G primary ( x ) i >,CONCLUSION,CONCLUSION,machine-translation,7,,,,40,0.25806451612903225,257,0.6890080428954424,7,0.07,0,0,
259,0 .,CONCLUSION,CONCLUSION,machine-translation,7,,,,41,0.2645161290322581,258,0.6916890080428955,8,0.08,0,0,
260,"It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above .",CONCLUSION,CONCLUSION,machine-translation,7,,,,42,0.2709677419354839,259,0.6943699731903485,9,0.09,0,0,
261,C 1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS,CONCLUSION,CONCLUSION,machine-translation,7,,,,43,0.27741935483870966,260,0.6970509383378016,10,0.1,0,0,
262,C.1 8- MILLION - OPERATIONS - PER - TIMESTEP MODELS,CONCLUSION,CONCLUSION,machine-translation,7,,,,44,0.2838709677419355,261,0.6997319034852547,11,0.11,0,0,
263,Model Architecture :,CONCLUSION,CONCLUSION,machine-translation,7,,,,45,0.2903225806451613,262,0.7024128686327078,12,0.12,0,0,
264,"Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer , a MoE layer , a second LSTM layer , and a softmax layer .",CONCLUSION,CONCLUSION,machine-translation,7,,,,46,0.2967741935483871,263,0.7050938337801609,13,0.13,0,0,
265,"The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 .",CONCLUSION,CONCLUSION,machine-translation,7,,,,47,0.3032258064516129,264,0.707774798927614,14,0.14,0,0,
266,"For every layer other than the softmax , we apply drouput to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 ? DropP rob ) .",CONCLUSION,CONCLUSION,machine-translation,7,,,,48,0.3096774193548387,265,0.710455764075067,15,0.15,0,0,
267,"After dropout , the output of the previous layer is added to the layer output .",CONCLUSION,CONCLUSION,machine-translation,7,,,,49,0.3161290322580645,266,0.7131367292225201,16,0.16,0,0,
268,This residual connection encourages gradient flow .,CONCLUSION,CONCLUSION,machine-translation,7,,,,50,0.3225806451612903,267,0.7158176943699732,17,0.17,0,0,
269,"For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .",CONCLUSION,CONCLUSION,machine-translation,7,,,,51,0.32903225806451614,268,0.7184986595174263,18,0.18,0,0,
270,We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers .,CONCLUSION,CONCLUSION,machine-translation,7,,,,52,0.33548387096774196,269,0.7211796246648794,19,0.19,0,0,
271,"Thus , each example is processed by exactly 4 experts for a total of 4M ops / timestep .",CONCLUSION,CONCLUSION,machine-translation,7,,,,53,0.3419354838709677,270,0.7238605898123325,20,0.2,0,0,
272,The two LSTM layers contribute 2M ops / timestep each for the desired total of 8 M .,CONCLUSION,CONCLUSION,machine-translation,7,,,,54,0.34838709677419355,271,0.7265415549597856,21,0.21,0,0,
273,Computationally - Matched Baselines :,CONCLUSION,CONCLUSION,machine-translation,7,,,,55,0.3548387096774194,272,0.7292225201072386,22,0.22,0,0,
274,"The MoE - 4 model does not employ sparsity , since all 4 experts are always used .",CONCLUSION,CONCLUSION,machine-translation,7,,,,56,0.36129032258064514,273,0.7319034852546917,23,0.23,0,0,
275,"In addition , we trained four more computationally - matched baseline models with no sparsity :",CONCLUSION,CONCLUSION,machine-translation,7,,,,57,0.36774193548387096,274,0.7345844504021448,24,0.24,0,0,
276,MoE - 1 - Wide :,CONCLUSION,CONCLUSION,machine-translation,7,,,,58,0.3741935483870968,275,0.7372654155495979,25,0.25,0,0,
277,"The MoE layer consists of a single "" expert "" containing one ReLU - activated hidden layer of size 4096 .",CONCLUSION,CONCLUSION,machine-translation,7,,,,59,0.38064516129032255,276,0.739946380697051,26,0.26,0,0,
278,MoE - 1 - Deep :,CONCLUSION,CONCLUSION,machine-translation,7,,,,60,0.3870967741935484,277,0.7426273458445041,27,0.27,0,0,
279,"The MoE layer consists of a single "" expert "" containing four ReLU - activated hidden layers , each with size 1024 .",CONCLUSION,CONCLUSION,machine-translation,7,,,,61,0.3935483870967742,278,0.7453083109919572,28,0.28,0,0,
280,4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers .,CONCLUSION,CONCLUSION,machine-translation,7,,,,62,0.4,279,0.7479892761394102,29,0.29,0,0,
281,LSTM - 2048-512 :,CONCLUSION,CONCLUSION,machine-translation,7,,,,63,0.4064516129032258,280,0.7506702412868632,30,0.3,0,0,
282,The model contains one 2048 - unit LSTM layer ( and no MoE ) .,CONCLUSION,CONCLUSION,machine-translation,7,,,,64,0.4129032258064516,281,0.7533512064343163,31,0.31,0,0,
283,The output of the LSTM is projected down to 512 dimensions .,CONCLUSION,CONCLUSION,machine-translation,7,,,,65,0.41935483870967744,282,0.7560321715817694,32,0.32,0,0,
284,The next timestep of the LSTM receives the projected output .,CONCLUSION,CONCLUSION,machine-translation,7,,,,66,0.4258064516129032,283,0.7587131367292225,33,0.33,0,0,
285,This is identical to one of the models published in .,CONCLUSION,CONCLUSION,machine-translation,7,,,,67,0.432258064516129,284,0.7613941018766756,34,0.34,0,0,
286,"We re-ran it to account for differences in training regimen , and obtained results very similar to the published ones .",CONCLUSION,CONCLUSION,machine-translation,7,,,,68,0.43870967741935485,285,0.7640750670241286,35,0.35,0,0,
287,Training :,CONCLUSION,CONCLUSION,machine-translation,7,,,,69,0.44516129032258067,286,0.7667560321715817,36,0.36,0,0,
288,The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 .,CONCLUSION,CONCLUSION,machine-translation,7,,,,70,0.45161290322580644,287,0.7694369973190348,37,0.37,0,0,
289,"Each batch consisted of a set of sentences totaling roughly 300,000 words .",CONCLUSION,CONCLUSION,machine-translation,7,,,,71,0.45806451612903226,288,0.7721179624664879,38,0.38,0,0,
290,"In the interest of time , we limited training to 10 epochs , ( 27,000 steps ) .",CONCLUSION,CONCLUSION,machine-translation,7,,,,72,0.4645161290322581,289,0.774798927613941,39,0.39,0,0,
291,"Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) .",CONCLUSION,CONCLUSION,machine-translation,7,,,,73,0.47096774193548385,290,0.7774798927613941,40,0.4,0,0,
292,We used the Adam optimizer .,CONCLUSION,CONCLUSION,machine-translation,7,,,,74,0.4774193548387097,291,0.7801608579088471,41,0.41,0,0,
293,"The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number .",CONCLUSION,CONCLUSION,machine-translation,7,,,,75,0.4838709677419355,292,0.7828418230563002,42,0.42,0,0,
294,The Softmax output layer was trained efficiently using importance sampling similarly to the models in .,CONCLUSION,CONCLUSION,machine-translation,7,,,,76,0.49032258064516127,293,0.7855227882037533,43,0.43,0,0,
295,"For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 .",CONCLUSION,CONCLUSION,machine-translation,7,,,,77,0.4967741935483871,294,0.7882037533512064,44,0.44,0,0,
296,"To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A.",CONCLUSION,CONCLUSION,machine-translation,7,,,,78,0.5032258064516129,295,0.7908847184986595,45,0.45,0,0,
297,Results :,CONCLUSION,CONCLUSION,machine-translation,7,,,,79,0.5096774193548387,296,0.7935656836461126,46,0.46,0,0,
298,"We evaluate our model using perplexity on the holdout dataset , used by .",CONCLUSION,CONCLUSION,machine-translation,7,,,,80,0.5161290322580645,297,0.7962466487935657,47,0.47,0,0,
299,We follow the standard procedure and sum over all the words including the end of sentence symbol .,CONCLUSION,CONCLUSION,machine-translation,7,,,,81,0.5225806451612903,298,0.7989276139410187,48,0.48,0,0,
300,Results are reported in .,CONCLUSION,CONCLUSION,machine-translation,7,,,,82,0.5290322580645161,299,0.8016085790884718,49,0.49,0,0,
301,"For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .",CONCLUSION,CONCLUSION,machine-translation,7,,,,83,0.535483870967742,300,0.8042895442359249,50,0.5,0,0,
302,We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .,CONCLUSION,CONCLUSION,machine-translation,7,,,,84,0.5419354838709678,301,0.806970509383378,51,0.51,0,0,
303,"First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass .",CONCLUSION,CONCLUSION,machine-translation,7,,,,85,0.5483870967741935,302,0.8096514745308311,52,0.52,0,0,
304,"Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage :",CONCLUSION,CONCLUSION,machine-translation,7,,,,86,0.5548387096774193,303,0.8123324396782842,53,0.53,0,0,
305,The Adam optimizer keeps first and second moment estimates of the perparameter gradients .,CONCLUSION,CONCLUSION,machine-translation,7,,,,87,0.5612903225806452,304,0.8150134048257373,54,0.54,0,0,
306,This triples the required memory .,CONCLUSION,CONCLUSION,machine-translation,7,,,,88,0.567741935483871,305,0.8176943699731903,55,0.55,0,0,
307,"To avoid keeping a first - moment estimator , we set ?",CONCLUSION,CONCLUSION,machine-translation,7,,,,89,0.5741935483870968,306,0.8203753351206434,56,0.56,0,0,
308,"1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .",CONCLUSION,CONCLUSION,machine-translation,7,,,,90,0.5806451612903226,307,0.8230563002680965,57,0.57,0,0,
309,"For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix .",CONCLUSION,CONCLUSION,machine-translation,7,,,,91,0.5870967741935483,308,0.8257372654155496,58,0.58,0,0,
310,"At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one .",CONCLUSION,CONCLUSION,machine-translation,7,,,,92,0.5935483870967742,309,0.8284182305630027,59,0.59,0,0,
311,This technique could similarly be applied to Adagrad .,CONCLUSION,CONCLUSION,machine-translation,7,,,,93,0.6,310,0.8310991957104558,60,0.6,0,0,
312,Results :,CONCLUSION,CONCLUSION,machine-translation,7,,,,94,0.6064516129032258,311,0.8337801608579088,61,0.61,0,0,
313,We evaluate our model using perplexity on a holdout dataset .,CONCLUSION,CONCLUSION,machine-translation,7,,,,95,0.6129032258064516,312,0.8364611260053619,62,0.62,0,0,
314,Results are reported in .,CONCLUSION,CONCLUSION,machine-translation,7,,,,96,0.6193548387096774,313,0.839142091152815,63,0.63,0,0,
315,Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model .,CONCLUSION,CONCLUSION,machine-translation,7,,,,97,0.6258064516129033,314,0.8418230563002681,64,0.64,0,0,
316,It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models .,CONCLUSION,CONCLUSION,machine-translation,7,,,,98,0.632258064516129,315,0.8445040214477212,65,0.65,0,0,
317,"This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs .",CONCLUSION,CONCLUSION,machine-translation,7,,,,99,0.6387096774193548,316,0.8471849865951743,66,0.66,0,0,
318,"For comparison , we include results for a computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing .",CONCLUSION,CONCLUSION,machine-translation,7,,,,100,0.6451612903225806,317,0.8498659517426274,67,0.67,0,0,
319,4,CONCLUSION,CONCLUSION,machine-translation,7,,,,101,0.6516129032258065,318,0.8525469168900804,68,0.68,0,0,
320,E MACHINE TRANSLATION - EXPERIMENTAL DETAILS,CONCLUSION,CONCLUSION,machine-translation,7,,,,102,0.6580645161290323,319,0.8552278820375335,69,0.69,0,0,
321,Model Architecture for Single Language,CONCLUSION,,machine-translation,7,,,,103,0.6645161290322581,320,0.8579088471849866,70,0.7,0,0,
322,Pair MoE Models :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,104,0.6709677419354839,321,0.8605898123324397,71,0.71,0,0,
323,Our model is a modified version of the GNMT model described in .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,105,0.6774193548387096,322,0.8632707774798928,72,0.72,0,0,
324,"To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,106,0.6838709677419355,323,0.8659517426273459,73,0.73,0,0,
325,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,107,0.6903225806451613,324,0.868632707774799,74,0.74,0,0,
326,"We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,108,0.6967741935483871,325,0.871313672922252,75,0.75,0,0,
327,All of the layers in our model have input and output dimensionality of 512 .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,109,0.7032258064516129,326,0.8739946380697051,76,0.76,0,0,
328,"Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,110,0.7096774193548387,327,0.8766756032171582,77,0.77,0,0,
329,We add residual connections around all LSTM and MoE layers to encourage gradient flow .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,111,0.7161290322580646,328,0.8793565683646113,78,0.78,0,0,
330,"Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as "" wordpieces "" )",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,112,0.7225806451612903,329,0.8820375335120644,79,0.79,0,0,
331,"( Schuster & Nakajima , 2012 ) for inputs and outputs in our system .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,113,0.7290322580645161,330,0.8847184986595175,80,0.8,0,0,
332,We use a shared source and target vocabulary of 32 K wordpieces .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,114,0.7354838709677419,331,0.8873994638069705,81,0.81,0,0,
333,We also used the same beam search technique as proposed in Model Architecture for Multilingual MoE Model :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,115,0.7419354838709677,332,0.8900804289544236,82,0.82,0,0,
334,"We used the same model architecture as for the single - language - pair models , with the following exceptions :",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,116,0.7483870967741936,333,0.8927613941018767,83,0.83,0,0,
335,"We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,117,0.7548387096774194,334,0.8954423592493298,84,0.84,0,0,
336,Each expert has a larger hidden layer of size 8192 .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,118,0.7612903225806451,335,0.8981233243967829,85,0.85,0,0,
337,"This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,119,0.7677419354838709,336,0.900804289544236,86,0.86,0,0,
338,Training :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,120,0.7741935483870968,337,0.903485254691689,87,0.87,0,0,
339,We trained our networks using the Adam optimizer .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,121,0.7806451612903226,338,0.9061662198391421,88,0.88,0,0,
340,"The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,122,0.7870967741935484,339,0.9088471849865952,89,0.89,0,0,
341,"For the single - language - pair models , similarly to , we applied dropout to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,123,0.7935483870967742,340,0.9115281501340483,90,0.9,0,0,
342,Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,124,0.8,341,0.9142091152815014,91,0.91,0,0,
343,Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,125,0.8064516129032258,342,0.9168900804289544,92,0.92,0,0,
344,"To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A.",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,126,0.8129032258064516,343,0.9195710455764075,93,0.93,0,0,
345,Metrics : We evaluated our models using the perplexity and the standard BLEU score metric .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,127,0.8193548387096774,344,0.9222520107238605,94,0.94,0,0,
346,"We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,128,0.8258064516129032,345,0.9249329758713136,95,0.95,0,0,
347,Results : and 4 in Section 5.3 show comparisons of our results to other published methods .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,129,0.832258064516129,346,0.9276139410187667,96,0.96,0,0,
348,shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,130,0.8387096774193549,347,0.9302949061662198,97,0.97,0,0,
349,"As can be seen from the as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,131,0.8451612903225807,348,0.9329758713136729,98,0.98,0,0,
350,"We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,132,0.8516129032258064,349,0.935656836461126,99,0.99,0,0,
351,"For example , one expert is used when the indefinite article "" a "" introduces the direct object in a verb phrase indicating importance or leadership .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,133,0.8580645161290322,350,0.938337801608579,100,1.0,0,0,
352,F STRICTLY BALANCED GATING,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,134,0.864516129032258,351,0.9410187667560321,0,0.0,0,0,
353,"Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,135,0.8709677419354839,352,0.9436997319034852,1,0.07692307692307693,0,0,
354,"To accommodate this , we used a different gating function which we describe below .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,136,0.8774193548387097,353,0.9463806970509383,2,0.15384615384615385,0,0,
355,Recall that we define the softmax gating function to be :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,137,0.8838709677419355,354,0.9490616621983914,3,0.23076923076923078,0,0,
356,Sparse Gating ( alternate formulation ) :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,138,0.8903225806451613,355,0.9517426273458445,4,0.3076923076923077,0,0,
357,"To obtain a sparse gating vector , we multiply G ?",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,139,0.896774193548387,356,0.9544235924932976,5,0.38461538461538464,0,0,
358,( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,140,0.9032258064516129,357,0.9571045576407506,6,0.46153846153846156,0,0,
359,"The mask itself is a function of G ? ( x ) and specifies which experts are assigned to each input example : M batchwise ( X , m ) j , i = 1 if X j, i is in the top m values for to expert i 0 otherwise",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,141,0.9096774193548387,358,0.9597855227882037,7,0.5384615384615384,0,0,
360,"As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,142,0.9161290322580645,359,0.9624664879356568,8,0.6153846153846154,0,0,
361,Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,143,0.9225806451612903,360,0.9651474530831099,9,0.6923076923076923,0,0,
362,We use the following mask at inference time :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,144,0.9290322580645162,361,0.967828418230563,10,0.7692307692307693,0,0,
363,"To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,145,0.9354838709677419,362,0.9705093833780161,11,0.8461538461538461,0,0,
364,"L batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ?",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,146,0.9419354838709677,363,0.9731903485254692,12,0.9230769230769231,0,0,
365,"M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,147,0.9483870967741935,364,0.9758713136729222,13,1.0,0,0,
366,G ATTENTION FUNCTION,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,148,0.9548387096774194,365,0.9785522788203753,0,0.0,0,0,
367,"The attention mechanism described in GNMT involves a learned "" Attention Function "" A ( x i , y j ) which takes a "" source vector "" x i and a "" target vector "" y j , and must be computed for every source time step i and target time step j .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,149,0.9612903225806452,366,0.9812332439678284,1,0.14285714285714285,0,0,
368,"In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n.",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,150,0.967741935483871,367,0.9839142091152815,2,0.2857142857142857,0,0,
369,It can be expressed as :,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,151,0.9741935483870968,368,0.9865951742627346,3,0.42857142857142855,0,0,
370,Where U and Ware trainable weight matrices and V is a trainable weight vector .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,152,0.9806451612903225,369,0.9892761394101877,4,0.5714285714285714,0,0,
371,"For performance reasons , in our models , we used a slightly different attention function :",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,153,0.9870967741935484,370,0.9919571045576407,5,0.7142857142857143,0,0,
372,"With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications .",CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,154,0.9935483870967742,371,0.9946380697050938,6,0.8571428571428571,0,0,
373,We found little difference in quality between the two functions .,CONCLUSION,Model Architecture for Single Language,machine-translation,7,,,,155,1.0,372,0.9973190348525469,7,1.0,0,0,
1,title,,,named-entity-recognition,8,,,,0,0.0,0,0.0,0,0.0,1,0,
2,BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding,title,title,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob']",1,0.0,1,0.002583979328165375,1,0.0,1,1,research-problem
3,abstract,,,named-entity-recognition,8,,,,0,0.0,2,0.00516795865633075,0,0.0,1,0,
4,"We introduce a new language representation model called BERT , which stands for Bidirectional Encoder Representations from Transformers .",abstract,abstract,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.1111111111111111,3,0.007751937984496124,1,0.1111111111111111,1,1,research-problem
5,"Unlike recent language representation models ( Peters et al. , 2018 a ; Radford et al. , 2018 ) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .",abstract,abstract,named-entity-recognition,8,,,,2,0.2222222222222222,4,0.0103359173126615,2,0.2222222222222222,1,0,
6,"As a result , the pre-trained BERT model can be finetuned with just one additional output layer to create state - of - the - art models for a wide range of tasks , such as question answering and language inference , without substantial taskspecific architecture modifications .",abstract,abstract,named-entity-recognition,8,,,,3,0.3333333333333333,5,0.012919896640826873,3,0.3333333333333333,1,0,
7,BERT is conceptually simple and empirically powerful .,abstract,abstract,named-entity-recognition,8,,,,4,0.4444444444444444,6,0.015503875968992248,4,0.4444444444444444,1,0,
8,"It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQ u AD v 1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQ u AD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .",abstract,abstract,named-entity-recognition,8,,,,5,0.5555555555555556,7,0.01808785529715762,5,0.5555555555555556,1,0,
9,Jeremy Howard and Sebastian Ruder . 2018 .,abstract,abstract,named-entity-recognition,8,,,,6,0.6666666666666666,8,0.020671834625323,6,0.6666666666666666,1,0,
10,Universal language model fine - tuning for text classification .,abstract,abstract,named-entity-recognition,8,,,,7,0.7777777777777778,9,0.023255813953488372,7,0.7777777777777778,1,0,
11,In ACL .,abstract,abstract,named-entity-recognition,8,,,,8,0.8888888888888888,10,0.025839793281653745,8,0.8888888888888888,1,0,
12,Association for Computational Linguistics .,abstract,,named-entity-recognition,8,,,,9,1.0,11,0.028423772609819122,9,1.0,1,0,
13,Introduction,,,named-entity-recognition,8,,,,0,0.0,12,0.031007751937984496,0,0.0,1,0,
14,Language model pre-training has been shown to be effective for improving many natural language processing tasks .,Introduction,Introduction,named-entity-recognition,8,"['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.041666666666666664,13,0.03359173126614987,1,0.041666666666666664,1,1,research-problem
15,"These include sentence - level tasks such as natural language inference and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically , as well as token - level tasks such as named entity recognition and question answering , where models are required to produce fine - grained output at the token level .",Introduction,Introduction,named-entity-recognition,8,,,,2,0.08333333333333333,14,0.03617571059431524,2,0.08333333333333333,1,0,
16,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,Introduction,Introduction,named-entity-recognition,8,,,,3,0.125,15,0.03875968992248062,3,0.125,1,0,
17,"The feature - based approach , such as ELMo , uses task - specific architectures that include the pre-trained representations as additional features .",Introduction,Introduction,named-entity-recognition,8,,,,4,0.16666666666666666,16,0.041343669250646,4,0.16666666666666666,1,0,
18,"The fine - tuning approach , such as the Generative Pre-trained Transformer ( OpenAI GPT ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",Introduction,Introduction,named-entity-recognition,8,,,,5,0.20833333333333334,17,0.04392764857881137,5,0.20833333333333334,1,0,
19,"The two approaches share the same objective function during pre-training , where they use unidirectional language models to learn general language representations .",Introduction,Introduction,named-entity-recognition,8,,,,6,0.25,18,0.046511627906976744,6,0.25,1,0,
20,"We argue that current techniques restrict the power of the pre-trained representations , especially for the fine - tuning approaches .",Introduction,Introduction,named-entity-recognition,8,,,,7,0.2916666666666667,19,0.04909560723514212,7,0.2916666666666667,1,0,
21,"The major limitation is that standard language models are unidirectional , and this limits the choice of architectures that can be used during pre-training .",Introduction,Introduction,named-entity-recognition,8,,,,8,0.3333333333333333,20,0.05167958656330749,8,0.3333333333333333,1,0,
22,"For example , in Open AI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer .",Introduction,Introduction,named-entity-recognition,8,,,,9,0.375,21,0.05426356589147287,9,0.375,1,0,
23,"Such restrictions are sub-optimal for sentence - level tasks , and could be very harmful when applying finetuning based approaches to token - level tasks such as question answering , where it is crucial to incorporate context from both directions .",Introduction,Introduction,named-entity-recognition,8,,,,10,0.4166666666666667,22,0.056847545219638244,10,0.4166666666666667,1,0,
24,"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.4583333333333333,23,0.059431524547803614,11,0.4583333333333333,1,1,approach
25,"BERT alleviates the previously mentioned unidirectionality constraint by using a "" masked language model "" ( MLM ) pre-training objective , inspired by the Cloze task .",Introduction,Introduction,named-entity-recognition,8,"['B', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'B-p', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.5,24,0.06201550387596899,12,0.5,1,1,approach
26,"The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context .",Introduction,Introduction,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'O', 'O', 'O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'O']",13,0.5416666666666666,25,0.06459948320413436,13,0.5416666666666666,1,1,approach
27,"Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",14,0.5833333333333334,26,0.06718346253229975,14,0.5833333333333334,1,1,approach
28,"In addition to the masked language model , we also use a "" next sentence prediction "" task that jointly pretrains text - pair representations .",Introduction,Introduction,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",15,0.625,27,0.06976744186046512,15,0.625,1,1,approach
29,The contributions of our paper are as follows :,Introduction,Introduction,named-entity-recognition,8,,,,16,0.6666666666666666,28,0.07235142118863049,16,0.6666666666666666,1,0,
30,We demonstrate the importance of bidirectional pre-training for language representations .,Introduction,Introduction,named-entity-recognition,8,,,,17,0.7083333333333334,29,0.07493540051679587,17,0.7083333333333334,1,0,
31,"Unlike , which uses unidirectional language models for pre-training , BERT uses masked language models to enable pretrained deep bidirectional representations .",Introduction,Introduction,named-entity-recognition,8,,,,18,0.75,30,0.07751937984496124,18,0.75,1,0,
32,"This is also in contrast to , which uses a shallow concatenation of independently trained left - to - right and right - to - left LMs .",Introduction,Introduction,named-entity-recognition,8,,,,19,0.7916666666666666,31,0.08010335917312661,19,0.7916666666666666,1,0,
33,We show that pre-trained representations reduce the need for many heavily - engineered taskspecific architectures .,Introduction,Introduction,named-entity-recognition,8,,,,20,0.8333333333333334,32,0.082687338501292,20,0.8333333333333334,1,0,
34,"BERT is the first finetuning based representation model that achieves state - of - the - art performance on a large suite of sentence - level and token - level tasks , outperforming many task - specific architectures .",Introduction,Introduction,named-entity-recognition,8,,,,21,0.875,33,0.08527131782945736,21,0.875,1,0,
35,BERT advances the state of the art for eleven NLP tasks .,Introduction,Introduction,named-entity-recognition,8,,,,22,0.9166666666666666,34,0.08785529715762273,22,0.9166666666666666,1,0,
36,The code and pre-trained models are available at https://github.com/,Introduction,Introduction,named-entity-recognition,8,,,,23,0.9583333333333334,35,0.09043927648578812,23,0.9583333333333334,1,0,
37,google-research/bert .,Introduction,Introduction,named-entity-recognition,8,,,,24,1.0,36,0.09302325581395349,24,1.0,1,0,
38,Related Work,,,named-entity-recognition,8,,,,0,0.0,37,0.09560723514211886,0,0.0,1,0,
39,"There is along history of pre-training general language representations , and we briefly review the most widely - used approaches in this section .",Related Work,Related Work,named-entity-recognition,8,,,,1,0.07142857142857142,38,0.09819121447028424,1,0.03571428571428571,0,0,
40,Unsupervised Feature - based Approaches,Related Work,Related Work,named-entity-recognition,8,,,,2,0.14285714285714285,39,0.10077519379844961,2,0.07142857142857142,0,0,
41,"Learning widely applicable representations of words has been an active are a of research for decades , including non-neural and neural methods .",Related Work,Related Work,named-entity-recognition,8,,,,3,0.21428571428571427,40,0.10335917312661498,3,0.10714285714285714,0,0,
42,"Pre-trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch .",Related Work,Related Work,named-entity-recognition,8,,,,4,0.2857142857142857,41,0.10594315245478036,4,0.14285714285714285,0,0,
43,"To pretrain word embedding vectors , left - to - right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context .",Related Work,Related Work,named-entity-recognition,8,,,,5,0.35714285714285715,42,0.10852713178294573,5,0.17857142857142858,0,0,
44,"These approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .",Related Work,Related Work,named-entity-recognition,8,,,,6,0.42857142857142855,43,0.1111111111111111,6,0.21428571428571427,0,0,
45,"To train sentence representations , prior work has used objectives to rank candidate next sentences , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .",Related Work,Related Work,named-entity-recognition,8,,,,7,0.5,44,0.11369509043927649,7,0.25,0,0,
46,ELMo and its predecessor generalize traditional word embedding research along a different dimension .,Related Work,Related Work,named-entity-recognition,8,,,,8,0.5714285714285714,45,0.11627906976744186,8,0.2857142857142857,0,0,
47,They extract context - sensitive features from a left - to - right and a right - to - left language model .,Related Work,Related Work,named-entity-recognition,8,,,,9,0.6428571428571429,46,0.11886304909560723,9,0.32142857142857145,0,0,
48,The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations .,Related Work,Related Work,named-entity-recognition,8,,,,10,0.7142857142857143,47,0.12144702842377261,10,0.35714285714285715,0,0,
49,"When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks including question answering , sentiment analysis , and named entity recognition .",Related Work,Related Work,named-entity-recognition,8,,,,11,0.7857142857142857,48,0.12403100775193798,11,0.39285714285714285,0,0,
50,proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs .,Related Work,Related Work,named-entity-recognition,8,,,,12,0.8571428571428571,49,0.12661498708010335,12,0.42857142857142855,0,0,
51,"Similar to ELMo , their model is feature - based and not deeply bidirectional .",Related Work,Related Work,named-entity-recognition,8,,,,13,0.9285714285714286,50,0.12919896640826872,13,0.4642857142857143,0,0,
52,shows that the cloze task can be used to improve the robustness of text generation models .,Related Work,Related Work,named-entity-recognition,8,,,,14,1.0,51,0.13178294573643412,14,0.5,0,0,
53,Unsupervised Fine- tuning Approaches,,,named-entity-recognition,8,,,,0,0.0,52,0.1343669250645995,15,0.5357142857142857,1,0,
54,"As with the feature - based approaches , the first works in this direction only pre-trained word embedding parameters from unlabeled text .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,1,0.04,53,0.13695090439276486,16,0.5714285714285714,1,0,
55,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned for a supervised downstream task .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,2,0.08,54,0.13953488372093023,17,0.6071428571428571,1,0,
56,The advantage of these approaches is that few parameters need to be learned from scratch .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,3,0.12,55,0.1421188630490956,18,0.6428571428571429,1,0,
57,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,4,0.16,56,0.14470284237726097,19,0.6785714285714286,1,0,
58,Left - to - right language model - BERT BERT E E 1 E ...,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,5,0.2,57,0.14728682170542637,20,0.7142857142857143,1,0,
59,CT 1 T ... E 1 E ...,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,6,0.24,58,0.14987080103359174,21,0.75,1,0,
60,CT 1 T ... :,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,7,0.28,59,0.1524547803617571,22,0.7857142857142857,1,0,
61,Overall pre-training and fine - tuning procedures for BERT .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,8,0.32,60,0.15503875968992248,23,0.8214285714285714,1,0,
62,"Apart from output layers , the same architectures are used in both pre-training and fine - tuning .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,9,0.36,61,0.15762273901808785,24,0.8571428571428571,1,0,
63,The same pre-trained model parameters are used to initialize models for different down - stream tasks .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,10,0.4,62,0.16020671834625322,25,0.8928571428571429,1,0,
64,"During fine - tuning , all parameters are fine - tuned .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,11,0.44,63,0.16279069767441862,26,0.9285714285714286,1,0,
65,"[ CLS ] is a special symbol added in front of every input example , and [ SEP ] is a special separator token ( e.g. separating questions / answers ) .",Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,12,0.48,64,0.165374677002584,27,0.9642857142857143,1,0,
66,ing and auto - encoder objectives have been used for pre-training such models .,Unsupervised Fine- tuning Approaches,Unsupervised Fine- tuning Approaches,named-entity-recognition,8,,,,13,0.52,65,0.16795865633074936,28,1.0,1,0,
67,Transfer Learning from Supervised Data,Unsupervised Fine- tuning Approaches,,named-entity-recognition,8,,,,14,0.56,66,0.17054263565891473,0,0.0,1,0,
68,"There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference and machine translation .",Unsupervised Fine- tuning Approaches,Transfer Learning from Supervised Data,named-entity-recognition,8,,,,15,0.6,67,0.1731266149870801,1,0.5,1,0,
69,"Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models , where an effective recipe is to fine - tune models pre-trained with I ma - geNet .",Unsupervised Fine- tuning Approaches,Transfer Learning from Supervised Data,named-entity-recognition,8,,,,16,0.64,68,0.17571059431524547,2,1.0,1,0,
70,BERT,Unsupervised Fine- tuning Approaches,,named-entity-recognition,8,,,,17,0.68,69,0.17829457364341086,0,0.0,1,0,
71,We introduce BERT and its detailed implementation in this section .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,18,0.72,70,0.18087855297157623,1,0.012195121951219513,1,0,
72,There are two steps in our framework : pre-training and fine - tuning .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,19,0.76,71,0.1834625322997416,2,0.024390243902439025,1,0,
73,"During pre-training , the model is trained on unlabeled data over different pre-training tasks .",Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,20,0.8,72,0.18604651162790697,3,0.036585365853658534,1,0,
74,"For finetuning , the BERT model is first initialized with the pre-trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,21,0.84,73,0.18863049095607234,4,0.04878048780487805,1,0,
75,"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre-trained parameters .",Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,22,0.88,74,0.19121447028423771,5,0.06097560975609756,1,0,
76,The question - answering example in will serve as a running example for this section .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,23,0.92,75,0.1937984496124031,6,0.07317073170731707,1,0,
77,A distinctive feature of BERT is its unified architecture across different tasks .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,24,0.96,76,0.19638242894056848,7,0.08536585365853659,1,0,
78,There is mini-mal difference between the pre-trained architecture and the final downstream architecture .,Unsupervised Fine- tuning Approaches,BERT,named-entity-recognition,8,,,,25,1.0,77,0.19896640826873385,8,0.0975609756097561,1,0,
79,Model Architecture,,,named-entity-recognition,8,,,,0,0.0,78,0.20155038759689922,9,0.10975609756097561,1,0,
80,BERT 's model architecture is a multi - layer bidirectional Transformer encoder based on the original implementation and released in the tensor2 tensor library .,Model Architecture,Model Architecture,named-entity-recognition,8,,,,1,0.0136986301369863,79,0.2041343669250646,10,0.12195121951219512,1,0,
81,1,Model Architecture,Model Architecture,named-entity-recognition,8,,,,2,0.0273972602739726,80,0.20671834625322996,11,0.13414634146341464,1,0,
82,"Because the use of Transformers has become common and our implementation is almost identical to the original , we will omit an exhaustive background description of the model architecture and refer readers to as well as excellent guides such as "" The Annotated Transformer . """,Model Architecture,Model Architecture,named-entity-recognition,8,,,,3,0.0410958904109589,81,0.20930232558139536,12,0.14634146341463414,1,0,
83,2,Model Architecture,Model Architecture,named-entity-recognition,8,,,,4,0.0547945205479452,82,0.21188630490956073,13,0.15853658536585366,1,0,
84,"In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A .",Model Architecture,Model Architecture,named-entity-recognition,8,,,,5,0.0684931506849315,83,0.2144702842377261,14,0.17073170731707318,1,0,
85,"We primarily report results on two model sizes : BERT BASE ( L=12 , H = 768 , A = 12 , Total Param-eters=110M ) and BERT LARGE ( L=24 , H = 1024 , A = 16 , Total Parameters=340M ) .",Model Architecture,Model Architecture,named-entity-recognition,8,,,,6,0.0821917808219178,84,0.21705426356589147,15,0.18292682926829268,1,0,
86,BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes .,Model Architecture,Model Architecture,named-entity-recognition,8,,,,7,0.0958904109589041,85,0.21963824289405684,16,0.1951219512195122,1,0,
87,"Critically , however , the BERT Transformer uses bidirectional self - attention , while the GPT Transformer uses constrained self - attention where every token can only attend to context to its left .",Model Architecture,Model Architecture,named-entity-recognition,8,,,,8,0.1095890410958904,86,0.2222222222222222,17,0.2073170731707317,1,0,
88,4,Model Architecture,Model Architecture,named-entity-recognition,8,,,,9,0.1232876712328767,87,0.2248062015503876,18,0.21951219512195122,1,0,
89,Input / Output Representations,Model Architecture,,named-entity-recognition,8,,,,10,0.136986301369863,88,0.22739018087855298,19,0.23170731707317074,1,0,
90,"To make BERT handle a variety of down - stream tasks , our input representation is able to unambiguously represent both a single sentence and a pair of sentences ( e.g. , Question , Answer ) in one token sequence .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,11,0.1506849315068493,89,0.22997416020671835,20,0.24390243902439024,1,0,
91,"Throughout this work , a "" sentence "" can bean arbitrary span of contiguous text , rather than an actual linguistic sentence .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,12,0.1643835616438356,90,0.23255813953488372,21,0.25609756097560976,1,0,
92,"A "" sequence "" refers to the input token sequence to BERT , which maybe a single sentence or two sentences packed together .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,13,0.1780821917808219,91,0.2351421188630491,22,0.2682926829268293,1,0,
93,We use WordPiece embeddings,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,14,0.1917808219178082,92,0.23772609819121446,23,0.2804878048780488,1,0,
94,"( Wu et al. , 2016 ) with a 30,000 token vocabulary .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,15,0.2054794520547945,93,0.24031007751937986,24,0.2926829268292683,1,0,
95,The first token of every sequence is always a special classification token ( [ CLS ] ) .,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,16,0.2191780821917808,94,0.24289405684754523,25,0.3048780487804878,1,0,
96,The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks .,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,17,0.2328767123287671,95,0.2454780361757106,26,0.3170731707317073,1,0,
97,Sentence pairs are packed together into a single sequence .,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,18,0.2465753424657534,96,0.24806201550387597,27,0.32926829268292684,1,0,
98,We differentiate the sentences in two ways .,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,19,0.2602739726027397,97,0.25064599483204136,28,0.34146341463414637,1,0,
99,"First , we separate them with a special token ( [ SEP ] ) .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,20,0.273972602739726,98,0.2532299741602067,29,0.35365853658536583,1,0,
100,"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,21,0.2876712328767123,99,0.2558139534883721,30,0.36585365853658536,1,0,
101,"As shown in , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ? R H , and the final hidden vector for the i th input token as",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,22,0.3013698630136986,100,0.25839793281653745,31,0.3780487804878049,1,0,
102,"For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,23,0.3150684931506849,101,0.26098191214470284,32,0.3902439024390244,1,0,
103,"A visualization of this construction can be seen in . ( 2018 ) , we do not use traditional left - to - right or right - to - left language models to pre-train BERT .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,24,0.3287671232876712,102,0.26356589147286824,33,0.4024390243902439,1,0,
104,"Instead , we pre-train BERT using two unsupervised tasks , described in this section .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,25,0.3424657534246575,103,0.2661498708010336,34,0.4146341463414634,1,0,
105,This step is presented in the left part of .,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,26,0.3561643835616438,104,0.268733850129199,35,0.4268292682926829,1,0,
106,Task # 1 : Masked LM,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,27,0.3698630136986301,105,0.2713178294573643,36,0.43902439024390244,1,0,
107,"Intuitively , it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left - to - right model or the shallow concatenation of a left - toright and a right - to - left model .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,28,0.3835616438356164,106,0.2739018087855297,37,0.45121951219512196,1,0,
108,"Unfortunately , standard conditional language models can only be trained left - to - right or right - to - left , since bidirectional conditioning would allow each word to indirectly "" see itself "" , and the model could trivially predict the target word in a multi - layered context .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,29,0.3972602739726027,107,0.27648578811369506,38,0.4634146341463415,1,0,
109,"former is often referred to as a "" Transformer encoder "" while the left - context - only version is referred to as a "" Transformer decoder "" since it can be used for text generation .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,30,0.410958904109589,108,0.27906976744186046,39,0.47560975609756095,1,0,
110,"In order to train a deep bidirectional representation , we simply mask some percentage of the input tokens at random , and then predict those masked tokens .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,31,0.4246575342465753,109,0.28165374677002586,40,0.4878048780487805,1,0,
111,"We refer to this procedure as a "" masked LM "" ( MLM ) , although it is often referred to as a Cloze task in the literature .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,32,0.4383561643835616,110,0.2842377260981912,41,0.5,1,0,
112,"In this case , the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary , as in a standard LM .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,33,0.4520547945205479,111,0.2868217054263566,42,0.5121951219512195,1,0,
113,"In all of our experiments , we mask 15 % of all WordPiece tokens in each sequence at random .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,34,0.4657534246575342,112,0.28940568475452194,43,0.524390243902439,1,0,
114,"In contrast to denoising auto - encoders , we only predict the masked words rather than reconstructing the entire input .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,35,0.4794520547945205,113,0.29198966408268734,44,0.5365853658536586,1,0,
115,"Although this allows us to obtain a bidirectional pre-trained model , a downside is that we are creating a mismatch between pre-training and fine - tuning , since the [ MASK ] token does not appear during fine - tuning .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,36,0.4931506849315068,114,0.29457364341085274,45,0.5487804878048781,1,0,
116,"To mitigate this , we do not always replace "" masked "" words with the actual [ MASK ] token .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,37,0.5068493150684932,115,0.2971576227390181,46,0.5609756097560976,1,0,
117,The training data generator chooses 15 % of the token positions at random for prediction .,Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,38,0.5205479452054794,116,0.2997416020671835,47,0.573170731707317,1,0,
118,"If the i - th token is chosen , we replace the i - th token with ( 1 ) the [ MASK ] token 80 % of the time ( 2 ) a random token 10 % of the time ( 3 ) the unchanged i - th token 10 % of the time .",Model Architecture,Input / Output Representations,named-entity-recognition,8,,,,39,0.5342465753424658,117,0.3023255813953488,48,0.5853658536585366,1,0,
119,"Then ,",Model Architecture,,named-entity-recognition,8,,,,40,0.547945205479452,118,0.3049095607235142,49,0.5975609756097561,1,0,
120,Ti will be used to predict the original token with cross entropy loss .,Model Architecture,"Then ,",named-entity-recognition,8,,,,41,0.5616438356164384,119,0.30749354005167956,50,0.6097560975609756,1,0,
121,We compare variations of this procedure in Appendix C.2 .,Model Architecture,"Then ,",named-entity-recognition,8,,,,42,0.5753424657534246,120,0.31007751937984496,51,0.6219512195121951,1,0,
122,"Task # 2 : Next Sentence Prediction ( NSP ) Many important downstream tasks such as Question Answering ( QA ) and Natural Language Inference ( NLI ) are based on understanding the relationship between two sentences , which is not directly captured by language modeling .",Model Architecture,"Then ,",named-entity-recognition,8,,,,43,0.589041095890411,121,0.31266149870801035,52,0.6341463414634146,1,0,
123,"In order to train a model that understands sentence relationships , we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus .",Model Architecture,"Then ,",named-entity-recognition,8,,,,44,0.6027397260273972,122,0.3152454780361757,53,0.6463414634146342,1,0,
124,"Specifically , when choosing the sentences A and B for each pretraining example , 50 % of the time B is the actual next sentence that follows A ( labeled as IsNext ) , and 50 % of the time it is a random sentence from the corpus ( labeled as NotNext ) .",Model Architecture,"Then ,",named-entity-recognition,8,,,,45,0.6164383561643836,123,0.3178294573643411,54,0.6585365853658537,1,0,
125,"As we show in , C is used for next sentence prediction ( NSP ) .",Model Architecture,"Then ,",named-entity-recognition,8,,,,46,0.6301369863013698,124,0.32041343669250644,55,0.6707317073170732,1,0,
126,"5 Despite its simplicity , we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI .",Model Architecture,"Then ,",named-entity-recognition,8,,,,47,0.6438356164383562,125,0.32299741602067183,56,0.6829268292682927,1,0,
127,6,Model Architecture,"Then ,",named-entity-recognition,8,,,,48,0.6575342465753424,126,0.32558139534883723,57,0.6951219512195121,1,0,
128,5 The final model achieves 97 % - 98 % accuracy on NSP .,Model Architecture,"Then ,",named-entity-recognition,8,,,,49,0.6712328767123288,127,0.3281653746770026,58,0.7073170731707317,1,0,
129,"The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",Model Architecture,"Then ,",named-entity-recognition,8,,,,50,0.684931506849315,128,0.330749354005168,59,0.7195121951219512,1,0,
130,he likes play ##ing my dog is cute Input,Model Architecture,"Then ,",named-entity-recognition,8,,,,51,0.6986301369863014,129,0.3333333333333333,60,0.7317073170731707,1,0,
131,Position,Model Architecture,,named-entity-recognition,8,,,,52,0.7123287671232876,130,0.3359173126614987,61,0.7439024390243902,1,0,
132,Embeddings : BERT input representation .,Model Architecture,Position,named-entity-recognition,8,,,,53,0.726027397260274,131,0.3385012919896641,62,0.7560975609756098,1,0,
133,"The input embeddings are the sum of the token embeddings , the segmentation embeddings and the position embeddings .",Model Architecture,Position,named-entity-recognition,8,,,,54,0.7397260273972602,132,0.34108527131782945,63,0.7682926829268293,1,0,
134,The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee ( 2018 ) .,Model Architecture,Position,named-entity-recognition,8,,,,55,0.7534246575342466,133,0.34366925064599485,64,0.7804878048780488,1,0,
135,"However , in prior work , only sentence embeddings are transferred to down - stream tasks , where BERT transfers all parameters to initialize end - task model parameters .",Model Architecture,Position,named-entity-recognition,8,,,,56,0.7671232876712328,134,0.3462532299741602,65,0.7926829268292683,1,0,
136,Pre-training data,Model Architecture,Position,named-entity-recognition,8,,,,57,0.7808219178082192,135,0.3488372093023256,66,0.8048780487804879,1,0,
137,The pre-training procedure largely follows the existing literature on language model pre-training .,Model Architecture,Position,named-entity-recognition,8,,,,58,0.7945205479452054,136,0.35142118863049093,67,0.8170731707317073,1,0,
138,"For the pre-training corpus we use the Books Corpus ( 800M words ) and English Wikipedia ( 2,500 M words ) .",Model Architecture,Position,named-entity-recognition,8,,,,59,0.8082191780821918,137,0.35400516795865633,68,0.8292682926829268,1,0,
139,"For Wikipedia we extract only the text passages and ignore lists , tables , and headers .",Model Architecture,Position,named-entity-recognition,8,,,,60,0.821917808219178,138,0.35658914728682173,69,0.8414634146341463,1,0,
140,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,Model Architecture,Position,named-entity-recognition,8,,,,61,0.8356164383561644,139,0.35917312661498707,70,0.8536585365853658,1,0,
141,Fine- tuning BERT,Model Architecture,Position,named-entity-recognition,8,,,,62,0.8493150684931506,140,0.36175710594315247,71,0.8658536585365854,1,0,
142,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,Model Architecture,Position,named-entity-recognition,8,,,,63,0.863013698630137,141,0.3643410852713178,72,0.8780487804878049,1,0,
143,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",Model Architecture,Position,named-entity-recognition,8,,,,64,0.8767123287671232,142,0.3669250645994832,73,0.8902439024390244,1,0,
144,"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",Model Architecture,Position,named-entity-recognition,8,,,,65,0.8904109589041096,143,0.3695090439276486,74,0.9024390243902439,1,0,
145,"For each task , we simply plugin the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end .",Model Architecture,Position,named-entity-recognition,8,,,,66,0.9041095890410958,144,0.37209302325581395,75,0.9146341463414634,1,0,
146,"At the input , sentence A and sentence B from pre-training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text -?",Model Architecture,Position,named-entity-recognition,8,,,,67,0.9178082191780822,145,0.37467700258397935,76,0.926829268292683,1,0,
147,pair in text classification or sequence tagging .,Model Architecture,Position,named-entity-recognition,8,,,,68,0.9315068493150684,146,0.3772609819121447,77,0.9390243902439024,1,0,
148,"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .",Model Architecture,Position,named-entity-recognition,8,,,,69,0.9452054794520548,147,0.3798449612403101,78,0.9512195121951219,1,0,
149,"Compared to pre-training , fine - tuning is relatively inexpensive .",Model Architecture,Position,named-entity-recognition,8,,,,70,0.958904109589041,148,0.38242894056847543,79,0.9634146341463414,1,0,
150,"All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre-trained model .",Model Architecture,Position,named-entity-recognition,8,,,,71,0.9726027397260274,149,0.3850129198966408,80,0.975609756097561,1,0,
151,We describe the task - specific details in the corresponding subsections of Section 4 .,Model Architecture,Position,named-entity-recognition,8,,,,72,0.9863013698630136,150,0.3875968992248062,81,0.9878048780487805,1,0,
152,More details can be found in Appendix A.5 .,Model Architecture,Position,named-entity-recognition,8,,,,73,1.0,151,0.39018087855297157,82,1.0,1,0,
153,Experiments,,,named-entity-recognition,8,,,,0,0.0,152,0.39276485788113696,0,0.0,1,0,
154,"In this section , we present BERT fine - tuning results on 11 NLP tasks .",Experiments,Experiments,named-entity-recognition,8,,,,1,0.014285714285714285,153,0.3953488372093023,1,0.0,1,0,
155,GLUE,Experiments,,named-entity-recognition,8,['B'],['B-n'],['B-b'],2,0.02857142857142857,154,0.3979328165374677,0,0.0,1,1,tasks
156,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.04285714285714286,155,0.4005167958656331,1,0.14285714285714285,1,1,tasks
157,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,Experiments,GLUE,named-entity-recognition,8,,,,4,0.05714285714285714,156,0.40310077519379844,2,0.2857142857142857,1,0,
158,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ?",Experiments,GLUE,named-entity-recognition,8,,,,5,0.07142857142857142,157,0.40568475452196384,3,0.42857142857142855,1,0,
159,R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .,Experiments,GLUE,named-entity-recognition,8,,,,6,0.08571428571428572,158,0.4082687338501292,4,0.5714285714285714,1,0,
160,The only new parameters introduced during fine - tuning are classification layer weights W ?,Experiments,GLUE,named-entity-recognition,8,,,,7,0.1,159,0.4108527131782946,5,0.7142857142857143,1,0,
161,"R KH , where K is the number of labels .",Experiments,GLUE,named-entity-recognition,8,,,,8,0.11428571428571428,160,0.4134366925064599,6,0.8571428571428571,1,0,
162,"We compute a standard classification loss with C and W , i.e. , log ( softmax ( CW T ) ) . :",Experiments,GLUE,named-entity-recognition,8,,,,9,0.12857142857142856,161,0.4160206718346253,7,1.0,1,0,
163,GLUE,Experiments,,named-entity-recognition,8,,,,10,0.14285714285714285,162,0.4186046511627907,0,0.0,1,0,tasks
164,"Test results , scored by the evaluation server ( https://gluebenchmark.com/leaderboard ) .",Experiments,GLUE,named-entity-recognition,8,,,,11,0.15714285714285714,163,0.42118863049095606,1,0.019230769230769232,1,0,
165,The number below each task denotes the number of training examples .,Experiments,GLUE,named-entity-recognition,8,,,,12,0.17142857142857143,164,0.42377260981912146,2,0.038461538461538464,1,0,
166,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .",Experiments,GLUE,named-entity-recognition,8,,,,13,0.18571428571428572,165,0.4263565891472868,3,0.057692307692307696,1,0,
167,"8 BERT and OpenAI GPT are singlemodel , single task .",Experiments,GLUE,named-entity-recognition,8,,,,14,0.2,166,0.4289405684754522,4,0.07692307692307693,1,0,
168,"F1 scores are reported for QQP and MRPC , Spearman correlations are reported for STS - B , and accuracy scores are reported for the other tasks .",Experiments,GLUE,named-entity-recognition,8,,,,15,0.21428571428571427,167,0.4315245478036176,5,0.09615384615384616,1,0,
169,We exclude entries that use BERT as one of their components .,Experiments,GLUE,named-entity-recognition,8,,,,16,0.22857142857142856,168,0.43410852713178294,6,0.11538461538461539,1,0,
170,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-p', 'I-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.24285714285714285,169,0.43669250645994834,7,0.1346153846153846,1,1,tasks
171,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",18,0.2571428571428571,170,0.4392764857881137,8,0.15384615384615385,1,1,tasks
172,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,0.2714285714285714,171,0.4418604651162791,9,0.17307692307692307,1,1,tasks
173,"With random restarts , we use the same pre-trained checkpoint but perform different fine - tuning data shuffling and classifier layer initialization .",Experiments,GLUE,named-entity-recognition,8,,,,20,0.2857142857142857,172,0.4444444444444444,10,0.19230769230769232,1,0,
174,Results are presented in .,Experiments,GLUE,named-entity-recognition,8,,,,21,0.3,173,0.4470284237726098,11,0.21153846153846154,1,0,
175,"Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .",Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",22,0.3142857142857143,174,0.4496124031007752,12,0.23076923076923078,1,1,tasks
176,Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking .,Experiments,GLUE,named-entity-recognition,8,,,,23,0.32857142857142857,175,0.45219638242894056,13,0.25,1,0,
177,"For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .",Experiments,GLUE,named-entity-recognition,8,,,,24,0.34285714285714286,176,0.45478036175710596,14,0.2692307692307692,1,0,
178,"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",Experiments,GLUE,named-entity-recognition,8,,,,25,0.35714285714285715,177,0.4573643410852713,15,0.28846153846153844,1,0,
179,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",Experiments,GLUE,named-entity-recognition,8,"['O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",26,0.37142857142857144,178,0.4599483204134367,16,0.3076923076923077,1,1,tasks
180,The effect of model size is explored more thoroughly in Section 5.2 .,Experiments,GLUE,named-entity-recognition,8,,,,27,0.38571428571428573,179,0.4625322997416021,17,0.3269230769230769,1,0,
181,SQuAD v 1.1,Experiments,GLUE,named-entity-recognition,8,"['B', 'I', 'I']","['B-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b']",28,0.4,180,0.46511627906976744,18,0.34615384615384615,1,1,tasks
182,The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",29,0.4142857142857143,181,0.46770025839793283,19,0.36538461538461536,1,1,tasks
183,"Given a question and a passage from The GLUE data set distribution does not include the Test labels , and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE .",Experiments,GLUE,named-entity-recognition,8,,,,30,0.42857142857142855,182,0.4702842377260982,20,0.38461538461538464,1,0,
184,10 https://gluebenchmark.com/leaderboard,Experiments,GLUE,named-entity-recognition,8,,,,31,0.44285714285714284,183,0.4728682170542636,21,0.40384615384615385,1,0,
185,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",Experiments,GLUE,named-entity-recognition,8,,,,32,0.45714285714285713,184,0.4754521963824289,22,0.4230769230769231,1,0,
186,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",Experiments,GLUE,named-entity-recognition,8,,,,33,0.4714285714285714,185,0.4780361757105943,23,0.4423076923076923,1,0,
187,We only introduce a start vector S ?,Experiments,GLUE,named-entity-recognition,8,,,,34,0.4857142857142857,186,0.4806201550387597,24,0.46153846153846156,1,0,
188,R H and an end vector E ?,Experiments,GLUE,named-entity-recognition,8,,,,35,0.5,187,0.48320413436692505,25,0.4807692307692308,1,0,
189,R H during fine - tuning .,Experiments,GLUE,named-entity-recognition,8,,,,36,0.5142857142857142,188,0.48578811369509045,26,0.5,1,0,
190,The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph : P i = e ST i j e ST j .,Experiments,GLUE,named-entity-recognition,8,,,,37,0.5285714285714286,189,0.4883720930232558,27,0.5192307692307693,1,0,
191,The analogous formula is used for the end of the answer span .,Experiments,GLUE,named-entity-recognition,8,,,,38,0.5428571428571428,190,0.4909560723514212,28,0.5384615384615384,1,0,
192,"The score of a candidate span from position i to position j is defined as ST i + ET j , and the maximum scoring span where j ?",Experiments,GLUE,named-entity-recognition,8,,,,39,0.5571428571428572,191,0.4935400516795866,29,0.5576923076923077,1,0,
193,i is used as a prediction .,Experiments,GLUE,named-entity-recognition,8,,,,40,0.5714285714285714,192,0.49612403100775193,30,0.5769230769230769,1,0,
194,The training objective is the sum of the log-likelihoods of the correct start and end positions .,Experiments,GLUE,named-entity-recognition,8,,,,41,0.5857142857142857,193,0.49870801033591733,31,0.5961538461538461,1,0,
195,We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",42,0.6,194,0.5012919896640827,32,0.6153846153846154,1,1,tasks
196,shows top leaderboard entries as well as results from top published systems .,Experiments,GLUE,named-entity-recognition,8,,,,43,0.6142857142857143,195,0.5038759689922481,33,0.6346153846153846,1,0,
197,"The top results from the SQuAD leaderboard do not have up - to - date public system descriptions available , 11 and are allowed to use any public data when training their systems .",Experiments,GLUE,named-entity-recognition,8,,,,44,0.6285714285714286,196,0.5064599483204134,34,0.6538461538461539,1,0,
198,"We therefore use modest data augmentation in our system by first fine - tuning on TriviaQA ( Joshi et al. , 2017 ) befor fine - tuning on SQuAD .",Experiments,GLUE,named-entity-recognition,8,,,,45,0.6428571428571429,197,0.5090439276485789,35,0.6730769230769231,1,0,
199,Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",46,0.6571428571428571,198,0.5116279069767442,36,0.6923076923076923,1,1,tasks
200,"In fact , our single BERT model outperforms the top ensemble system in terms of F1 score .",Experiments,GLUE,named-entity-recognition,8,,,,47,0.6714285714285714,199,0.5142118863049095,37,0.7115384615384616,1,0,
201,"Without Trivia QA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .",Experiments,GLUE,named-entity-recognition,8,,,,48,0.6857142857142857,200,0.5167958656330749,38,0.7307692307692307,1,0,
202,12,Experiments,GLUE,named-entity-recognition,8,,,,49,0.7,201,0.5193798449612403,39,0.75,1,0,
203,SQuAD v 2.0,Experiments,GLUE,named-entity-recognition,8,"['B', 'I', 'I']","['B-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b']",50,0.7142857142857143,202,0.5219638242894057,40,0.7692307692307693,1,1,tasks
204,"The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph , making the problem more realistic .",Experiments,GLUE,named-entity-recognition,8,,,,51,0.7285714285714285,203,0.524547803617571,41,0.7884615384615384,1,0,
205,We use a simple approach to extend the SQuAD v1.1 BERT model for this task .,Experiments,GLUE,named-entity-recognition,8,,,,52,0.7428571428571429,204,0.5271317829457365,42,0.8076923076923077,1,0,
206,We treat questions that do not have an answer as having an answer span with start and end at the [ CLS ] token .,Experiments,GLUE,named-entity-recognition,8,,,,53,0.7571428571428571,205,0.5297157622739018,43,0.8269230769230769,1,0,
207,The probability space for the start and end answer span positions is extended to include the position of the [ CLS ] token .,Experiments,GLUE,named-entity-recognition,8,,,,54,0.7714285714285715,206,0.5322997416020672,44,0.8461538461538461,1,0,
208,"For prediction , we compare the score of the no -answer span : s null = SC + EC to the score of the best non - null span The Trivia QA data we used consists of paragraphs from TriviaQA - Wiki formed of the first 400 tokens in documents , that contain at least one of the provided possible answers .",Experiments,GLUE,named-entity-recognition,8,,,,55,0.7857142857142857,207,0.5348837209302325,45,0.8653846153846154,1,0,
209,We predict a non-null answer when ?,Experiments,GLUE,named-entity-recognition,8,,,,56,0.8,208,0.537467700258398,46,0.8846153846153846,1,0,
210,"i , j > s null + ? , where the threshold ?",Experiments,GLUE,named-entity-recognition,8,,,,57,0.8142857142857143,209,0.5400516795865633,47,0.9038461538461539,1,0,
211,is selected on the dev set to maximize F 1 .,Experiments,GLUE,named-entity-recognition,8,,,,58,0.8285714285714286,210,0.5426356589147286,48,0.9230769230769231,1,0,
212,We did not use Trivia QA data for this model .,Experiments,GLUE,named-entity-recognition,8,,,,59,0.8428571428571429,211,0.5452196382428941,49,0.9423076923076923,1,0,
213,We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'O']","['O', 'B-p', 'I-p', 'I-p', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'O']",60,0.8571428571428571,212,0.5478036175710594,50,0.9615384615384616,1,1,tasks
214,"The results compared to prior leaderboard entries and top published work are shown in , excluding systems that use BERT as one of their components .",Experiments,GLUE,named-entity-recognition,8,,,,61,0.8714285714285714,213,0.5503875968992248,51,0.9807692307692307,1,0,
215,We observe a + 5.1 F1 improvement over the previous best system .,Experiments,GLUE,named-entity-recognition,8,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",62,0.8857142857142857,214,0.5529715762273901,52,1.0,1,1,tasks
216,SWAG,Experiments,,named-entity-recognition,8,['B'],['B-n'],['B-b'],63,0.9,215,0.5555555555555556,0,0.0,1,1,tasks
217,The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference .,Experiments,SWAG,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",64,0.9142857142857143,216,0.5581395348837209,1,0.14285714285714285,1,1,tasks
218,"Given a sentence , the task is to choose the most plausible continuation among four choices .",Experiments,SWAG,named-entity-recognition,8,,,,65,0.9285714285714286,217,0.5607235142118863,2,0.2857142857142857,1,0,
219,"When fine - tuning on the SWAG dataset , we construct four input sequences , each containing the concatenation of the given sentence ( sentence A ) and a possible continuation ( sentence B ) .",Experiments,SWAG,named-entity-recognition,8,,,,66,0.9428571428571428,218,0.5633074935400517,3,0.42857142857142855,1,0,
220,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,Experiments,SWAG,named-entity-recognition,8,,,,67,0.9571428571428572,219,0.5658914728682171,4,0.5714285714285714,1,0,
221,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,Experiments,SWAG,named-entity-recognition,8,"['O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O']",,,68,0.9714285714285714,220,0.5684754521963824,5,0.7142857142857143,1,1,tasks
222,Results are presented in .,Experiments,SWAG,named-entity-recognition,8,,,,69,0.9857142857142858,221,0.5710594315245479,6,0.8571428571428571,1,0,
223,BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % .,Experiments,SWAG,named-entity-recognition,8,"['B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'O']",,,70,1.0,222,0.5736434108527132,7,1.0,1,1,tasks
224,Ablation Studies,,,named-entity-recognition,8,,,,0,0.0,223,0.5762273901808785,0,0.0,1,0,
225,"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .",Ablation Studies,Ablation Studies,named-entity-recognition,8,,,,1,0.16666666666666666,224,0.5788113695090439,1,0.16666666666666666,1,0,
226,Additional : Ablation over the pre-training tasks using the BERT BASE architecture .,Ablation Studies,Ablation Studies,named-entity-recognition,8,,,,2,0.3333333333333333,225,0.5813953488372093,2,0.3333333333333333,1,0,
227,""" No NSP "" is trained without the next sentence prediction task .",Ablation Studies,Ablation Studies,named-entity-recognition,8,,,,3,0.5,226,0.5839793281653747,3,0.5,1,0,
228,""" LTR & No NSP "" is trained as a left - to - right LM without the next sentence prediction , like OpenAI GPT .",Ablation Studies,Ablation Studies,named-entity-recognition,8,,,,4,0.6666666666666666,227,0.58656330749354,4,0.6666666666666666,1,0,
229,""" + BiLSTM "" adds a randomly initialized BiLSTM on top of the "" LTR + No NSP "" model during fine - tuning .",Ablation Studies,Ablation Studies,named-entity-recognition,8,,,,5,0.8333333333333334,228,0.5891472868217055,5,0.8333333333333334,1,0,
230,ablation studies can be found in Appendix C.,Ablation Studies,Ablation Studies,named-entity-recognition,8,,,,6,1.0,229,0.5917312661498708,6,1.0,1,0,
231,Effect of Pre-training Tasks,,,named-entity-recognition,8,,,,0,0.0,230,0.5943152454780362,0,0.0,1,0,
232,"We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE :",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,1,0.05555555555555555,231,0.5968992248062015,1,0.05555555555555555,1,0,
233,No NSP :,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,2,0.1111111111111111,232,0.599483204134367,2,0.1111111111111111,1,0,
234,"A bidirectional model which is trained using the "" masked LM "" ( MLM ) but without the "" next sentence prediction "" ( NSP ) task .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,3,0.16666666666666666,233,0.6020671834625323,3,0.16666666666666666,1,0,
235,LTR & No NSP :,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,4,0.2222222222222222,234,0.6046511627906976,4,0.2222222222222222,1,0,
236,"A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,5,0.2777777777777778,235,0.6072351421188631,5,0.2777777777777778,1,0,
237,"The left - only constraint was also applied at fine - tuning , because removing it introduced a pre-train / fine - tune mismatch that degraded downstream performance .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,6,0.3333333333333333,236,0.6098191214470284,6,0.3333333333333333,1,0,
238,"Additionally , this model was pre-trained without the NSP task .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,7,0.3888888888888889,237,0.6124031007751938,7,0.3888888888888889,1,0,
239,"This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,8,0.4444444444444444,238,0.6149870801033591,8,0.4444444444444444,1,0,
240,We first examine the impact brought by the NSP task .,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,9,0.5,239,0.6175710594315246,9,0.5,1,0,
241,"In , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQu AD 1.1 .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,10,0.5555555555555556,240,0.6201550387596899,10,0.5555555555555556,1,0,
242,"Next , we evaluate the impact of training bidirectional representations by comparing "" No NSP "" to "" LTR & No NSP "" .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,11,0.6111111111111112,241,0.6227390180878553,11,0.6111111111111112,1,0,
243,"The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,12,0.6666666666666666,242,0.6253229974160207,12,0.6666666666666666,1,0,
244,"For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,13,0.7222222222222222,243,0.627906976744186,13,0.7222222222222222,1,0,
245,"In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,14,0.7777777777777778,244,0.6304909560723514,14,0.7777777777777778,1,0,
246,"This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,15,0.8333333333333334,245,0.6330749354005168,15,0.8333333333333334,1,0,
247,The BiLSTM hurts performance on the GLUE tasks .,Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,16,0.8888888888888888,246,0.6356589147286822,16,0.8888888888888888,1,0,
248,"We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,17,0.9444444444444444,247,0.6382428940568475,17,0.9444444444444444,1,0,
249,"However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non-intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .",Effect of Pre-training Tasks,Effect of Pre-training Tasks,named-entity-recognition,8,,,,18,1.0,248,0.6408268733850129,18,1.0,1,0,
250,Effect of Model Size,,,named-entity-recognition,8,"['B', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b', 'I-b']",0,0.0,249,0.6434108527131783,0,0.0,1,1,ablation-analysis
251,"In this section , we explore the effect of model size on fine - tuning task accuracy .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,1,0.037037037037037035,250,0.6459948320413437,1,0.037037037037037035,1,0,
252,"We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,2,0.07407407407407407,251,0.648578811369509,2,0.07407407407407407,1,0,
253,Results on selected GLUE tasks are shown in .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,3,0.1111111111111111,252,0.6511627906976745,3,0.1111111111111111,1,0,
254,"In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,4,0.14814814814814814,253,0.6537467700258398,4,0.14814814814814814,1,0,
255,"We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3,600 labeled training examples , and is substantially different from the pre-training tasks .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,5,0.18518518518518517,254,0.6563307493540051,5,0.18518518518518517,1,0,
256,It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,6,0.2222222222222222,255,0.6589147286821705,6,0.2222222222222222,1,0,
257,"For example , the largest Transformer explored in is ( L=6 , H = 1024 , A = 16 ) with 100M parameters for the encoder , and the largest Transformer we have found in the literature is ( L=64 , H = 512 , A=2 ) with 235M parameters .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,7,0.25925925925925924,256,0.661498708010336,7,0.25925925925925924,1,0,
258,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,8,0.2962962962962963,257,0.6640826873385013,8,0.2962962962962963,1,0,
259,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,9,0.3333333333333333,258,0.6666666666666666,9,0.3333333333333333,1,0,
260,"However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",10,0.37037037037037035,259,0.6692506459948321,10,0.37037037037037035,1,1,ablation-analysis
261,"presented mixed results on the downstream task impact of increasing the pre-trained bi - LM size from two to four layers and mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1,000 did not bring further improvements .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,11,0.4074074074074074,260,0.6718346253229974,11,0.4074074074074074,1,0,
262,"Both of these prior works used a featurebased approach - we hypothesize that when the model is fine - tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters , the taskspecific models can benefit from the larger , more expressive pre-trained representations even when downstream task data is very small .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,12,0.4444444444444444,261,0.6744186046511628,12,0.4444444444444444,1,0,
263,Feature - based Approach with BERT,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['B', 'I', 'I', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b']",13,0.48148148148148145,262,0.6770025839793282,13,0.48148148148148145,1,1,ablation-analysis
264,"All of the BERT results presented so far have used the fine - tuning approach , where a simple classification layer is added to the pre-trained model , and all parameters are jointly fine - tuned on a downstream task .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,14,0.5185185185185185,263,0.6795865633074936,14,0.5185185185185185,1,0,
265,"However , the feature - based approach , where fixed features are extracted from the pretrained model , has certain advantages .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,15,0.5555555555555556,264,0.6821705426356589,15,0.5555555555555556,1,0,
266,"First , not all tasks can be easily represented by a Transformer encoder architecture , and therefore require a task - specific model architecture to be added .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,16,0.5925925925925926,265,0.6847545219638242,16,0.5925925925925926,1,0,
267,"Second , there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,17,0.6296296296296297,266,0.6873385012919897,17,0.6296296296296297,1,0,
268,"In this section , we compare the two approaches by applying BERT to the CoNLL - 2003 Named Entity Recognition ( NER ) task .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,18,0.6666666666666666,267,0.689922480620155,18,0.6666666666666666,1,0,
269,"In the input to BERT , we use a case - preserving WordPiece model , and we include the maximal document context provided by the data .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,19,0.7037037037037037,268,0.6925064599483204,19,0.7037037037037037,1,0,
270,"Following standard practice , we formulate this as a tagging task but do not use a CRF layer in the output .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,20,0.7407407407407407,269,0.6950904392764858,20,0.7407407407407407,1,0,
271,We use the representation of the first sub-token as the input to the token - level classifier over the NER label set .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,21,0.7777777777777778,270,0.6976744186046512,21,0.7777777777777778,1,0,
272,"To ablate the fine - tuning approach , we apply the feature - based approach by extracting the activations from one or more layers without fine - tuning any parameters of BERT .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,22,0.8148148148148148,271,0.7002583979328165,22,0.8148148148148148,1,0,
273,These contextual embeddings are used as input to a randomly initialized two - layer 768 - dimensional BiLSTM before the classification layer .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,23,0.8518518518518519,272,0.7028423772609819,23,0.8518518518518519,1,0,
274,Results are presented in .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,24,0.8888888888888888,273,0.7054263565891473,24,0.8888888888888888,1,0,
275,BERT LARGE performs competitively with state - of - the - art methods .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",25,0.9259259259259259,274,0.7080103359173127,25,0.9259259259259259,1,1,ablation-analysis
276,"The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .",Effect of Model Size,Effect of Model Size,named-entity-recognition,8,,,,26,0.9629629629629629,275,0.710594315245478,26,0.9629629629629629,1,0,
277,This demonstrates that BERT is effective for both finetuning and feature - based approaches .,Effect of Model Size,Effect of Model Size,named-entity-recognition,8,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",27,1.0,276,0.7131782945736435,27,1.0,1,1,ablation-analysis
278,Conclusion,,,named-entity-recognition,8,,,,0,0.0,277,0.7157622739018088,0,0.0,1,0,
279,"Recent empirical improvements due to transfer learning with language models have demonstrated that rich , unsupervised pre-training is an integral part of many language understanding systems .",Conclusion,Conclusion,named-entity-recognition,8,,,,1,0.012048192771084338,278,0.7183462532299741,1,0.1111111111111111,0,0,
280,"In particular , these results enable even low - resource tasks to benefit from deep unidirectional architectures .",Conclusion,Conclusion,named-entity-recognition,8,,,,2,0.024096385542168676,279,0.7209302325581395,2,0.2222222222222222,0,0,
281,"Our major contribution is further generalizing these findings to deep bidirectional architectures , allowing the same pre-trained model to successfully tackle a broad set of NLP tasks .",Conclusion,Conclusion,named-entity-recognition,8,,,,3,0.03614457831325301,280,0.7235142118863049,3,0.3333333333333333,0,0,
282,We organize the appendix into three sections :,Conclusion,Conclusion,named-entity-recognition,8,,,,4,0.04819277108433735,281,0.7260981912144703,4,0.4444444444444444,0,0,
283,Additional implementation details for BERT are presented in Appendix A ;,Conclusion,Conclusion,named-entity-recognition,8,,,,5,0.060240963855421686,282,0.7286821705426356,5,0.5555555555555556,0,0,
284,Additional details for our experiments are presented in Appendix B ; and,Conclusion,Conclusion,named-entity-recognition,8,,,,6,0.07228915662650602,283,0.7312661498708011,6,0.6666666666666666,0,0,
285,Additional ablation studies are presented in Appendix C.,Conclusion,Conclusion,named-entity-recognition,8,,,,7,0.08433734939759036,284,0.7338501291989664,7,0.7777777777777778,0,0,
286,We present additional ablation studies for BERT including :,Conclusion,Conclusion,named-entity-recognition,8,,,,8,0.0963855421686747,285,0.7364341085271318,8,0.8888888888888888,0,0,
287,- Effect of Number of Training Steps ; and - Ablation for Different Masking Procedures .,Conclusion,Conclusion,named-entity-recognition,8,,,,9,0.10843373493975904,286,0.7390180878552972,9,1.0,0,0,
288,A Additional Details for BERT,Conclusion,,named-entity-recognition,8,,,,10,0.12048192771084337,287,0.7416020671834626,0,0.0,0,0,
289,A.1 Illustration of the Pre-training Tasks,Conclusion,,named-entity-recognition,8,,,,11,0.13253012048192772,288,0.7441860465116279,1,0.0136986301369863,0,0,
290,We provide examples of the pre-training tasks in the following .,Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,12,0.14457831325301204,289,0.7467700258397932,2,0.0273972602739726,0,0,
291,"Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy , and during the random masking procedure we chose the 4 - th token ( which corresponding to hairy ) , our masking procedure can be further illustrated by The purpose of this is to bias the representation towards the actual observed word .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,13,0.1566265060240964,290,0.7493540051679587,3,0.0410958904109589,0,0,
292,"The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predictor which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,14,0.1686746987951807,291,0.751937984496124,4,0.0547945205479452,0,0,
293,"Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,15,0.18072289156626506,292,0.7545219638242894,5,0.0684931506849315,0,0,
294,"In Section C.2 , we evaluate the impact this procedure .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,16,0.1927710843373494,293,0.7571059431524548,6,0.0821917808219178,0,0,
295,"Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre-training steps maybe required for the model to converge .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,17,0.20481927710843373,294,0.7596899224806202,7,0.0958904109589041,0,0,
296,"In Section C.1 we demonstrate that MLM does converge marginally slower than a leftto - right model ( which predicts every token ) , but the empirical improvements of the MLM model far outweigh the increased training cost .",Conclusion,A.1 Illustration of the Pre-training Tasks,named-entity-recognition,8,,,,18,0.21686746987951808,295,0.7622739018087855,8,0.1095890410958904,0,0,
297,Next Sentence Prediction,Conclusion,,named-entity-recognition,8,,,,19,0.2289156626506024,296,0.7648578811369509,9,0.1232876712328767,0,0,
298,The next sentence prediction task can be illustrated in the following examples .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,20,0.24096385542168675,297,0.7674418604651163,10,0.136986301369863,0,0,
299,"To generate each training input sequence , we sample two spans of text from the corpus , which we refer to as "" sentences "" even though they are typically much longer than single sentences ( but can be shorter also ) .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,21,0.25301204819277107,298,0.7700258397932817,11,0.1506849315068493,0,0,
300,The first sentence receives the A embedding and the second receives the B embedding .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,22,0.26506024096385544,299,0.772609819121447,12,0.1643835616438356,0,0,
301,"50 % of the time B is the actual next sentence that follows A and 50 % of the time it is a random sentence , which is done for the "" next sentence prediction "" task .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,23,0.27710843373493976,300,0.7751937984496124,13,0.1780821917808219,0,0,
302,They are sampled such that the combined length is ? 512 tokens .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,24,0.2891566265060241,301,0.7777777777777778,14,0.1917808219178082,0,0,
303,"The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15 % , and no special consideration given to partial word pieces .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,25,0.30120481927710846,302,0.7803617571059431,15,0.2054794520547945,0,0,
304,"We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128,000 tokens / batch ) for 1,000,000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,26,0.3132530120481928,303,0.7829457364341085,16,0.2191780821917808,0,0,
305,"We use Adam with learning rate of 1 e - 4 , ? 1 = 0.9 , ? 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10,000 steps , and linear decay of the learning rate .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,27,0.3253012048192771,304,0.7855297157622739,17,0.2328767123287671,0,0,
306,We use a dropout probability of 0.1 on all layers .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,28,0.3373493975903614,305,0.7881136950904393,18,0.2465753424657534,0,0,
307,"We use a gelu activation rather than the standard relu , following OpenAI GPT .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,29,0.3493975903614458,306,0.7906976744186046,19,0.2602739726027397,0,0,
308,The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,30,0.3614457831325301,307,0.7932816537467701,20,0.273972602739726,0,0,
309,Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration ( 16 TPU chips total ) .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,31,0.37349397590361444,308,0.7958656330749354,21,0.2876712328767123,0,0,
310,13 Training of BERT LARGE was performed on 16 Cloud TPUs ( 64 TPU chips total ) .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,32,0.3855421686746988,309,0.7984496124031008,22,0.3013698630136986,0,0,
311,Each pretraining took 4 days to complete .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,33,0.39759036144578314,310,0.8010335917312662,23,0.3150684931506849,0,0,
312,Longer sequences are disproportionately expensive because attention is quadratic to the sequence length .,Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,34,0.40963855421686746,311,0.8036175710594315,24,0.3287671232876712,0,0,
313,"To speedup pretraing in our experiments , we pre-train the model with sequence length of 128 for 90 % of the steps .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,35,0.42168674698795183,312,0.8062015503875969,25,0.3424657534246575,0,0,
314,"Then , we train the rest 10 % of the steps of sequence of 512 to learn the positional embeddings .",Conclusion,Next Sentence Prediction,named-entity-recognition,8,,,,36,0.43373493975903615,313,0.8087855297157622,26,0.3561643835616438,0,0,
315,A.3 Fine- tuning Procedure,Conclusion,,named-entity-recognition,8,,,,37,0.4457831325301205,314,0.8113695090439277,27,0.3698630136986301,0,0,
316,"For fine - tuning , most model hyperparameters are the same as in pre-training , with the exception of the batch size , learning rate , and number of training epochs .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,38,0.4578313253012048,315,0.813953488372093,28,0.3835616438356164,0,0,
317,The dropout probability was always kept at 0.1 .,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,39,0.46987951807228917,316,0.8165374677002584,29,0.3972602739726027,0,0,
318,"The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks :",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,40,0.4819277108433735,317,0.8191214470284238,30,0.410958904109589,0,0,
319,"Batch size : 16 , 32",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,41,0.4939759036144578,318,0.8217054263565892,31,0.4246575342465753,0,0,
320,"Learning rate ( Adam ) : 5 e - 5 , 3 e - 5 , 2 e - 5 Number of epochs : 2 , 3 , 4",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,42,0.5060240963855421,319,0.8242894056847545,32,0.4383561643835616,0,0,
321,"We also observed that large data sets ( e.g. , 100 k + labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,43,0.5180722891566265,320,0.8268733850129198,33,0.4520547945205479,0,0,
322,"Fine - tuning is typically very fast , so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,44,0.5301204819277109,321,0.8294573643410853,34,0.4657534246575342,0,0,
323,"A.4 Comparison of BERT , ELMo , and",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,45,0.5421686746987951,322,0.8320413436692506,35,0.4794520547945205,0,0,
324,Open AI GPT,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,46,0.5542168674698795,323,0.834625322997416,36,0.4931506849315068,0,0,
325,"Here we studies the differences in recent popular representation learning models including ELMo , OpenAI GPT and BERT .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,47,0.5662650602409639,324,0.8372093023255814,37,0.5068493150684932,0,0,
326,The comparisons between the model architectures are shown visually in .,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,48,0.5783132530120482,325,0.8397932816537468,38,0.5205479452054794,0,0,
327,"Note that in addition to the architecture differences , BERT and OpenAI GPT are finetuning approaches , while ELMo is a feature - based approach .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,49,0.5903614457831325,326,0.8423772609819121,39,0.5342465753424658,0,0,
328,"The most comparable existing pre-training method to BERT is OpenAI GPT , which trains a left - to - right Transformer LM on a large text corpus .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,50,0.6024096385542169,327,0.8449612403100775,40,0.547945205479452,0,0,
329,"In fact , many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,51,0.6144578313253012,328,0.8475452196382429,41,0.5616438356164384,0,0,
330,"The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements , but we do note that there are several other differences between how BERT and GPT were trained :",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,52,0.6265060240963856,329,0.8501291989664083,42,0.5753424657534246,0,0,
331,"GPT is trained on the Books Corpus ( 800M words ) ; BERT is trained on the Books Corpus ( 800M words ) and Wikipedia ( 2,500 M words ) .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,53,0.6385542168674698,330,0.8527131782945736,43,0.589041095890411,0,0,
332,"GPT was trained for 1 M steps with a batch size of 32,000 words ; BERT was trained for 1 M steps with a batch size of 128,000 words .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,54,0.6506024096385542,331,0.8552971576227391,44,0.6027397260273972,0,0,
333,GPT used the same learning rate of 5 e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .,Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,55,0.6626506024096386,332,0.8578811369509044,45,0.6164383561643836,0,0,
334,"To isolate the effect of these differences , we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable .",Conclusion,A.3 Fine- tuning Procedure,named-entity-recognition,8,,,,56,0.6746987951807228,333,0.8604651162790697,46,0.6301369863013698,0,0,
335,A.5 Illustrations of Fine - tuning on Different Tasks,Conclusion,,named-entity-recognition,8,,,,57,0.6867469879518072,334,0.8630490956072352,47,0.6438356164383562,0,0,
336,The illustration of fine - tuning BERT on different tasks can be seen in .,Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,,,,58,0.6987951807228916,335,0.8656330749354005,48,0.6575342465753424,0,0,
337,"Our task - specific models are formed by incorporating BERT with one additional output layer , so a minimal number of parameters need to be learned from scratch .",Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,,,,59,0.7108433734939759,336,0.8682170542635659,49,0.6712328767123288,0,0,
338,"Among the tasks , ( a ) and MNLI Multi - Genre Natural Language Inference is a large - scale , crowdsourced entailment classification task .",Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,,,,60,0.7228915662650602,337,0.8708010335917312,50,0.684931506849315,0,0,
339,"Given a pair of sentences , the goal is to predict whether the second sentence is an entailment , contradiction , or neutral with respect to the first one .",Conclusion,A.5 Illustrations of Fine - tuning on Different Tasks,named-entity-recognition,8,,,,61,0.7349397590361446,338,0.8733850129198967,51,0.6986301369863014,0,0,
340,QQP,Conclusion,,named-entity-recognition,8,,,,62,0.7469879518072289,339,0.875968992248062,52,0.7123287671232876,0,0,
341,Quora Question,Conclusion,,named-entity-recognition,8,,,,63,0.7590361445783133,340,0.8785529715762274,53,0.726027397260274,0,0,
342,Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent .,Conclusion,Quora Question,named-entity-recognition,8,,,,64,0.7710843373493976,341,0.8811369509043928,54,0.7397260273972602,0,0,
343,BERT E [ CLS ],Conclusion,Quora Question,named-entity-recognition,8,,,,65,0.7831325301204819,342,0.8837209302325582,55,0.7534246575342466,0,0,
344,E 1 E ...,Conclusion,Quora Question,named-entity-recognition,8,,,,66,0.7951807228915663,343,0.8863049095607235,56,0.7671232876712328,0,0,
345,...,Conclusion,Quora Question,named-entity-recognition,8,,,,67,0.8072289156626506,344,0.8888888888888888,57,0.7808219178082192,0,0,
346,SST - 2,Conclusion,Quora Question,named-entity-recognition,8,,,,68,0.8192771084337349,345,0.8914728682170543,58,0.7945205479452054,0,0,
347,The Stanford Sentiment Treebank is a binary single - sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment .,Conclusion,Quora Question,named-entity-recognition,8,,,,69,0.8313253012048193,346,0.8940568475452196,59,0.8082191780821918,0,0,
348,CoLA,Conclusion,,named-entity-recognition,8,,,,70,0.8433734939759037,347,0.896640826873385,60,0.821917808219178,0,0,
349,"The Corpus of Linguistic Acceptability is a binary single - sentence classification task , where the goal is to predict whether an English sentence is linguistically "" acceptable "" or not .",Conclusion,CoLA,named-entity-recognition,8,,,,71,0.8554216867469879,348,0.8992248062015504,61,0.8356164383561644,0,0,
350,STS - B,Conclusion,CoLA,named-entity-recognition,8,,,,72,0.8674698795180723,349,0.9018087855297158,62,0.8493150684931506,0,0,
351,The Semantic Textual Similarity,Conclusion,,named-entity-recognition,8,,,,73,0.8795180722891566,350,0.9043927648578811,63,0.863013698630137,0,0,
352,Benchmark is a collection of sentence pairs drawn from news headlines and other sources .,Conclusion,The Semantic Textual Similarity,named-entity-recognition,8,,,,74,0.891566265060241,351,0.9069767441860465,64,0.8767123287671232,0,0,
353,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,Conclusion,The Semantic Textual Similarity,named-entity-recognition,8,,,,75,0.9036144578313253,352,0.9095607235142119,65,0.8904109589041096,0,0,
354,MRPC,Conclusion,,named-entity-recognition,8,,,,76,0.9156626506024096,353,0.9121447028423773,66,0.9041095890410958,0,0,
355,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",Conclusion,MRPC,named-entity-recognition,8,,,,77,0.927710843373494,354,0.9147286821705426,67,0.9178082191780822,0,0,
356,RTE,Conclusion,,named-entity-recognition,8,,,,78,0.9397590361445783,355,0.917312661498708,68,0.9315068493150684,0,0,
357,"Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ) .",Conclusion,RTE,named-entity-recognition,8,,,,79,0.9518072289156626,356,0.9198966408268734,69,0.9452054794520548,0,0,
358,14 WNLI Winograd NLI is a small natural language inference dataset .,Conclusion,RTE,named-entity-recognition,8,,,,80,0.963855421686747,357,0.9224806201550387,70,0.958904109589041,0,0,
359,"The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .",Conclusion,RTE,named-entity-recognition,8,,,,81,0.9759036144578314,358,0.9250645994832042,71,0.9726027397260274,0,0,
360,We therefore exclude this set to be fair to OpenAI GPT .,Conclusion,RTE,named-entity-recognition,8,,,,82,0.9879518072289156,359,0.9276485788113695,72,0.9863013698630136,0,0,
361,"For our GLUE submission , we always predicted the ma-jority class .",Conclusion,RTE,named-entity-recognition,8,,,,83,1.0,360,0.9302325581395349,73,1.0,0,0,
362,C Additional Ablation Studies,,,named-entity-recognition,8,,,,0,0.0,361,0.9328165374677002,0,0.0,1,0,
363,C.1 Effect of Number of Training Steps presents MNLI,C Additional Ablation Studies,,named-entity-recognition,8,,,,1,0.04,362,0.9354005167958657,1,0.09090909090909091,1,0,
364,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,2,0.08,363,0.937984496124031,2,0.18181818181818182,1,0,
365,This allows us to answer the following questions :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,3,0.12,364,0.9405684754521964,3,0.2727272727272727,1,0,
366,1 .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,4,0.16,365,0.9431524547803618,4,0.36363636363636365,1,0,
367,Question :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,5,0.2,366,0.9457364341085271,5,0.45454545454545453,1,0,
368,"Does BERT really need such a large amount of pre-training ( 128,000 words / batch * 1,000,000 steps ) to achieve high fine - tuning accuracy ?",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,6,0.24,367,0.9483204134366925,6,0.5454545454545454,1,0,
369,"Answer : Yes , BERT BASE achieves almost 1.0 % additional accuracy on MNLI when trained on 1 M steps compared to 500 k steps .",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,7,0.28,368,0.9509043927648578,7,0.6363636363636364,1,0,
370,2 . Question :,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,8,0.32,369,0.9534883720930233,8,0.7272727272727273,1,0,
371,"Does MLM pre-training converge slower than LTR pre-training , since only 15 % of words are predicted in each batch rather than every word ?",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,9,0.36,370,0.9560723514211886,9,0.8181818181818182,1,0,
372,Answer : The MLM model does converge slightly slower than the LTR model .,C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,10,0.4,371,0.958656330749354,10,0.9090909090909091,1,0,
373,"However , in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately .",C Additional Ablation Studies,C.1 Effect of Number of Training Steps presents MNLI,named-entity-recognition,8,,,,11,0.44,372,0.9612403100775194,11,1.0,1,0,
374,C.2 Ablation for Different Masking Procedures,C Additional Ablation Studies,,named-entity-recognition,8,,,,12,0.48,373,0.9638242894056848,0,0.0,1,0,
375,"In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model ( MLM ) objective .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,13,0.52,374,0.9664082687338501,1,0.07692307692307693,1,0,
376,The following is an ablation study to evaluate the effect of different masking strategies .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,14,0.56,375,0.9689922480620154,2,0.15384615384615385,1,0,
377,"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,15,0.6,376,0.9715762273901809,3,0.23076923076923078,1,0,
378,We report the Dev results for both MNLI and NER .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,16,0.64,377,0.9741602067183462,4,0.3076923076923077,1,0,
379,"For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,17,0.68,378,0.9767441860465116,5,0.38461538461538464,1,0,
380,The results are presented in .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,18,0.72,379,0.979328165374677,6,0.46153846153846156,1,0,
381,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,19,0.76,380,0.9819121447028424,7,0.5384615384615384,1,0,
382,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,20,0.8,381,0.9844961240310077,8,0.6153846153846154,1,0,
383,The right part of the paper represents the Dev set results .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,21,0.84,382,0.9870801033591732,9,0.6923076923076923,1,0,
384,"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,22,0.88,383,0.9896640826873385,10,0.7692307692307693,1,0,
385,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,23,0.92,384,0.9922480620155039,11,0.8461538461538461,1,0,
386,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,24,0.96,385,0.9948320413436692,12,0.9230769230769231,1,0,
387,"Interestingly , using only the RND strategy performs much worse than our strategy as well .",C Additional Ablation Studies,C.2 Ablation for Different Masking Procedures,named-entity-recognition,8,,,,25,1.0,386,0.9974160206718347,13,1.0,1,0,
1,title,,,named-entity-recognition,3,,,,0,0.0,0,0.0,0,0.0,1,0,
2,Semi-supervised sequence tagging with bidirectional language models,title,,named-entity-recognition,3,"['B', 'I', 'I', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",1,0.0,1,0.005405405405405406,1,0.0,1,1,research-problem
3,abstract,,,named-entity-recognition,3,,,,0,0.0,2,0.010810810810810811,0,0.0,1,0,
4,Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks .,abstract,abstract,named-entity-recognition,3,,,,1,0.25,3,0.016216216216216217,1,0.25,1,0,
5,"However , in most cases , the recurrent network that operates on word - level representations to produce context sensitive representations is trained on relatively little labeled data .",abstract,abstract,named-entity-recognition,3,,,,2,0.5,4,0.021621621621621623,2,0.5,1,0,
6,"In this paper , we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks .",abstract,abstract,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.75,5,0.02702702702702703,3,0.75,1,1,research-problem
7,"We evaluate our model on two standard datasets for named entity recognition ( NER ) and chunking , and in both cases achieve state of the art results , surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers .",abstract,abstract,named-entity-recognition,3,,,,4,1.0,6,0.032432432432432434,4,1.0,1,0,
8,Introduction,,,named-entity-recognition,3,,,,0,0.0,7,0.03783783783783784,0,0.0,1,0,
9,"Due to their simplicity and efficacy , pre-trained word embedding have become ubiquitous in NLP systems .",Introduction,Introduction,named-entity-recognition,3,,,,1,0.05555555555555555,8,0.043243243243243246,1,0.05555555555555555,1,0,
10,Many prior studies have shown that they capture useful semantic and syntactic information and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks .,Introduction,Introduction,named-entity-recognition,3,,,,2,0.1111111111111111,9,0.04864864864864865,2,0.1111111111111111,1,0,
11,"However , in many NLP tasks it is essential to represent not just the meaning of a word , but also the word in context .",Introduction,Introduction,named-entity-recognition,3,,,,3,0.16666666666666666,10,0.05405405405405406,3,0.16666666666666666,1,0,
12,"For example , in the two phrases "" A Central Bank spokesman "" and "" The Central African Republic "" , the word ' Central ' is used as part of both an Organization and Location .",Introduction,Introduction,named-entity-recognition,3,,,,4,0.2222222222222222,11,0.05945945945945946,4,0.2222222222222222,1,0,
13,"Accordingly , current state of the art sequence tagging models typically include a bidirectional re-current neural network ( RNN ) that encodes token sequences into a context sensitive representation before making token specific predictions .",Introduction,Introduction,named-entity-recognition,3,,,,5,0.2777777777777778,12,0.06486486486486487,5,0.2777777777777778,1,0,
14,"Although the token representation is initialized with pre-trained embeddings , the parameters of the bidirectional RNN are typically learned only on labeled data .",Introduction,Introduction,named-entity-recognition,3,,,,6,0.3333333333333333,13,0.07027027027027027,6,0.3333333333333333,1,0,
15,"Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks ( e.g. , .",Introduction,Introduction,named-entity-recognition,3,,,,7,0.3888888888888889,14,0.07567567567567568,7,0.3888888888888889,1,0,
16,"In this paper , we explore an alternate semisupervised approach which does not require additional labeled data .",Introduction,Introduction,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O']",8,0.4444444444444444,15,0.08108108108108109,8,0.4444444444444444,1,1,approach
17,"We use a neural language model ( LM ) , pre-trained on a large , unlabeled corpus to compute an encoding of the context at each position in the sequence ( hereafter an LM embedding ) and use it in the supervised sequence tagging model .",Introduction,Introduction,named-entity-recognition,3,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.5,16,0.08648648648648649,9,0.5,1,1,approach
18,"Since the LM embeddings are used to compute the probability of future words in a neural LM , they are likely to encode both the semantic and syntactic roles of words in context .",Introduction,Introduction,named-entity-recognition,3,,,,10,0.5555555555555556,17,0.0918918918918919,10,0.5555555555555556,1,0,
19,Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting .,Introduction,Introduction,named-entity-recognition,3,,,,11,0.6111111111111112,18,0.0972972972972973,11,0.6111111111111112,1,0,
20,"When we include the LM embeddings in our system over all performance increases from 90. 87 % to 91.93 % F 1 for the CoNLL 2003 NER task , a more then 1 % absolute F1 increase , and a substantial improvement over the previous state of the art .",Introduction,Introduction,named-entity-recognition,3,,,,12,0.6666666666666666,19,0.10270270270270271,12,0.6666666666666666,1,0,
21,We also establish a new state of the art result ( 96.37 % F 1 ) for the CoNLL 2000 Chunking task .,Introduction,Introduction,named-entity-recognition,3,,,,13,0.7222222222222222,20,0.10810810810810811,13,0.7222222222222222,1,0,
22,"As a secondary contribution , we show that using both forward and backward LM embeddings boosts performance over a forward only LM .",Introduction,Introduction,named-entity-recognition,3,,,,14,0.7777777777777778,21,0.11351351351351352,14,0.7777777777777778,1,0,
23,We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers .,Introduction,Introduction,named-entity-recognition,3,,,,15,0.8333333333333334,22,0.11891891891891893,15,0.8333333333333334,1,0,
24,The main components in our language - modelaugmented sequence tagger ( TagLM ) are illustrated in .,Introduction,Introduction,named-entity-recognition,3,,,,16,0.8888888888888888,23,0.12432432432432433,16,0.8888888888888888,1,0,
25,"After pre-training word embeddings and a neural LM on large , unlabeled corpora ( Step 1 ) , we extract the word and LM embeddings for every token in a given input sequence Step 2 ) and use them in the supervised sequence tagging model (",Introduction,Introduction,named-entity-recognition,3,,,,17,0.9444444444444444,24,0.12972972972972974,17,0.9444444444444444,1,0,
26,Step 3 ) .,Introduction,,named-entity-recognition,3,,,,18,1.0,25,0.13513513513513514,18,1.0,1,0,
27,Baseline sequence tagging model,,,named-entity-recognition,3,,,,0,0.0,26,0.14054054054054055,0,0.0,1,0,
28,"Our baseline sequence tagging model is a hierarchical neural tagging model , closely following a number of recent studies ) ( left side of ) .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,1,0.023809523809523808,27,0.14594594594594595,1,0.05,1,0,
29,"Given a sentence of tokens ( t 1 , t 2 , . . . , t N ) it first forms a representation , x k , for each token by concatenating a character based representation ck with a token embedding wk :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,2,0.047619047619047616,28,0.15135135135135136,2,0.1,1,0,
30,The character representation ck captures morphological information and is either a convolutional neural network ( CNN ) or RNN .,Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,3,0.07142857142857142,29,0.15675675675675677,3,0.15,1,0,
31,"It is parameterized by C ( , ? c ) with parameters ? c .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,4,0.09523809523809523,30,0.16216216216216217,4,0.2,1,0,
32,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,5,0.11904761904761904,31,0.16756756756756758,5,0.25,1,0,
33,"To learn a context sensitive representation , we employ multiple layers of bidirectional RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,6,0.14285714285714285,32,0.17297297297297298,6,0.3,1,0,
34,"For each token position , k , the hidden state h k , i of RNN layer i is formed by concatenating the hidden states from the forward ( ? ? h k , i ) and backward ( ? ? h k , i ) RNNs .",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,7,0.16666666666666666,33,0.1783783783783784,7,0.35,1,0,
35,"As a result , the bidirectional RNN is able to use both past and future information to make a prediction at token k.",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,8,0.19047619047619047,34,0.1837837837837838,8,0.4,1,0,
36,"More formally , for the first RNN layer that operates on x k to output h k,1 :",Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,9,0.21428571428571427,35,0.1891891891891892,9,0.45,1,0,
37,Step 2 : Prepare word embedding and LM embedding for each token in the input sequence .,Baseline sequence tagging model,Baseline sequence tagging model,named-entity-recognition,3,,,,10,0.23809523809523808,36,0.1945945945945946,10,0.5,1,0,
38,"Two representations of the word "" York """,Baseline sequence tagging model,,named-entity-recognition,3,,,,11,0.2619047619047619,37,0.2,11,0.55,1,0,
39,Step 3 : Use both word embeddings and LM embeddings in the sequence tagging model .,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,12,0.2857142857142857,38,0.20540540540540542,12,0.6,1,0,
40,New York is located :,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,13,0.30952380952380953,39,0.21081081081081082,13,0.65,1,0,
41,"The main components in TagLM , our language - model - augmented sequence tagging system .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,14,0.3333333333333333,40,0.21621621621621623,14,0.7,1,0,
42,The language model component ( in orange ) is used to augment the input token representation in a traditional sequence tagging models ( in grey ) .,Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,15,0.35714285714285715,41,0.22162162162162163,15,0.75,1,0,
43,"The second RNN layer is similar and uses h k , 1 to output h k ,2 .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,16,0.38095238095238093,42,0.22702702702702704,16,0.8,1,0,
44,"In this paper , we use L = 2 layers of RNNs in all experiments and parameterize R i as either Gated Recurrent Units ( GRU ) or Long Short - Term Memory units ( LSTM ) depending on the task .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,17,0.40476190476190477,43,0.23243243243243245,17,0.85,1,0,
45,"Finally , the output of the final RNN layer h k,L is used to predict a score for each possible tag using a single dense layer .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,18,0.42857142857142855,44,0.23783783783783785,18,0.9,1,0,
46,"Due to the dependencies between successive tags in our sequence labeling tasks ( e.g. using the BIOES labeling scheme , it is not possible for I - PER to follow B - LOC ) , it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,19,0.4523809523809524,45,0.24324324324324326,19,0.95,1,0,
47,"Accordingly , we add another layer with parameters for each label bigram , computing the sentence conditional random field ( CRF ) loss using the forward - backward algorithm at training time , and using the Viterbi algorithm to find the most likely tag sequence at test time , similar to Collobert et al .",Baseline sequence tagging model,"Two representations of the word "" York """,named-entity-recognition,3,,,,20,0.47619047619047616,46,0.24864864864864866,20,1.0,1,0,
48,Bidirectional LM,Baseline sequence tagging model,,named-entity-recognition,3,,,,21,0.5,47,0.25405405405405407,0,0.0,1,0,
49,"A language model computes the probability of a token sequence ( t 1 , t 2 , . . . , t N )",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,,,,22,0.5238095238095238,48,0.2594594594594595,1,0.09090909090909091,1,0,
50,"Recent state of the art neural language models ) use a similar architecture to our baseline sequence tagger where they pass a token representation ( either from a CNN over characters or as token embeddings ) through multiple layers of LSTMs to embed the history ( t 1 , t 2 , . . . , t k ) into a fixed dimensional vector ? ? h LM k .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,,,,23,0.5476190476190477,49,0.2648648648648649,2,0.18181818181818182,1,0,
51,This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,,,,24,0.5714285714285714,50,0.2702702702702703,3,0.2727272727272727,1,0,
52,"Finally , the language model predicts the probability of token t k + 1 using a softmax layer over words in the vocabulary .",Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,,,,25,0.5952380952380952,51,0.2756756756756757,4,0.36363636363636365,1,0,
53,The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,,,,26,0.6190476190476191,52,0.2810810810810811,5,0.45454545454545453,1,0,
54,A backward LM predicts the previous token given the future context .,Baseline sequence tagging model,Bidirectional LM,named-entity-recognition,3,,,,27,0.6428571428571429,53,0.2864864864864865,6,0.5454545454545454,1,0,
55,"Given a sentence with N tokens , it computes",Baseline sequence tagging model,,named-entity-recognition,3,,,,28,0.6666666666666666,54,0.2918918918918919,7,0.6363636363636364,1,0,
56,A backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ? ?,Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,,,,29,0.6904761904761905,55,0.2972972972972973,8,0.7272727272727273,1,0,
57,"h LM k , for the sequence ( t k , t k+1 , . . . , t N ) , the output embeddings of the top layer LSTM .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,,,,30,0.7142857142857143,56,0.3027027027027027,9,0.8181818181818182,1,0,
58,"In our final system , after pre-training the forward and backward LMs separately , we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings , i.e. , h LM",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,,,,31,0.7380952380952381,57,0.3081081081081081,10,0.9090909090909091,1,0,
59,"Note that in our formulation , the forward and backward LMs are independent , without any shared parameters .",Baseline sequence tagging model,"Given a sentence with N tokens , it computes",named-entity-recognition,3,,,,32,0.7619047619047619,58,0.31351351351351353,11,1.0,1,0,
60,Combining LM with sequence model,Baseline sequence tagging model,,named-entity-recognition,3,,,,33,0.7857142857142857,59,0.31891891891891894,0,0.0,1,0,
61,"Our combined system , TagLM , uses the LM embeddings as additional inputs to the sequence tagging model .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,34,0.8095238095238095,60,0.32432432432432434,1,0.1111111111111111,1,0,
62,"In particular , we concatenate the LM embeddings h LM with the output from one of the bidirectional RNN layers in the sequence model .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,35,0.8333333333333334,61,0.32972972972972975,2,0.2222222222222222,1,0,
63,"In our experiments , we found that introducing the LM embeddings at the output of the first layer performed the best .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,36,0.8571428571428571,62,0.33513513513513515,3,0.3333333333333333,1,0,
64,"More formally , we simply replace ( 2 ) with",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,37,0.8809523809523809,63,0.34054054054054056,4,0.4444444444444444,1,0,
65,There are alternate possibilities for adding the LM embeddings to the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,38,0.9047619047619048,64,0.34594594594594597,5,0.5555555555555556,1,0,
66,One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,39,0.9285714285714286,65,0.35135135135135137,6,0.6666666666666666,1,0,
67,where f is a non-linear function ) .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,40,0.9523809523809523,66,0.3567567567567568,7,0.7777777777777778,1,0,
68,Another possibility introduces an attention - like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model .,Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,41,0.9761904761904762,67,0.3621621621621622,8,0.8888888888888888,1,0,
69,"Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study , preferring to leave them for future work .",Baseline sequence tagging model,Combining LM with sequence model,named-entity-recognition,3,,,,42,1.0,68,0.3675675675675676,9,1.0,1,0,
70,Experiments,,,named-entity-recognition,3,,,,0,0.0,69,0.372972972972973,0,0.0,1,0,
71,"We evaluate our approach on two well benchmarked sequence tagging tasks , the CoNLL 2003 NER task and the CoNLL 2000 Chunking task .",Experiments,Experiments,named-entity-recognition,3,,,,1,0.05263157894736842,70,0.3783783783783784,1,0.023809523809523808,1,0,
72,We report the official evaluation metric ( micro - averaged F 1 ) .,Experiments,Experiments,named-entity-recognition,3,,,,2,0.10526315789473684,71,0.3837837837837838,2,0.047619047619047616,1,0,
73,"In both cases , we use the BIOES labeling scheme for the output tags , following previous work which showed it outperforms other options ( e.g. , ) .",Experiments,Experiments,named-entity-recognition,3,,,,3,0.15789473684210525,72,0.3891891891891892,3,0.07142857142857142,1,0,
74,"Following , we use the Senna word embeddings and pre-processed the text by lowercasing all tokens and replacing all digits with 0 .",Experiments,Experiments,named-entity-recognition,3,,,,4,0.21052631578947367,73,0.3945945945945946,4,0.09523809523809523,1,0,
75,CoNLL 2003 NER .,Experiments,,named-entity-recognition,3,"['B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'O']",5,0.2631578947368421,74,0.4,5,0.11904761904761904,1,1,tasks
76,"The CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types ( PER , LOC , ORG , MISC ) .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,,,,6,0.3157894736842105,75,0.40540540540540543,6,0.14285714285714285,1,0,
77,"It includes standard train , development and test sets .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,,,,7,0.3684210526315789,76,0.41081081081081083,7,0.16666666666666666,1,0,
78,Following previous work we trained on both the train and development sets after tuning hyperparameters on the development set .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,,,,8,0.42105263157894735,77,0.41621621621621624,8,0.19047619047619047,1,0,
79,The hyperparameters for our baseline model are similar to .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,,,,9,0.47368421052631576,78,0.42162162162162165,9,0.21428571428571427,1,0,
80,We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'O']",10,0.5263157894736842,79,0.42702702702702705,10,0.23809523809523808,1,1,tasks
81,The sequence layer uses two bidirectional GRUs with 300 hidden units each .,Experiments,CoNLL 2003 NER .,named-entity-recognition,3,"['O', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",11,0.5789473684210527,80,0.43243243243243246,11,0.2619047619047619,1,1,tasks
82,"For regularization , we add 25 % dropout to the input of each GRU , but not to the recurrent connections .",Experiments,CoNLL 2003 NER .,named-entity-recognition,3,"['O', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",12,0.631578947368421,81,0.43783783783783786,12,0.2857142857142857,1,1,tasks
83,CoNLL 2000 chunking .,Experiments,,named-entity-recognition,3,"['B', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'O']",13,0.6842105263157895,82,0.44324324324324327,13,0.30952380952380953,1,1,tasks
84,The CoNLL 2000 chunking task uses sections 15 - 18 from the Wall Street Journal corpus for training and section 20 for testing .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,,,,14,0.7368421052631579,83,0.4486486486486487,14,0.3333333333333333,1,0,
85,"It defines 11 syntactic chunk types ( e.g. , NP , VP , ADJP ) in addition to other .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,,,,15,0.7894736842105263,84,0.4540540540540541,15,0.35714285714285715,1,0,
86,We randomly sampled 1000 sentences from the training set as a held - out development set .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,,,,16,0.8421052631578947,85,0.4594594594594595,16,0.38095238095238093,1,0,
87,The baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",17,0.8947368421052632,86,0.4648648648648649,17,0.40476190476190477,1,1,tasks
88,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'B', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'B-p', 'I-p', 'O']","['O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'B-ob', 'B-p', 'I-p', 'O']",18,0.9473684210526315,87,0.4702702702702703,18,0.42857142857142855,1,1,tasks
89,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",Experiments,CoNLL 2000 chunking .,named-entity-recognition,3,"['O', 'O', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",19,1.0,88,0.4756756756756757,19,0.4523809523809524,1,1,tasks
90,Pre-trained language models .,,,named-entity-recognition,3,,,,0,0.0,89,0.4810810810810811,20,0.47619047619047616,1,0,
91,"The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark , a publicly available benchmark for largescale language modeling .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,1,0.045454545454545456,90,0.4864864864864865,21,0.5,1,0,
92,"The training split has approximately 800 million tokens , about a 4000X increase over the number training tokens in the CoNLL datasets .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,2,0.09090909090909091,91,0.4918918918918919,22,0.5238095238095238,1,0,
93,explored several model architectures and released their best single model and training recipes .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,3,0.13636363636363635,92,0.4972972972972973,23,0.5476190476190477,1,0,
94,"Following , they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,4,0.18181818181818182,93,0.5027027027027027,24,0.5714285714285714,1,0,
95,Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,5,0.22727272727272727,94,0.5081081081081081,25,0.5952380952380952,1,0,
96,"It uses a character CNN with 4096 filters for input , followed by two stacked LSTMs , each with 8192 hidden units and a 1024 dimensional projection layer .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,6,0.2727272727272727,95,0.5135135135135135,26,0.6190476190476191,1,0,
97,We use CNN - BIG - LSTM to refer to this language model in our results .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,7,0.3181818181818182,96,0.518918918918919,27,0.6428571428571429,1,0,
98,"In addition to CNN - BIG - LSTM from , 1 we used the same corpus to train two additional language models with fewer parameters : forward LSTM - 2048-512 and backward LSTM - 2048-512 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,8,0.36363636363636365,97,0.5243243243243243,28,0.6666666666666666,1,0,
99,Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,9,0.4090909090909091,98,0.5297297297297298,29,0.6904761904761905,1,0,
100,"We closely followed the procedure outlined in , except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,10,0.45454545454545453,99,0.5351351351351351,30,0.7142857142857143,1,0,
101,"The test set perplexities for our forward and backward LSTM - 2048 - 512 language models are 47.7 and 47.3 , respectively .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,11,0.5,100,0.5405405405405406,31,0.7380952380952381,1,0,
102,2,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,12,0.5454545454545454,101,0.5459459459459459,32,0.7619047619047619,1,0,
103,Model F 1 std 90.91 0.20 90.94 91.37 Our baseline without LM 90.87 0.13 TagLM 91.93 0.19 Training .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,13,0.5909090909090909,102,0.5513513513513514,33,0.7857142857142857,1,0,
104,"All experiments use the Adam optimizer ( Kingma and Ba , 2015 ) with gradient norms clipped at 5.0 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'B', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'I', 'B', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O']",14,0.6363636363636364,103,0.5567567567567567,34,0.8095238095238095,1,1,hyperparameters
105,"In all experiments , we fine tune the pre-trained Senna word embeddings but fix all weights in the pre-trained language models .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",15,0.6818181818181818,104,0.5621621621621622,35,0.8333333333333334,1,1,hyperparameters
106,"In addition to explicit dropout regularization , we also use early stopping to prevent over-fitting and use the following process to determine when to stop training .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",16,0.7272727272727273,105,0.5675675675675675,36,0.8571428571428571,1,1,hyperparameters
107,We first train with a constant learning rate ? = 0.001 on the training data and monitor the development set performance at each epoch .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",17,0.7727272727272727,106,0.572972972972973,37,0.8809523809523809,1,1,hyperparameters
108,"Then , at the epoch with the highest development performance , we start a simple learning rate annealing schedule : decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,18,0.8181818181818182,107,0.5783783783783784,38,0.9047619047619048,1,0,
109,"an order of magnitude ( i.e. , divide by ten ) , train for five epochs , decrease ?",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,19,0.8636363636363636,108,0.5837837837837838,39,0.9285714285714286,1,0,
110,"an order of magnitude again , train for five more epochs and stop .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,20,0.9090909090909091,109,0.5891891891891892,40,0.9523809523809523,1,0,
111,"Following , we train each final model configuration ten times with different random seeds and report the mean and standard deviation F 1 .",Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,21,0.9545454545454546,110,0.5945945945945946,41,0.9761904761904762,1,0,
112,It is important to estimate the variance of model performance since the test data sets are relatively small .,Pre-trained language models .,Pre-trained language models .,named-entity-recognition,3,,,,22,1.0,111,0.6,42,1.0,1,0,
113,Overall system results,,,named-entity-recognition,3,,,,0,0.0,112,0.6054054054054054,0,0.0,1,0,
114,Tables 1 and 2 compare results from Tag LM with previously published state of the art results without additional labeled data or task specific gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,,,,1,0.02631578947368421,113,0.6108108108108108,1,0.0,1,0,
115,compare results of,Overall system results,Overall system results,named-entity-recognition,3,,,,2,0.05263157894736842,114,0.6162162162162163,0,0.0,1,0,
116,Tag LM to other systems that include additional labeled data or gazetteers .,Overall system results,Overall system results,named-entity-recognition,3,,,,3,0.07894736842105263,115,0.6216216216216216,1,0.09090909090909091,1,0,
117,"In both tasks , Tag LM establishes a new state of the art using bidirectional LMs ( the forward CNN - BIG - LSTM and the backward LSTM - 2048 - 512 ) .",Overall system results,Overall system results,named-entity-recognition,3,,,,4,0.10526315789473684,116,0.6270270270270271,2,0.18181818181818182,1,0,
118,"In the CoNLL 2003 NER task , our model scores 91.93 mean F 1 , which is a statistically significant increase over the previous best result of 91.62 0.33 from that used gazetteers ( at 95 % , two - sided Welch t- test , p = 0.021 ) .",Overall system results,Overall system results,named-entity-recognition,3,"['B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.13157894736842105,117,0.6324324324324324,3,0.2727272727272727,1,1,results
119,"In the CoNLL 2000 Chunking task , Tag LM achieves 96.37 mean F 1 , exceeding all previously published results without additional labeled data by more then 1 % absolute F 1 .",Overall system results,Overall system results,named-entity-recognition,3,"['O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",6,0.15789473684210525,118,0.6378378378378379,4,0.36363636363636365,1,1,results
120,The improvement over the previous best result of 95.77 in that jointly trains with Penn Treebank ( PTB ) POS tags is statistically significant at 95 % ( p < 0.001 assuming standard deviation of 0.1 ) .,Overall system results,Overall system results,named-entity-recognition,3,,,,7,0.18421052631578946,119,0.6432432432432432,5,0.45454545454545453,1,0,
121,"Importantly , the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F 1 in the NER and Chunking tasks , respectively .",Overall system results,Overall system results,named-entity-recognition,3,,,,8,0.21052631578947367,120,0.6486486486486487,6,0.5454545454545454,1,0,
122,Adding external resources .,Overall system results,,named-entity-recognition,3,,,,9,0.23684210526315788,121,0.654054054054054,7,0.6363636363636364,1,0,
123,"Although we do not use external labeled data or gazetteers , we found that TagLM outperforms previous state of the art results in both tasks when external resources ( labeled data or task specific gazetteers ) are available .",Overall system results,Adding external resources .,named-entity-recognition,3,,,,10,0.2631578947368421,122,0.6594594594594595,8,0.7272727272727273,1,0,
124,"Furthermore , show that , in most cases , the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning .",Overall system results,Adding external resources .,named-entity-recognition,3,,,,11,0.2894736842105263,123,0.6648648648648648,9,0.8181818181818182,1,0,
125,"For example , noted an improvement of only 0.06 F 1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and reported an increase of 0.71 F 1 when adding gazetteers to their baseline .",Overall system results,Adding external resources .,named-entity-recognition,3,,,,12,0.3157894736842105,124,0.6702702702702703,10,0.9090909090909091,1,0,
126,"In the Chunking task , previous work has reported from 0.28 to 0.75 improvement in F 1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities .",Overall system results,Adding external resources .,named-entity-recognition,3,,,,13,0.34210526315789475,125,0.6756756756756757,11,1.0,1,0,
127,Analysis,Overall system results,,named-entity-recognition,3,,,,14,0.3684210526315789,126,0.6810810810810811,0,0.0,1,0,
128,"To elucidate the characteristics of our LM augmented sequence tagger , we ran a number of additional experiments on the CoNLL 2003 NER task .",Overall system results,Analysis,named-entity-recognition,3,,,,15,0.39473684210526316,127,0.6864864864864865,1,0.041666666666666664,1,0,
129,How to use LM embeddings ?,Overall system results,Analysis,named-entity-recognition,3,,,,16,0.42105263157894735,128,0.6918918918918919,2,0.08333333333333333,1,0,
130,"In this experiment , we concatenate the LM embeddings at dif - shows that the second alternative performs best .",Overall system results,Analysis,named-entity-recognition,3,,,,17,0.4473684210526316,129,0.6972972972972973,3,0.125,1,0,
131,We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves over all system performance .,Overall system results,Analysis,named-entity-recognition,3,,,,18,0.47368421052631576,130,0.7027027027027027,4,0.16666666666666666,1,0,
132,These results are consistent with who found that chunking performance was sensitive to the level at which additional POS supervision was added .,Overall system results,Analysis,named-entity-recognition,3,,,,19,0.5,131,0.7081081081081081,5,0.20833333333333334,1,0,
133,Does it matter which language model to use ?,Overall system results,Analysis,named-entity-recognition,3,,,,20,0.5263157894736842,132,0.7135135135135136,6,0.25,1,0,
134,"In this experiment , we compare six different configurations of the forward and backward language models ( including the baseline model which does not use any language models ) .",Overall system results,Analysis,named-entity-recognition,3,,,,21,0.5526315789473685,133,0.7189189189189189,7,0.2916666666666667,1,0,
135,The results are reported in .,Overall system results,,named-entity-recognition,3,,,,22,0.5789473684210527,134,0.7243243243243244,8,0.3333333333333333,1,0,
136,"We find that adding backward LM embeddings consistently outperforms forward - only LM embeddings , with F 1 improvements between 0.22 and 0.27 % , even with the relatively small backward LSTM - 2048-512 LM .",Overall system results,The results are reported in .,named-entity-recognition,3,,,,23,0.6052631578947368,135,0.7297297297297297,9,0.375,1,0,
137,"LM size is important , and replacing the forward LSTM - 2048 - 512 with CNN - BIG - LSTM ( test perplexities of 47.7 to 30.0 on 1B Word Benchmark ) improves F 1 by 0.26 - 0.31 % , about as much as adding backward LM .",Overall system results,The results are reported in .,named-entity-recognition,3,,,,24,0.631578947368421,136,0.7351351351351352,10,0.4166666666666667,1,0,
138,"Accordingly , we hypothesize ( but have not tested ) that replacing the backward LSTM - 2048 - 512 with a backward LM analogous to the CNN - BIG - LSTM would further improve performance .",Overall system results,The results are reported in .,named-entity-recognition,3,,,,25,0.6578947368421053,137,0.7405405405405405,11,0.4583333333333333,1,0,
139,"To highlight the importance of including language models trained on a large scale data , we also experimented with training a language model on just the CoNLL 2003 training and development data .",Overall system results,The results are reported in .,named-entity-recognition,3,,,,26,0.6842105263157895,138,0.745945945945946,12,0.5,1,0,
140,Due to the much smaller size of this data Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM .,Overall system results,The results are reported in .,named-entity-recognition,3,,,,27,0.7105263157894737,139,0.7513513513513513,13,0.5416666666666666,1,0,
141,"This result supports the hypothesis that adding language models help because they learn composition functions ( i.e. , the RNN parameters in the language model ) from much larger data compared to the composition functions in the baseline tagger , which are only learned from labeled data .",Overall system results,The results are reported in .,named-entity-recognition,3,,,,28,0.7368421052631579,140,0.7567567567567568,14,0.5833333333333334,1,0,
142,Importance of task specific RNN .,Overall system results,,named-entity-recognition,3,,,,29,0.7631578947368421,141,0.7621621621621621,15,0.625,1,0,
143,To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,30,0.7894736842105263,142,0.7675675675675676,16,0.6666666666666666,1,0,
144,"In this setup , performance was very low , 88.17 F 1 , well below our baseline .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,31,0.8157894736842105,143,0.772972972972973,17,0.7083333333333334,1,0,
145,This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,32,0.8421052631578947,144,0.7783783783783784,18,0.75,1,0,
146,Does the LM transfer across domains ?,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,33,0.868421052631579,145,0.7837837837837838,19,0.7916666666666666,1,0,
147,One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,34,0.8947368421052632,146,0.7891891891891892,20,0.8333333333333334,1,0,
148,"To test the sensitivity to the LM training domain , we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 , Science IE .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,35,0.9210526315789473,147,0.7945945945945946,21,0.875,1,0,
149,"5 Scien - ce IE requires end - to - end joint entity and relationship extraction from scientific publications across three diverse fields ( computer science , material sciences , and physics ) and defines three broad entity types ( Task , Material and Process ) .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,36,0.9473684210526315,148,0.8,22,0.9166666666666666,1,0,
150,"For this task , Tag LM increased F 1 on the development set by 4.12 % ( from 49.93 to to 54.05 % ) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to Science IE , Scenario 1 .",Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,37,0.9736842105263158,149,0.8054054054054054,23,0.9583333333333334,1,0,
151,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain .,Overall system results,Importance of task specific RNN .,named-entity-recognition,3,,,,38,1.0,150,0.8108108108108109,24,1.0,1,0,
152,Related work,,,named-entity-recognition,3,,,,0,0.0,151,0.8162162162162162,0,0.0,1,0,
153,Unlabeled data .,Related work,,named-entity-recognition,3,,,,1,0.09090909090909091,152,0.8216216216216217,1,0.03571428571428571,0,0,
154,Tag LM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models .,Related work,Unlabeled data .,named-entity-recognition,3,,,,2,0.18181818181818182,153,0.827027027027027,2,0.07142857142857142,0,0,
155,"Besides pre-trained word embeddings , our method is most closely related to .",Related work,Unlabeled data .,named-entity-recognition,3,,,,3,0.2727272727272727,154,0.8324324324324325,3,0.10714285714285714,0,0,
156,"Instead of using a LM , uses a probabilistic generative model to infer contextsensitive latent variables for each token , which are then used as extra features in a supervised CRF tagger .",Related work,Unlabeled data .,named-entity-recognition,3,,,,4,0.36363636363636365,155,0.8378378378378378,4,0.14285714285714285,0,0,
157,"Other semisupervised learning methods for structured prediction problems include co-training , expectation maximization , structural learning ( Ando and Zhang , 2005 ) and maximum discriminant functions .",Related work,Unlabeled data .,named-entity-recognition,3,,,,5,0.45454545454545453,156,0.8432432432432433,5,0.17857142857142858,0,0,
158,It is easy to combine Tag LM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model ( except for expectation maximization ) .,Related work,Unlabeled data .,named-entity-recognition,3,,,,6,0.5454545454545454,157,0.8486486486486486,6,0.21428571428571427,0,0,
159,A detailed discussion of semisupervised learning methods in NLP can be found in .,Related work,Unlabeled data .,named-entity-recognition,3,,,,7,0.6363636363636364,158,0.8540540540540541,7,0.25,0,0,
160,"learned a context encoder from unlabeled data with an objective function similar to a bi-directional LM and applied it to several NLP tasks closely related to the unlabeled objective function : sentence completion , lexical substitution and word sense dis ambiguation .",Related work,Unlabeled data .,named-entity-recognition,3,,,,8,0.7272727272727273,159,0.8594594594594595,8,0.2857142857142857,0,0,
161,"LM embeddings are related to a class of methods ( e.g. , for learning sentence and document encoders from unlabeled data , which can be used for text classification and textual entailment among other tasks .",Related work,Unlabeled data .,named-entity-recognition,3,,,,9,0.8181818181818182,160,0.8648648648648649,9,0.32142857142857145,0,0,
162,Dai and Le pre-trained LSTMs using language models and sequence autoencoders then fine tuned the weights for classification tasks .,Related work,Unlabeled data .,named-entity-recognition,3,,,,10,0.9090909090909091,161,0.8702702702702703,10,0.35714285714285715,0,0,
163,"In contrast to our method that uses unlabeled data to learn token - in - context embeddings , all of these methods use unlabeled data to learn an encoder for an entire text sequence ( sentence or document ) .",Related work,Unlabeled data .,named-entity-recognition,3,,,,11,1.0,162,0.8756756756756757,11,0.39285714285714285,0,0,
164,Neural language models .,,,named-entity-recognition,3,,,,0,0.0,163,0.8810810810810811,12,0.42857142857142855,1,0,
165,LMs have always been a critical component in statistical machine translation systems .,Neural language models .,Neural language models .,named-entity-recognition,3,,,,1,0.0625,164,0.8864864864864865,13,0.4642857142857143,1,0,
166,"Recently , neural LMs have also been integrated in neural machine translation systems ( e.g. , to score candidate translations .",Neural language models .,Neural language models .,named-entity-recognition,3,,,,2,0.125,165,0.8918918918918919,14,0.5,1,0,
167,"In contrast , Tag LM uses neural LMs to encode words in the input sequence .",Neural language models .,Neural language models .,named-entity-recognition,3,,,,3,0.1875,166,0.8972972972972973,15,0.5357142857142857,1,0,
168,"Unlike forward LMs , bidirectional LMs have received little prior attention .",Neural language models .,Neural language models .,named-entity-recognition,3,,,,4,0.25,167,0.9027027027027027,16,0.5714285714285714,1,0,
169,"Most similar to our formulation , used a bidirectional neural LM in a statistical machine translation system for instance selection .",Neural language models .,Neural language models .,named-entity-recognition,3,,,,5,0.3125,168,0.9081081081081082,17,0.6071428571428571,1,0,
170,"They tied the input token embeddings and softmax weights in the forward and backward directions , unlike our approach which uses two distinct models without any shared parameters .",Neural language models .,Neural language models .,named-entity-recognition,3,,,,6,0.375,169,0.9135135135135135,18,0.6428571428571429,1,0,
171,Frinken et al. ( 2012 ) also used a bidirectional n-gram LM for handwriting recognition .,Neural language models .,Neural language models .,named-entity-recognition,3,,,,7,0.4375,170,0.918918918918919,19,0.6785714285714286,1,0,
172,Interpreting RNN states .,Neural language models .,,named-entity-recognition,3,,,,8,0.5,171,0.9243243243243243,20,0.7142857142857143,1,0,
173,"Recently , there has been some interest in interpreting the activations of RNNs .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,,,,9,0.5625,172,0.9297297297297298,21,0.75,1,0,
174,showed that single LSTM units can learn to predict singular - plural distinctions .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,,,,10,0.625,173,0.9351351351351351,22,0.7857142857142857,1,0,
175,"visualized character level LSTM states and showed that individual cells capture long - range dependencies such as line lengths , quotes and brackets .",Neural language models .,Interpreting RNN states .,named-entity-recognition,3,,,,11,0.6875,174,0.9405405405405406,23,0.8214285714285714,1,0,
176,Our work complements these studies by showing that LM states are useful for downstream tasks as away of interpreting what they learn .,Neural language models .,Interpreting RNN states .,named-entity-recognition,3,,,,12,0.75,175,0.9459459459459459,24,0.8571428571428571,1,0,
177,Other sequence tagging models .,Neural language models .,,named-entity-recognition,3,,,,13,0.8125,176,0.9513513513513514,25,0.8928571428571429,1,0,
178,Current state of the art results in sequence tagging problems are based on bidirectional RNN models .,Neural language models .,Other sequence tagging models .,named-entity-recognition,3,,,,14,0.875,177,0.9567567567567568,26,0.9285714285714286,1,0,
179,"However , many other sequence tagging models have been proposed in the literature for this class of problems ( e.g. , .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,,,,15,0.9375,178,0.9621621621621622,27,0.9642857142857143,1,0,
180,"LM embeddings could also be used as additional features in other models , although it is not clear whether the model complexity would be sufficient to effectively make use of them .",Neural language models .,Other sequence tagging models .,named-entity-recognition,3,,,,16,1.0,179,0.9675675675675676,28,1.0,1,0,
181,Conclusion,,,named-entity-recognition,3,,,,0,0.0,180,0.972972972972973,0,0.0,1,0,
182,"In this paper , we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models .",Conclusion,Conclusion,named-entity-recognition,3,,,,1,0.25,181,0.9783783783783784,1,0.25,0,0,
183,Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking .,Conclusion,Conclusion,named-entity-recognition,3,,,,2,0.5,182,0.9837837837837838,2,0.5,0,0,
184,Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance .,Conclusion,Conclusion,named-entity-recognition,3,,,,3,0.75,183,0.9891891891891892,3,0.75,0,0,
185,"The proposed method is robust even when the LM is trained on unlabeled data from a different domain , or when the baseline model is trained on a large number of labeled examples .",Conclusion,Conclusion,named-entity-recognition,3,,,,4,1.0,184,0.9945945945945946,4,1.0,0,0,
1,title,,,named-entity-recognition,4,,,,0,0.0,0,0.0,0,0.0,1,0,
2,Deep contextualized word representations,title,,named-entity-recognition,4,"['B', 'I', 'I', 'I']","['B-n', 'I-n', 'I-n', 'I-n']","['B-ob', 'I-ob', 'I-ob', 'I-ob']",1,0.0,1,0.003676470588235294,1,0.0,1,1,research-problem
3,abstract,,,named-entity-recognition,4,,,,0,0.0,2,0.007352941176470588,0,0.0,1,0,
4,"We introduce a new type of deep contextualized word representation that models both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",abstract,abstract,named-entity-recognition,4,,,,1,0.25,3,0.011029411764705883,1,0.25,1,0,
5,"Our word vectors are learned functions of the internal states of a deep bidirectional language model ( biLM ) , which is pretrained on a large text corpus .",abstract,abstract,named-entity-recognition,4,,,,2,0.5,4,0.014705882352941176,2,0.5,1,0,
6,"We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems , including question answering , textual entailment and sentiment analysis .",abstract,abstract,named-entity-recognition,4,,,,3,0.75,5,0.01838235294117647,3,0.75,1,0,
7,"We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial , allowing downstream models to mix different types of semi-supervision signals .",abstract,abstract,named-entity-recognition,4,,,,4,1.0,6,0.022058823529411766,4,1.0,1,0,
8,Introduction,,,named-entity-recognition,4,,,,0,0.0,7,0.025735294117647058,0,0.0,1,0,
9,Pre-trained word representations are a key component in many neural language understanding models .,Introduction,Introduction,named-entity-recognition,4,,,,1,0.02564102564102564,8,0.029411764705882353,1,0.02631578947368421,1,0,
10,"However , learning high quality representations can be challenging .",Introduction,Introduction,named-entity-recognition,4,,,,2,0.05128205128205128,9,0.03308823529411765,2,0.05263157894736842,1,0,
11,"They should ideally model both ( 1 ) complex characteristics of word use ( e.g. , syntax and semantics ) , and ( 2 ) how these uses vary across linguistic contexts ( i.e. , to model polysemy ) .",Introduction,Introduction,named-entity-recognition,4,,,,3,0.07692307692307693,10,0.03676470588235294,3,0.07894736842105263,1,0,
12,"In this paper , we introduce a new type of deep contextualized word representation that directly addresses both challenges , can be easily integrated into existing models , and significantly improves the state of the art in every considered case across a range of challenging language understanding problems .",Introduction,Introduction,named-entity-recognition,4,,,,4,0.10256410256410256,11,0.04044117647058824,4,0.10526315789473684,1,0,
13,Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence .,Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'O', 'B-p', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",5,0.1282051282051282,12,0.04411764705882353,5,0.13157894736842105,1,1,approach
14,We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus .,Introduction,Introduction,named-entity-recognition,4,"['O', 'B', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'O']","['O', 'B-p', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'O']","['O', 'B-p', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O']",6,0.15384615384615385,13,0.04779411764705882,6,0.15789473684210525,1,1,approach
15,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",7,0.1794871794871795,14,0.051470588235294115,7,0.18421052631578946,1,1,approach
16,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",Introduction,Introduction,named-entity-recognition,4,,,,8,0.20512820512820512,15,0.05514705882352941,8,0.21052631578947367,1,0,
17,"More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",Introduction,Introduction,named-entity-recognition,4,"['O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",9,0.23076923076923078,16,0.058823529411764705,9,0.23684210526315788,1,1,approach
18,Combining the internal states in this manner allows for very rich word representations .,Introduction,Introduction,named-entity-recognition,4,,,,10,0.2564102564102564,17,0.0625,10,0.2631578947368421,1,0,
19,"Using intrinsic evaluations , we show that the higher - level LSTM states capture context - dependent aspects of word meaning ( e.g. , they can be used without modification to perform well on supervised word sense dis ambiguation tasks ) while lowerlevel states model aspects of syntax ( e.g. , they can be used to do part - of - speech tagging ) .",Introduction,Introduction,named-entity-recognition,4,,,,11,0.28205128205128205,18,0.0661764705882353,11,0.2894736842105263,1,0,
20,"Simultaneously exposing all of these signals is highly beneficial , allowing the learned models select the types of semi-supervision thatare most useful for each end task .",Introduction,Introduction,named-entity-recognition,4,,,,12,0.3076923076923077,19,0.06985294117647059,12,0.3157894736842105,1,0,
21,Extensive experiments demonstrate that ELMo representations work extremely well in practice .,Introduction,Introduction,named-entity-recognition,4,,,,13,0.3333333333333333,20,0.07352941176470588,13,0.34210526315789475,1,0,
22,"We first show that they can be easily added to existing models for six diverse and challenging language understanding problems , including textual entailment , question answering and sentiment analysis .",Introduction,Introduction,named-entity-recognition,4,,,,14,0.358974358974359,21,0.07720588235294118,14,0.3684210526315789,1,0,
23,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .",Introduction,Introduction,named-entity-recognition,4,,,,15,0.38461538461538464,22,0.08088235294117647,15,0.39473684210526316,1,0,
24,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",Introduction,Introduction,named-entity-recognition,4,,,,16,0.41025641025641024,23,0.08455882352941177,16,0.42105263157894735,1,0,
25,"Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform ar Xiv : 1802.05365v2 [ cs. CL ] 22 Mar 2018 those derived from just the top layer of an LSTM .",Introduction,Introduction,named-entity-recognition,4,,,,17,0.4358974358974359,24,0.08823529411764706,17,0.4473684210526316,1,0,
26,"Our trained models and code are publicly available , and we expect that ELMo will provide similar gains for many other NLP problems .",Introduction,Introduction,named-entity-recognition,4,,,,18,0.46153846153846156,25,0.09191176470588236,18,0.47368421052631576,1,0,
27,1,Introduction,Introduction,named-entity-recognition,4,,,,19,0.48717948717948717,26,0.09558823529411764,19,0.5,1,0,
28,"2 Related work Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text , pretrained word vectors are a standard component of most state - of the - art NLP architectures , including for question answering , textual entailment and semantic role labeling .",Introduction,Introduction,named-entity-recognition,4,,,,20,0.5128205128205128,27,0.09926470588235294,20,0.5263157894736842,1,0,
29,"However , these approaches for learning word vectors only allow a single contextindependent representation for each word .",Introduction,Introduction,named-entity-recognition,4,,,,21,0.5384615384615384,28,0.10294117647058823,21,0.5526315789473685,1,0,
30,"Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .",Introduction,Introduction,named-entity-recognition,4,,,,22,0.5641025641025641,29,0.10661764705882353,22,0.5789473684210527,1,0,
31,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .",Introduction,Introduction,named-entity-recognition,4,,,,23,0.5897435897435898,30,0.11029411764705882,23,0.6052631578947368,1,0,
32,Other recent work has also focused on learning context - dependent representations .,Introduction,Introduction,named-entity-recognition,4,,,,24,0.6153846153846154,31,0.11397058823529412,24,0.631578947368421,1,0,
33,context2vec,Introduction,Introduction,named-entity-recognition,4,,,,25,0.6410256410256411,32,0.11764705882352941,25,0.6578947368421053,1,0,
34,"( Melamud et al. , 2016 ) uses a bidirectional Long Short Term Memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) to encode the context around a pivot word .",Introduction,Introduction,named-entity-recognition,4,,,,26,0.6666666666666666,33,0.1213235294117647,26,0.6842105263157895,1,0,
35,Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation ( MT ) system or an unsupervised language model .,Introduction,Introduction,named-entity-recognition,4,,,,27,0.6923076923076923,34,0.125,27,0.7105263157894737,1,0,
36,"Both of these approaches benefit from large datasets , although the MT approach is limited by the size of parallel corpora .",Introduction,Introduction,named-entity-recognition,4,,,,28,0.717948717948718,35,0.12867647058823528,28,0.7368421052631579,1,0,
37,"In this paper , we take full advantage of access to plentiful monolingual data , and train our biLM on a corpus with approximately 30 million sentences .",Introduction,Introduction,named-entity-recognition,4,,,,29,0.7435897435897436,36,0.1323529411764706,29,0.7631578947368421,1,0,
38,"We also generalize these approaches to deep contextual representations , which we show work well across a broad range of diverse NLP tasks .",Introduction,Introduction,named-entity-recognition,4,,,,30,0.7692307692307693,37,0.13602941176470587,30,0.7894736842105263,1,0,
39,1 http://allennlp.org/elmo,Introduction,Introduction,named-entity-recognition,4,,,,31,0.7948717948717948,38,0.13970588235294118,31,0.8157894736842105,1,0,
40,Previous work has also shown that different layers of deep biRNNs encode different types of information .,Introduction,Introduction,named-entity-recognition,4,,,,32,0.8205128205128205,39,0.14338235294117646,32,0.8421052631578947,1,0,
41,"For example , introducing multi-task syntactic supervision ( e.g. , part - of - speech tags ) at the lower levels of a deep LSTM can improve over all performance of higher level tasks such as dependency parsing or CCG super tagging .",Introduction,Introduction,named-entity-recognition,4,,,,33,0.8461538461538461,40,0.14705882352941177,33,0.868421052631579,1,0,
42,"In an RNN - based encoder - decoder machine translation system , showed that the representations learned at the first layer in a 2 layer LSTM encoder are better at predicting POS tags then second layer .",Introduction,Introduction,named-entity-recognition,4,,,,34,0.8717948717948718,41,0.15073529411764705,34,0.8947368421052632,1,0,
43,"Finally , the top layer of an LSTM for encoding word context has been shown to learn representations of word sense .",Introduction,Introduction,named-entity-recognition,4,,,,35,0.8974358974358975,42,0.15441176470588236,35,0.9210526315789473,1,0,
44,"We show that similar signals are also induced by the modified language model objective of our ELMo representations , and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision .",Introduction,Introduction,named-entity-recognition,4,,,,36,0.9230769230769231,43,0.15808823529411764,36,0.9473684210526315,1,0,
45,and pretrain encoder - decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision .,Introduction,Introduction,named-entity-recognition,4,,,,37,0.9487179487179487,44,0.16176470588235295,37,0.9736842105263158,1,0,
46,"In contrast , after pretraining the biLM with unlabeled data , we fix the weights and add additional taskspecific model capacity , allowing us to leverage large , rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model .",Introduction,Introduction,named-entity-recognition,4,,,,38,0.9743589743589743,45,0.16544117647058823,38,1.0,1,0,
47,Dai and Le,Introduction,,named-entity-recognition,4,,,,39,1.0,46,0.16911764705882354,0,0.0,1,0,
48,Bidirectional language models,,,named-entity-recognition,4,,,,0,0.0,47,0.17279411764705882,0,0.0,1,0,
49,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k?1 ) :",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,1,0.017543859649122806,48,0.17647058823529413,1,0.1,1,0,
50,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,2,0.03508771929824561,49,0.1801470588235294,2,0.2,1,0,
51,"At each position k , each LSTM layer outputs a context - dependent representation ? ? h LM k , j where j = 1 , . . . , L. The top layer LSTM output , ? ? h LM k , L , is used to predict the next token t k +1 with a Softmax layer .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,3,0.05263157894736842,50,0.18382352941176472,3,0.3,1,0,
52,"A backward LM is similar to a forward LM , except it runs over the sequence in reverse , predicting the previous token given the future context :",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,4,0.07017543859649122,51,0.1875,4,0.4,1,0,
53,"It can be implemented in an analogous way to a forward LM , with each backward LSTM layer j in a L layer deep model producing representations ? ? h LM k , j oft k given ( t k+1 , . . . , t N ) .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,5,0.08771929824561403,52,0.19117647058823528,5,0.5,1,0,
54,A bi LM combines both a forward and backward LM .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,6,0.10526315789473684,53,0.1948529411764706,6,0.6,1,0,
55,Our formulation jointly maximizes the log likelihood of the forward and backward directions :,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,7,0.12280701754385964,54,0.19852941176470587,7,0.7,1,0,
56,We tie the parameters for both the token representation (? x ) and Softmax layer (? s ) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction .,Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,8,0.14035087719298245,55,0.20220588235294118,8,0.8,1,0,
57,"Overall , this formulation is similar to the approach of Peters et al. , with the exception that we share some weights between directions instead of using completely independent parameters .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,9,0.15789473684210525,56,0.20588235294117646,9,0.9,1,0,
58,"In the next section , we depart from previous work by introducing a new approach for learning word representations thatare a linear combination of the biLM layers .",Bidirectional language models,Bidirectional language models,named-entity-recognition,4,,,,10,0.17543859649122806,57,0.20955882352941177,10,1.0,1,0,
59,ELMo,Bidirectional language models,,named-entity-recognition,4,,,,11,0.19298245614035087,58,0.21323529411764705,0,0.0,1,0,
60,ELMo is a task specific combination of the intermediate layer representations in the biLM .,Bidirectional language models,ELMo,named-entity-recognition,4,,,,12,0.21052631578947367,59,0.21691176470588236,1,0.09090909090909091,1,0,
61,"For each token t k , a L-layer biLM computes a set of 2L + 1 representations",Bidirectional language models,ELMo,named-entity-recognition,4,,,,13,0.22807017543859648,60,0.22058823529411764,2,0.18181818181818182,1,0,
62,"where h LM k ,0 is the token layer and h LM",Bidirectional language models,ELMo,named-entity-recognition,4,,,,14,0.24561403508771928,61,0.22426470588235295,3,0.2727272727272727,1,0,
63,", for each biLSTM layer .",Bidirectional language models,ELMo,named-entity-recognition,4,,,,15,0.2631578947368421,62,0.22794117647058823,4,0.36363636363636365,1,0,
64,"For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",Bidirectional language models,ELMo,named-entity-recognition,4,,,,16,0.2807017543859649,63,0.23161764705882354,5,0.45454545454545453,1,0,
65,"In the simplest case , ELMo just selects the top layer , E( R k ) = h LM k , L , as in Tag LM and CoVe .",Bidirectional language models,ELMo,named-entity-recognition,4,,,,17,0.2982456140350877,64,0.23529411764705882,6,0.5454545454545454,1,0,
66,"More generally , we compute a task specific weighting of all biLM layers :",Bidirectional language models,ELMo,named-entity-recognition,4,,,,18,0.3157894736842105,65,0.23897058823529413,7,0.6363636363636364,1,0,
67,"( 1 ) In ( 1 ) , s task are softmax - normalized weights and the scalar parameter ?",Bidirectional language models,ELMo,named-entity-recognition,4,,,,19,0.3333333333333333,66,0.2426470588235294,8,0.7272727272727273,1,0,
68,task allows the task model to scale the entire ELMo vector . ?,Bidirectional language models,ELMo,named-entity-recognition,4,,,,20,0.3508771929824561,67,0.24632352941176472,9,0.8181818181818182,1,0,
69,is of practical importance to aid the optimization process ( see supplemental material for details ) .,Bidirectional language models,ELMo,named-entity-recognition,4,,,,21,0.3684210526315789,68,0.25,10,0.9090909090909091,1,0,
70,"Considering that the activations of each biLM layer have a different distribution , in some cases it also helped to apply layer normalization to each biLM layer before weighting .",Bidirectional language models,ELMo,named-entity-recognition,4,,,,22,0.38596491228070173,69,0.2536764705882353,11,1.0,1,0,
71,Using biLMs for supervised NLP tasks,Bidirectional language models,,named-entity-recognition,4,,,,23,0.40350877192982454,70,0.25735294117647056,0,0.0,1,0,
72,"Given a pre-trained biLM and a supervised architecture for a target NLP task , it is a simple process to use the biLM to improve the task model .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,24,0.42105263157894735,71,0.2610294117647059,1,0.07142857142857142,1,0,
73,We simply run the biLM and record all of the layer representations for each word .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,25,0.43859649122807015,72,0.2647058823529412,2,0.14285714285714285,1,0,
74,"Then , we let the end task model learn a linear combination of these representations , as described below .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,26,0.45614035087719296,73,0.26838235294117646,3,0.21428571428571427,1,0,
75,First consider the lowest layers of the supervised model without the biLM .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,27,0.47368421052631576,74,0.27205882352941174,4,0.2857142857142857,1,0,
76,"Most supervised NLP models share a common architecture at the lowest layers , allowing us to add ELMo in a consistent , unified manner .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,28,0.49122807017543857,75,0.2757352941176471,5,0.35714285714285715,1,0,
77,"Given a sequence of tokens ( t 1 , . . . , t N ) , it is standard to form a context - independent token representation x k for each token position using pre-trained word embeddings and optionally character - based representations .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,29,0.5087719298245614,76,0.27941176470588236,6,0.42857142857142855,1,0,
78,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,30,0.5263157894736842,77,0.28308823529411764,7,0.5,1,0,
79,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,31,0.543859649122807,78,0.2867647058823529,8,0.5714285714285714,1,0,
80,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,32,0.5614035087719298,79,0.29044117647058826,9,0.6428571428571429,1,0,
81,"As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,33,0.5789473684210527,80,0.29411764705882354,10,0.7142857142857143,1,0,
82,"For example , see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs , or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs .",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,34,0.5964912280701754,81,0.2977941176470588,11,0.7857142857142857,1,0,
83,"Finally , we found it beneficial to add a moderate amount of dropout to and in some cases to regularize the ELMo weights by adding ?",Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,35,0.6140350877192983,82,0.3014705882352941,12,0.8571428571428571,1,0,
84,w 2 2 to the loss .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,36,0.631578947368421,83,0.30514705882352944,13,0.9285714285714286,1,0,
85,This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers .,Bidirectional language models,Using biLMs for supervised NLP tasks,named-entity-recognition,4,,,,37,0.6491228070175439,84,0.3088235294117647,14,1.0,1,0,
86,Pre-trained bidirectional language model architecture,Bidirectional language models,,named-entity-recognition,4,,,,38,0.6666666666666666,85,0.3125,0,0.0,1,0,
87,"The pre-trained biLMs in this paper are similar to the architectures in Jzefowicz et al. and , but modified to support joint training of both directions and add a residual connection between LSTM layers .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,39,0.6842105263157895,86,0.3161764705882353,1,0.05263157894736842,1,0,
88,"We focus on large scale biLMs in this work , as Peters et al .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,40,0.7017543859649122,87,0.31985294117647056,2,0.10526315789473684,1,0,
89,highlighted the importance of using biLMs over forward - only LMs and large scale training .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,41,0.7192982456140351,88,0.3235294117647059,3,0.15789473684210525,1,0,
90,"To balance over all language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character - based input representation , we halved all embedding and hidden dimensions from the single best model CNN - BIG - LSTM in Jzefowicz et al ..",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,42,0.7368421052631579,89,0.3272058823529412,4,0.21052631578947367,1,0,
91,The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,43,0.7543859649122807,90,0.33088235294117646,5,0.2631578947368421,1,0,
92,The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,44,0.7719298245614035,91,0.33455882352941174,6,0.3157894736842105,1,0,
93,"As a result , the biLM provides three layers of representations for each input token , including those outside the training set due to the purely character input .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,45,0.7894736842105263,92,0.3382352941176471,7,0.3684210526315789,1,0,
94,"In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,46,0.8070175438596491,93,0.34191176470588236,8,0.42105263157894735,1,0,
95,"After training for 10 epochs on the 1B Word Benchmark , the average forward and backward perplexities is 39.7 , compared to 30.0 for the forward CNN - BIG - LSTM .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,47,0.8245614035087719,94,0.34558823529411764,9,0.47368421052631576,1,0,
96,"Generally , we found the forward and backward perplexities to be approximately equal , with the backward value slightly lower .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,48,0.8421052631578947,95,0.3492647058823529,10,0.5263157894736842,1,0,
97,"Once pretrained , the biLM can compute representations for any task .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,49,0.8596491228070176,96,0.35294117647058826,11,0.5789473684210527,1,0,
98,"In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,50,0.8771929824561403,97,0.35661764705882354,12,0.631578947368421,1,0,
99,This can be seen as a type of domain transfer for the biLM .,Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,51,0.8947368421052632,98,0.3602941176470588,13,0.6842105263157895,1,0,
100,"As a result , in most cases we used a fine - tuned biLM in the downstream task .",Bidirectional language models,Pre-trained bidirectional language model architecture,named-entity-recognition,4,,,,52,0.9122807017543859,99,0.3639705882352941,14,0.7368421052631579,1,0,
101,See supplemental material for details .,Bidirectional language models,,named-entity-recognition,4,,,,53,0.9298245614035088,100,0.36764705882352944,15,0.7894736842105263,1,0,
102,shows the performance of ELMo across a diverse set of six benchmark NLP tasks .,Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,,,,54,0.9473684210526315,101,0.3713235294117647,16,0.8421052631578947,1,0,
103,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,,,,55,0.9649122807017544,102,0.375,17,0.8947368421052632,1,0,
104,This is a very general result across a diverse set model architectures and language understanding tasks .,Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,,,,56,0.9824561403508771,103,0.3786764705882353,18,0.9473684210526315,1,0,
105,In the remainder of this section we provide high - level sketches of the individual task results ; see the supplemental material for full experimental details .,Bidirectional language models,See supplemental material for details .,named-entity-recognition,4,,,,57,1.0,104,0.38235294117647056,19,1.0,1,0,
106,Evaluation,,,named-entity-recognition,4,,,,0,0.0,105,0.3860294117647059,0,0.0,1,0,
107,Question Textual entailment,Evaluation,,named-entity-recognition,4,,,,1,0.008695652173913044,106,0.3897058823529412,1,0.07142857142857142,1,0,
108,"Textual entailment is the task of determining whether a "" hypothesis "" is true , given a "" premise "" .",Evaluation,Question Textual entailment,named-entity-recognition,4,"['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",2,0.017391304347826087,107,0.39338235294117646,2,0.14285714285714285,1,1,tasks
109,The Stanford Natural Language Inference ( SNLI ) corpus provides approximately 550K hypothesis / premise pairs .,Evaluation,Question Textual entailment,named-entity-recognition,4,"['O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.02608695652173913,108,0.39705882352941174,3,0.21428571428571427,1,1,tasks
110,"Our baseline , the ESIM sequence model from Chen et al. , uses a biL - STM to encode the premise and hypothesis , followed by a matrix attention layer , a local inference layer , another biLSTM inference composition layer , and finally a pooling operation before the output layer .",Evaluation,Question Textual entailment,named-entity-recognition,4,,,,4,0.034782608695652174,109,0.4007352941176471,4,0.2857142857142857,1,0,
111,"Overall , adding ELMo to the ESIM model improves accuracy by an average of 0.7 % across five random seeds .",Evaluation,Question Textual entailment,named-entity-recognition,4,"['O', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'B-p', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",5,0.043478260869565216,110,0.40441176470588236,5,0.35714285714285715,1,1,tasks
112,"A five member ensemble pushes the over all accuracy to 89.3 % , exceeding the previous ensemble best of 88.9 % .",Evaluation,Question Textual entailment,named-entity-recognition,4,,,,6,0.05217391304347826,111,0.40808823529411764,6,0.42857142857142855,1,0,
113,Semantic role labeling,Evaluation,,named-entity-recognition,4,,,,7,0.06086956521739131,112,0.4117647058823529,7,0.5,1,0,
114,"A semantic role labeling ( SRL ) system models the predicate - argument structure of a sentence , and is often described as answering "" Who did what to whom "" .",Evaluation,Semantic role labeling,named-entity-recognition,4,,,,8,0.06956521739130435,113,0.41544117647058826,8,0.5714285714285714,1,0,
115,He et al .,Evaluation,,named-entity-recognition,4,,,,9,0.0782608695652174,114,0.41911764705882354,9,0.6428571428571429,1,0,
116,"( 2017 ) modeled SRL as a BIO tagging problem and used an 8 - layer deep biLSTM with forward and backward directions interleaved , following .",Evaluation,He et al .,named-entity-recognition,4,,,,10,0.08695652173913043,115,0.4227941176470588,10,0.7142857142857143,1,0,
117,As shown in Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities .,Evaluation,He et al .,named-entity-recognition,4,"['O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'B-b', 'I-b', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",11,0.09565217391304348,116,0.4264705882352941,11,0.7857142857142857,1,1,tasks
118,Our baseline model is the end - to - end span - based neural model of .,Evaluation,He et al .,named-entity-recognition,4,,,,12,0.10434782608695652,117,0.43014705882352944,12,0.8571428571428571,1,0,
119,It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains .,Evaluation,He et al .,named-entity-recognition,4,,,,13,0.11304347826086956,118,0.4338235294117647,13,0.9285714285714286,1,0,
120,"In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task , adding ELMo improved the average F 1 by 3.2 % from 67.2 to 70.4 , establishing a new state of the art , again improving over the previous best ensemble result by 1.6 % F 1 .",Evaluation,He et al .,named-entity-recognition,4,"['O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,14,0.12173913043478261,119,0.4375,14,1.0,1,1,tasks
121,Analysis,Evaluation,,named-entity-recognition,4,,,,15,0.13043478260869565,120,0.4411764705882353,0,0.0,1,0,
122,This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations .,Evaluation,Analysis,named-entity-recognition,4,,,,16,0.1391304347826087,121,0.44485294117647056,1,0.2,1,0,
123,"Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer , regardless of whether they are produced from a biLM or MT encoder , and that ELMo representations provide the best over all performance .",Evaluation,Analysis,named-entity-recognition,4,,,,17,0.14782608695652175,122,0.4485294117647059,2,0.4,1,0,
124,"Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers , consistent with MT encoders .",Evaluation,Analysis,named-entity-recognition,4,,,,18,0.1565217391304348,123,0.4522058823529412,3,0.6,1,0,
125,It also shows that our biLM consistently provides richer representations then CoVe .,Evaluation,Analysis,named-entity-recognition,4,,,,19,0.16521739130434782,124,0.45588235294117646,4,0.8,1,0,
126,"Additionally , we analyze the sensitivity to where ELMo is included in the task model ( Sec. 5.2 ) , training set size ( Sec. 5.4 ) , and visualize the ELMo learned weights across the tasks ( Sec. 5.5 ) .",Evaluation,Analysis,named-entity-recognition,4,,,,20,0.17391304347826086,125,0.45955882352941174,5,1.0,1,0,
127,Alternate layer weighting schemes,Evaluation,,named-entity-recognition,4,,,,21,0.1826086956521739,126,0.4632352941176471,0,0.0,1,0,
128,There are many alternatives to Equation 1 for combining the biLM layers .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,22,0.19130434782608696,127,0.46691176470588236,1,0.02127659574468085,1,0,
129,"Previous work on contextual representations used only the last layer , whether it be from a biLM or an MT encoder ( CoVe ; .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,23,0.2,128,0.47058823529411764,2,0.0425531914893617,1,0,
130,The choice of the regularization parameter ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,24,0.20869565217391303,129,0.4742647058823529,3,0.06382978723404255,1,0,
131,"is also important , as large values such as ? = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , ? = 0.001 ) allow the layer weights to vary .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,25,0.21739130434782608,130,0.47794117647058826,4,0.0851063829787234,1,0,
132,"compares these alternatives for SQuAD , SNLI and SRL .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,26,0.22608695652173913,131,0.48161764705882354,5,0.10638297872340426,1,0,
133,"Including representations from all layers improves over all performance over just using the last layer , and including contextual representations from the last layer improves performance over the baseline .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,27,0.23478260869565218,132,0.4852941176470588,6,0.1276595744680851,1,0,
134,"For example , in the case of SQuAD , using just the last biLM layer improves development F 1 by 3.9 % over the baseline .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,28,0.24347826086956523,133,0.4889705882352941,7,0.14893617021276595,1,0,
135,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to ?= 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( ?= 1 vs. ?= 0.001 ) .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,29,0.25217391304347825,134,0.49264705882352944,8,0.1702127659574468,1,0,
136,A small ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,30,0.2608695652173913,135,0.4963235294117647,9,0.19148936170212766,1,0,
137,"is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ?",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,31,0.26956521739130435,136,0.5,10,0.2127659574468085,1,0,
138,( not shown ) .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,32,0.2782608695652174,137,0.5036764705882353,11,0.23404255319148937,1,0,
139,The over all trend is similar with CoVe but with smaller increases over the baseline .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,33,0.28695652173913044,138,0.5073529411764706,12,0.2553191489361702,1,0,
140,"For SNLI , averaging all layers with ?= 1 improves development accuracy from 88.2 to 88.7 % over using just the last layer .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,34,0.2956521739130435,139,0.5110294117647058,13,0.2765957446808511,1,0,
141,SRL F 1 increased a marginal 0.1 % to 82.2 for the ?= 1 case compared to using the last layer only .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,35,0.30434782608695654,140,0.5147058823529411,14,0.2978723404255319,1,0,
142,Where to include ELMo ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,36,0.3130434782608696,141,0.5183823529411765,15,0.3191489361702128,1,0,
143,All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,37,0.3217391304347826,142,0.5220588235294118,16,0.3404255319148936,1,0,
144,"However , we find that including ELMo at the output of the biRNN in task - specific architectures improves over all results for some tasks .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,38,0.33043478260869563,143,0.5257352941176471,17,0.3617021276595745,1,0,
145,"As shown in , including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer , but for SRL ( and coreference resolution , not shown ) performance is highest when it is included at just the input layer .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,39,0.3391304347826087,144,0.5294117647058824,18,0.3829787234042553,1,0,
146,"One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN , so introducing ELMo at this layer allows the model to attend directly to the biLM 's internal representations .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,40,0.34782608695652173,145,0.5330882352941176,19,0.40425531914893614,1,0,
147,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement . the task - specific context representations are likely more important than those from the biLM .",Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,41,0.3565217391304348,146,0.5367647058823529,20,0.425531914893617,1,0,
148,What information is captured by the biLM 's representations ?,Evaluation,Alternate layer weighting schemes,named-entity-recognition,4,,,,42,0.3652173913043478,147,0.5404411764705882,21,0.44680851063829785,1,0,
149,Since adding,Evaluation,,named-entity-recognition,4,,,,43,0.3739130434782609,148,0.5441176470588235,22,0.46808510638297873,1,0,
150,"ELMo improves task performance over word vectors alone , the biLM 's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors .",Evaluation,Since adding,named-entity-recognition,4,,,,44,0.3826086956521739,149,0.5477941176470589,23,0.48936170212765956,1,0,
151,"Intuitively , the biLM must be dis ambiguating the meaning of words using their context .",Evaluation,Since adding,named-entity-recognition,4,,,,45,0.391304347826087,150,0.5514705882352942,24,0.5106382978723404,1,0,
152,"Consider "" play "" , a highly polysemous word .",Evaluation,Since adding,named-entity-recognition,4,,,,46,0.4,151,0.5551470588235294,25,0.5319148936170213,1,0,
153,"The top of lists nearest neighbors to "" play "" using GloVe vectors .",Evaluation,Since adding,named-entity-recognition,4,,,,47,0.40869565217391307,152,0.5588235294117647,26,0.5531914893617021,1,0,
154,"They are spread across several parts of speech ( e.g. , "" played "" , "" playing "" as verbs , and "" player "" , "" game "" as nouns ) but concentrated in the sportsrelated senses of "" play "" .",Evaluation,Since adding,named-entity-recognition,4,,,,48,0.41739130434782606,153,0.5625,27,0.574468085106383,1,0,
155,"In contrast , the bottom two rows show nearest neighbor sentences from the SemCor dataset ( see below ) using the biLM 's context representation of "" play "" in the source sentence .",Evaluation,Since adding,named-entity-recognition,4,,,,49,0.4260869565217391,154,0.5661764705882353,28,0.5957446808510638,1,0,
156,"In these cases , the biLM is able to dis ambiguate both the part of speech and word sense in the source sentence .",Evaluation,Since adding,named-entity-recognition,4,,,,50,0.43478260869565216,155,0.5698529411764706,29,0.6170212765957447,1,0,
157,These observations can be quantified using an intrinsic evaluation of the contextual representations similar to .,Evaluation,Since adding,named-entity-recognition,4,,,,51,0.4434782608695652,156,0.5735294117647058,30,0.6382978723404256,1,0,
158,"To isolate the information encoded by the biLM , the representations are used to directly make predictions for a fine grained word sense dis ambiguation ( WSD ) task and a POS tagging task .",Evaluation,Since adding,named-entity-recognition,4,,,,52,0.45217391304347826,157,0.5772058823529411,31,0.6595744680851063,1,0,
159,"Using this approach , it is also possible to compare to CoVe , and across each of the individual layers .",Evaluation,Since adding,named-entity-recognition,4,,,,53,0.4608695652173913,158,0.5808823529411765,32,0.6808510638297872,1,0,
160,"Word sense dis ambiguation Given a sentence , we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach , similar to .",Evaluation,Since adding,named-entity-recognition,4,,,,54,0.46956521739130436,159,0.5845588235294118,33,0.7021276595744681,1,0,
161,"To do so , we first use the biLM to compute representations for all words in Sem - Cor 3.0 , our training corpus , and then take the average representation for each sense .",Evaluation,Since adding,named-entity-recognition,4,,,,55,0.4782608695652174,160,0.5882352941176471,34,0.723404255319149,1,0,
162,"At test time , we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set , falling back to the first sense from WordNet for lemmas not observed during training .",Evaluation,Since adding,named-entity-recognition,4,,,,56,0.48695652173913045,161,0.5919117647058824,35,0.7446808510638298,1,0,
163,compares WSD results using the evaluation framework from across the same suite of four test sets in .,Evaluation,Since adding,named-entity-recognition,4,,,,57,0.4956521739130435,162,0.5955882352941176,36,0.7659574468085106,1,0,
164,"Overall , the biLM top layer rep-resentations have F 1 of 69.0 and are better at WSD then the first layer .",Evaluation,Since adding,named-entity-recognition,4,,,,58,0.5043478260869565,163,0.5992647058823529,37,0.7872340425531915,1,0,
165,This is competitive with a state - of - the - art WSD - specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse - grained semantic labels and POS tags .,Evaluation,Since adding,named-entity-recognition,4,,,,59,0.5130434782608696,164,0.6029411764705882,38,0.8085106382978723,1,0,
166,"The CoVe biLSTM layers follow a similar pattern to those from the biLM ( higher over all performance at the second layer compared to the first ) ; however , our biLM outperforms the CoVe biLSTM , which trails the WordNet first sense baseline .",Evaluation,Since adding,named-entity-recognition,4,,,,60,0.5217391304347826,165,0.6066176470588235,39,0.8297872340425532,1,0,
167,POS tagging,Evaluation,,named-entity-recognition,4,,,,61,0.5304347826086957,166,0.6102941176470589,40,0.851063829787234,1,0,
168,"To examine whether the biLM captures basic syntax , we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank ( PTB ) .",Evaluation,POS tagging,named-entity-recognition,4,,,,62,0.5391304347826087,167,0.6139705882352942,41,0.8723404255319149,1,0,
169,"As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .",Evaluation,POS tagging,named-entity-recognition,4,,,,63,0.5478260869565217,168,0.6176470588235294,42,0.8936170212765957,1,0,
170,"Similar to WSD , the biLM representations are competitive with carefully tuned , task specific biLSTMs .",Evaluation,POS tagging,named-entity-recognition,4,,,,64,0.5565217391304348,169,0.6213235294117647,43,0.9148936170212766,1,0,
171,"However , unlike WSD , accuracies using the first biLM layer are higher than the top layer , consistent with results from deep biL - STMs in multi-task training and MT .",Evaluation,POS tagging,named-entity-recognition,4,,,,65,0.5652173913043478,170,0.625,44,0.9361702127659575,1,0,
172,"CoVe POS tagging accuracies follow the same pattern as those from the biLM , and just like for WSD , the biLM achieves higher accuracies than the CoVe encoder .",Evaluation,POS tagging,named-entity-recognition,4,,,,66,0.5739130434782609,171,0.6286764705882353,45,0.9574468085106383,1,0,
173,"Implications for supervised tasks Taken together , these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks .",Evaluation,POS tagging,named-entity-recognition,4,,,,67,0.5826086956521739,172,0.6323529411764706,46,0.9787234042553191,1,0,
174,"In addition , the biLM 's representations are more transferable to WSD and POS tagging than those in CoVe , helping to illustrate why ELMo outperforms CoVe in downstream tasks .",Evaluation,POS tagging,named-entity-recognition,4,,,,68,0.591304347826087,173,0.6360294117647058,47,1.0,1,0,
175,Sample efficiency,Evaluation,,named-entity-recognition,4,,,,69,0.6,174,0.6397058823529411,0,0.0,1,0,
176,"Adding ELMo to a model increases the sample efficiency considerably , both in terms of number of parameter updates to reach state - of - the - art performance and the over all training set size .",Evaluation,Sample efficiency,named-entity-recognition,4,,,,70,0.6086956521739131,175,0.6433823529411765,1,0.1,1,0,
177,"For example , the SRL model reaches a maximum development F 1 after 486 epochs of training without ELMo .",Evaluation,Sample efficiency,named-entity-recognition,4,,,,71,0.6173913043478261,176,0.6470588235294118,2,0.2,1,0,
178,"After adding ELMo , the model exceeds the baseline maximum at epoch 10 , a 98 % relative decrease in the number of updates needed to reach In addition , ELMo - enhanced models use smaller training sets more efficiently than models without ELMo.",Evaluation,Sample efficiency,named-entity-recognition,4,,,,72,0.6260869565217392,177,0.6507352941176471,3,0.3,1,0,
179,compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1 % to 100 % .,Evaluation,Sample efficiency,named-entity-recognition,4,,,,73,0.6347826086956522,178,0.6544117647058824,4,0.4,1,0,
180,Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance .,Evaluation,Sample efficiency,named-entity-recognition,4,,,,74,0.6434782608695652,179,0.6580882352941176,5,0.5,1,0,
181,"In the SRL case , the ELMo model with 1 % of the training set has about the same F 1 as the baseline model with 10 % of the training set .",Evaluation,Sample efficiency,named-entity-recognition,4,,,,75,0.6521739130434783,180,0.6617647058823529,6,0.6,1,0,
182,visualizes the softmax - normalized learned layer weights .,Evaluation,Sample efficiency,named-entity-recognition,4,,,,76,0.6608695652173913,181,0.6654411764705882,7,0.7,1,0,
183,"At the input layer , the task model favors the first biLSTM layer .",Evaluation,Sample efficiency,named-entity-recognition,4,,,,77,0.6695652173913044,182,0.6691176470588235,8,0.8,1,0,
184,"For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks .",Evaluation,Sample efficiency,named-entity-recognition,4,,,,78,0.6782608695652174,183,0.6727941176470589,9,0.9,1,0,
185,"The output layer weights are relatively balanced , with a slight preference for the lower layers .",Evaluation,Sample efficiency,named-entity-recognition,4,,,,79,0.6869565217391305,184,0.6764705882352942,10,1.0,1,0,
186,Visualization of learned weights,Evaluation,,named-entity-recognition,4,,,,80,0.6956521739130435,185,0.6801470588235294,0,0.0,1,0,
187,"We have introduced a general approach for learning high - quality deep context - dependent representations from biLMs , and shown large improvements when applying ELMo to a broad range of NLP tasks .",Evaluation,Visualization of learned weights,named-entity-recognition,4,,,,81,0.7043478260869566,186,0.6838235294117647,1,0.5,1,0,
188,"Through ablations and other controlled experiments , we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin - context , and that using all layers improves over all task performance .",Evaluation,Visualization of learned weights,named-entity-recognition,4,,,,82,0.7130434782608696,187,0.6875,2,1.0,1,0,
189,A Supplemental Material to accompany Deep contextualized word representations,Evaluation,,named-entity-recognition,4,,,,83,0.7217391304347827,188,0.6911764705882353,0,0.0,1,0,
190,"This supplement contains details of the model architectures , training routines and hyper - parameter choices for the state - of - the - art models in Section 4 .",Evaluation,A Supplemental Material to accompany Deep contextualized word representations,named-entity-recognition,4,,,,84,0.7304347826086957,189,0.6948529411764706,1,0.0625,1,0,
191,All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs .,Evaluation,A Supplemental Material to accompany Deep contextualized word representations,named-entity-recognition,4,,,,85,0.7391304347826086,190,0.6985294117647058,2,0.125,1,0,
192,A.1,Evaluation,,named-entity-recognition,4,,,,86,0.7478260869565218,191,0.7022058823529411,3,0.1875,1,0,
193,Fine tuning biLM,Evaluation,,named-entity-recognition,4,,,,87,0.7565217391304347,192,0.7058823529411765,4,0.25,1,0,
194,"As noted in Sec. 3.4 , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,88,0.7652173913043478,193,0.7095588235294118,5,0.3125,1,0,
195,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,89,0.7739130434782608,194,0.7132352941176471,6,0.375,1,0,
196,"Once fine tuned , the biLM weights were fixed during task training .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,90,0.782608695652174,195,0.7169117647058824,7,0.4375,1,0,
197,lists the development set perplexities for the considered tasks .,Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,91,0.7913043478260869,196,0.7205882352941176,8,0.5,1,0,
198,"In every case except CoNLL 2012 , fine tuning results in a large improvement in perplexity , e.g. , from 72.1 to 16.8 for SNLI .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,92,0.8,197,0.7242647058823529,9,0.5625,1,0,
199,The impact of fine tuning on supervised performance is task dependent .,Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,93,0.808695652173913,198,0.7279411764705882,10,0.625,1,0,
200,"In the case of SNLI , fine tuning the biLM increased development accuracy 0.6 % from 88.9 % to 89.5 % for our single best model .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,94,0.8173913043478261,199,0.7316176470588235,11,0.6875,1,0,
201,"However , for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,95,0.8260869565217391,200,0.7352941176470589,12,0.75,1,0,
202,A.2 Importance of ? in Eqn .,Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,96,0.8347826086956521,201,0.7389705882352942,13,0.8125,1,0,
203,"The ? parameter in Eqn. ( 1 ) was of practical importance to aid optimization , due to the different distributions between the biLM internal representations and the task specific representations .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,97,0.8434782608695652,202,0.7426470588235294,14,0.875,1,0,
204,It is especially important in the last - only casein Sec. 5.1 .,Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,98,0.8521739130434782,203,0.7463235294117647,15,0.9375,1,0,
205,"Without this parameter , the last - only case performed poorly ( well below the baseline ) for SNLI and training failed completely for SRL .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,99,0.8608695652173913,204,0.75,16,1.0,1,0,
206,A.3 Textual Entailment,Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,100,0.8695652173913043,205,0.7536764705882353,0,0.0,1,0,
207,Our baseline SNLI model is the ESIM sequence model from .,Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,101,0.8782608695652174,206,0.7573529411764706,1,0.1,1,0,
208,"Following the original implementation , we used 300 dimensions for all LSTM and feed forward layers and pretrained 300 dimensional GloVe embeddings that were fixed during training .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,102,0.8869565217391304,207,0.7610294117647058,2,0.2,1,0,
209,"For regularization , we added 50 % variational dropout to the input of each LSTM layer and 50 % dropout at the input to the final two fully connected layers .",Evaluation,Fine tuning biLM,named-entity-recognition,4,,,,103,0.8956521739130435,208,0.7647058823529411,3,0.3,1,0,
210,All feed forward layers use ReLU activations .,Evaluation,,named-entity-recognition,4,,,,104,0.9043478260869565,209,0.7683823529411765,4,0.4,1,0,
211,"Parameters were optimized using Adam ( Kingma and with gradient norms clipped at 5.0 and initial learning rate 0.0004 , decreasing by half each time accuracy on the development set did not increase in subsequent epochs .",Evaluation,All feed forward layers use ReLU activations .,named-entity-recognition,4,,,,105,0.9130434782608695,210,0.7720588235294118,5,0.5,1,0,
212,The batch size was 32 .,Evaluation,,named-entity-recognition,4,,,,106,0.9217391304347826,211,0.7757352941176471,6,0.6,1,0,
213,"The best ELMo configuration added ELMo vectors to both the input and output of the lowest layer LSTM , using ( 1 ) with layer normalization and ? = 0.001 .",Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,107,0.9304347826086956,212,0.7794117647058824,7,0.7,1,0,
214,"Due to the increased number of parameters in the ELMo model , we added 2 regularization with regularization coefficient 0.0001 to all recurrent and feed forward weight matrices and 50 % dropout after the attention layer .",Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,108,0.9391304347826087,213,0.7830882352941176,8,0.8,1,0,
215,compares test set accuracy of our system to previously published systems .,Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,109,0.9478260869565217,214,0.7867647058823529,9,0.9,1,0,
216,"Overall , adding ELMo to the ESIM model improved accuracy by 0.7 % establishing a new single model state - of - the - art of 88.7 % , and a five member ensemble pushes the over all accuracy to 89.3 % .",Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,110,0.9565217391304348,215,0.7904411764705882,10,1.0,1,0,
217,A.4 Question Answering,Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,111,0.9652173913043478,216,0.7941176470588235,0,0.0,1,0,
218,Our QA model is a simplified version of the model from .,Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,112,0.9739130434782609,217,0.7977941176470589,1,0.25,1,0,
219,It embeds tokens by concatenating each token 's case - sensitive 300 dimensional Glo Ve word vector with a character - derived embedding produced using a convolutional neural network followed by max - pooling on learned character embeddings .,Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,113,0.9826086956521739,218,0.8014705882352942,2,0.5,1,0,
220,"The token embeddings are passed through a shared bi-directional GRU , and then the bi-directional attention mechanism from .",Evaluation,The batch size was 32 .,named-entity-recognition,4,,,,114,0.991304347826087,219,0.8051470588235294,3,0.75,1,0,
221,The augmented con-,Evaluation,,named-entity-recognition,4,,,,115,1.0,220,0.8088235294117647,4,1.0,1,0,
222,Model,,,named-entity-recognition,4,,,,0,0.0,221,0.8125,0,0.0,1,0,
223,Acc.,Model,,named-entity-recognition,4,,,,1,0.02,222,0.8161764705882353,1,0.08333333333333333,1,0,
224,"Feature based 78.2 DIIN 88.0 BCN+Char+CoVe 88.0 ESIM + TreeLSTM 88.6 ESIM+ELMo 88.7 0.17 DIIN ensemble 88.9 ESIM + ELMo ensemble 89.3 text vectors are then passed through a linear layer with ReLU activations , a residual self - attention layer that uses a GRU followed by the same attention mechanism applied context - to - context , and another linear layer with ReLU activations .",Model,Acc.,named-entity-recognition,4,,,,2,0.04,223,0.8198529411764706,2,0.16666666666666666,1,0,
225,"Finally , the results are fed through linear layers to predict the start and end token of the answer .",Model,Acc.,named-entity-recognition,4,,,,3,0.06,224,0.8235294117647058,3,0.25,1,0,
226,Variational dropout is used before the input to the GRUs and the linear layers at a rate of 0.2 .,Model,Acc.,named-entity-recognition,4,,,,4,0.08,225,0.8272058823529411,4,0.3333333333333333,1,0,
227,"A dimensionality of 90 is used for the GRUs , and 180 for the linear layers .",Model,Acc.,named-entity-recognition,4,,,,5,0.1,226,0.8308823529411765,5,0.4166666666666667,1,0,
228,We optimize the model using Adadelta with a batch size of 45 .,Model,Acc.,named-entity-recognition,4,,,,6,0.12,227,0.8345588235294118,6,0.5,1,0,
229,At test time we use an exponential moving average of the weights and limit the output span to be of at most size 17 .,Model,Acc.,named-entity-recognition,4,,,,7,0.14,228,0.8382352941176471,7,0.5833333333333334,1,0,
230,We do not update the word vectors during training .,Model,Acc.,named-entity-recognition,4,,,,8,0.16,229,0.8419117647058824,8,0.6666666666666666,1,0,
231,Performance was highest when adding ELMo without layer normalization to both the input and output of the contextual GRU layer and leaving the ELMo weights unregularized (? = 0 ) .,Model,Acc.,named-entity-recognition,4,,,,9,0.18,230,0.8455882352941176,9,0.75,1,0,
232,"compares test set results from the SQuAD leaderboard as of November 17 , 2017 when we submitted our system .",Model,Acc.,named-entity-recognition,4,,,,10,0.2,231,0.8492647058823529,10,0.8333333333333334,1,0,
233,"Overall , our submission had the highest single model and ensemble results , improving the previous single model result ( SAN ) by 1.4 % F 1 and our baseline by 4.2 % .",Model,Acc.,named-entity-recognition,4,,,,11,0.22,232,0.8529411764705882,11,0.9166666666666666,1,0,
234,"A 11 member ensemble pushes F 1 to 87.4 % , 1.0 % increase over the previous ensemble best .",Model,Acc.,named-entity-recognition,4,,,,12,0.24,233,0.8566176470588235,12,1.0,1,0,
235,A.5 Semantic Role Labeling,Model,Acc.,named-entity-recognition,4,,,,13,0.26,234,0.8602941176470589,0,0.0,1,0,
236,Our baseline SRL model is an exact reimplementation of .,Model,Acc.,named-entity-recognition,4,,,,14,0.28,235,0.8639705882352942,1,0.05263157894736842,1,0,
237,"Words are represented using a concatenation of 100 dimensional vector representations , initialized using GloVe and a binary , per-word predicate feature , represented using an 100 dimensional em-bedding .",Model,Acc.,named-entity-recognition,4,,,,15,0.3,236,0.8676470588235294,2,0.10526315789473684,1,0,
238,"This 200 dimensional token representation is then passed through an 8 layer "" interleaved "" biLSTM with a 300 dimensional hidden size , in which the directions of the LSTM layers alternate per layer .",Model,Acc.,named-entity-recognition,4,,,,16,0.32,237,0.8713235294117647,3,0.15789473684210525,1,0,
239,This deep LSTM uses Highway connections between layers and variational recurrent dropout .,Model,Acc.,named-entity-recognition,4,,,,17,0.34,238,0.875,4,0.21052631578947367,1,0,
240,This deep representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags .,Model,Acc.,named-entity-recognition,4,,,,18,0.36,239,0.8786764705882353,5,0.2631578947368421,1,0,
241,Labels consist of semantic roles from PropBank augmented with a BIO labeling scheme to represent argument spans .,Model,Acc.,named-entity-recognition,4,,,,19,0.38,240,0.8823529411764706,6,0.3157894736842105,1,0,
242,"During training , we minimize the negative log likelihood of the tag sequence using Adadelta with a learning rate of 1.0 and ? = 0.95 .",Model,Acc.,named-entity-recognition,4,,,,20,0.4,241,0.8860294117647058,7,0.3684210526315789,1,0,
243,"At test time , we perform Viterbi decoding to enforce valid spans using BIO constraints .",Model,Acc.,named-entity-recognition,4,,,,21,0.42,242,0.8897058823529411,8,0.42105263157894735,1,0,
244,Variational dropout of 10 % is added to all LSTM hidden layers .,Model,Acc.,named-entity-recognition,4,,,,22,0.44,243,0.8933823529411765,9,0.47368421052631576,1,0,
245,Gradients are clipped if their value exceeds 1.0 .,Model,,named-entity-recognition,4,,,,23,0.46,244,0.8970588235294118,10,0.5263157894736842,1,0,
246,"Models are trained for 500 epochs or until validation F1 does not improve for 200 epochs , whichever is sooner .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,24,0.48,245,0.9007352941176471,11,0.5789473684210527,1,0,
247,The pretrained Glo Ve vectors are fine - tuned during training .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,25,0.5,246,0.9044117647058824,12,0.631578947368421,1,0,
248,The final dense layer and all cells of all LSTMs are initialized to be orthogonal .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,26,0.52,247,0.9080882352941176,13,0.6842105263157895,1,0,
249,"The forget gate bias is initialized to 1 for all LSTMs , with all other gates initialized to 0 , as per . perparameters exactly following the original implementation .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,27,0.54,248,0.9117647058823529,14,0.7368421052631579,1,0,
250,The best configuration added ELMo to the input of the lowest layer biLSTM and weighted the biLM layers using ( 1 ) without any regularization ( ? = 0 ) or layer normalization .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,28,0.56,249,0.9154411764705882,15,0.7894736842105263,1,0,
251,50 % dropout was added to the ELMo representations .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,29,0.58,250,0.9191176470588235,16,0.8421052631578947,1,0,
252,compares our results with previously published results .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,30,0.6,251,0.9227941176470589,17,0.8947368421052632,1,0,
253,"Overall , we improve the single model state - of - the - art by 3.2 % average F 1 , and our single model result improves the previous ensemble best by 1.6 % F 1 .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,31,0.62,252,0.9264705882352942,18,0.9473684210526315,1,0,
254,Adding ELMo to the output from the biLSTM in addition to the biLSTM input reduced F 1 by approximately 0.7 % ( not shown ) .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,32,0.64,253,0.9301470588235294,19,1.0,1,0,
255,A.7 Named Entity Recognition,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,33,0.66,254,0.9338235294117647,0,0.0,1,0,
256,Our baseline NER model concatenates 50 dimensional pre-trained Senna vectors with a CNN character based representation .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,34,0.68,255,0.9375,1,0.09090909090909091,1,0,
257,"The character representation uses 16 dimensional character embeddings and 128 convolutional filters of width three characters , a ReLU activation and by max pooling .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,35,0.7,256,0.9411764705882353,2,0.18181818181818182,1,0,
258,"The token representation is passed through two biLSTM layers , the first with 200 hidden units and the second with 100 hidden units before a final dense layer and softmax layer .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,36,0.72,257,0.9448529411764706,3,0.2727272727272727,1,0,
259,"During training , we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,37,0.74,258,0.9485294117647058,4,0.36363636363636365,1,0,
260,Variational dropout is added to the input of both biLSTM layers .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,38,0.76,259,0.9522058823529411,5,0.45454545454545453,1,0,
261,During training the gradients are rescaled if their 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001 .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,39,0.78,260,0.9558823529411765,6,0.5454545454545454,1,0,
262,The pre-trained Senna embeddings are fine tuned during training .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,40,0.8,261,0.9595588235294118,7,0.6363636363636364,1,0,
263,We employ early stopping on the development set and report the averaged test set score across five runs with different random seeds .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,41,0.82,262,0.9632352941176471,8,0.7272727272727273,1,0,
264,ELMo was added to the input of the lowest layer task biLSTM .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,42,0.84,263,0.9669117647058824,9,0.8181818181818182,1,0,
265,"As the CoNLL 2003 NER data set is relatively small , we found the best performance by constraining the trainable layer weights to be effectively constant by setting ? = 0.1 with ( 1 ) .",Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,43,0.86,264,0.9705882352941176,10,0.9090909090909091,1,0,
266,from all layers of the biLM provides a modest improvement .,Model,Gradients are clipped if their value exceeds 1.0 .,named-entity-recognition,4,,,,44,0.88,265,0.9742647058823529,11,1.0,1,0,
267,A.8 Sentiment classification,Model,,named-entity-recognition,4,,,,45,0.9,266,0.9779411764705882,0,0.0,1,0,
268,"We use almost the same biattention classification network architecture described in , with the exception of replacing the final maxout network with a simpler feedforward network composed of two ReLu layers with dropout .",Model,A.8 Sentiment classification,named-entity-recognition,4,,,,46,0.92,267,0.9816176470588235,1,0.2,1,0,
269,"A BCN model with a batch - normalized maxout network reached significantly lower validation accuracies in our experiments , although there maybe discrepancies between our implementation and that of .",Model,A.8 Sentiment classification,named-entity-recognition,4,,,,47,0.94,268,0.9852941176470589,2,0.4,1,0,
270,"To match the CoVe training setup , we only train on phrases that contain four or more tokens .",Model,A.8 Sentiment classification,named-entity-recognition,4,,,,48,0.96,269,0.9889705882352942,3,0.6,1,0,
271,We use 300 -d hidden states for the biLSTM and optimize the model parameters with Adam ( Kingma and using a learning rate of 0.0001 .,Model,A.8 Sentiment classification,named-entity-recognition,4,,,,49,0.98,270,0.9926470588235294,4,0.8,1,0,
272,"The trainable biLM layer weights are regularized by ? = 0.001 , and we add ELMo to both the input and output of the biLSTM ; the output ELMo vectors are computed with a second biLSTM and concatenated to the input .",Model,A.8 Sentiment classification,named-entity-recognition,4,,,,50,1.0,271,0.9963235294117647,5,1.0,1,0,
