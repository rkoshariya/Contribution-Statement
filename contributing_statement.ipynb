{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfBXmoW0FjQ9",
        "outputId": "90c55736-f6d4-4095-8ee7-bbd816aebabe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhpovOo1vvDP"
      },
      "source": [
        "Preprocessing of Trial Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rjr1MsoNKphJ"
      },
      "outputs": [],
      "source": [
        "def find_source(data, ls = []):\n",
        "    if isinstance(data,dict):\n",
        "        for key in data.keys():\n",
        "            if key==\"from sentence\":\n",
        "                sentences=data[key].split('\\n') # The value can be one or more sentences\n",
        "                for s in sentences:\n",
        "                    ls.append(s.strip())\n",
        "            elif isinstance(data[key], dict):\n",
        "                find_source(data[key],ls)\n",
        "            elif isinstance(data[key], list):\n",
        "                for i in data[key]:\n",
        "                    find_source(i,ls)\n",
        "    elif isinstance(data,list):\n",
        "        for i in data:\n",
        "            find_source(i,ls)\n",
        "    return ls    # might have repeated sentences\n",
        "\n",
        "# determine if a triple is contained in the trace of traversing the json object in a depth-firsr manner\n",
        "def is_contained(trace, triple):\n",
        "    if len(trace) >= len(triple):\n",
        "        for i in range(len(trace)-2):\n",
        "            if trace[i:i+3] == triple:\n",
        "                return True\n",
        "        return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Get the previous two words in a trace,\n",
        "# to be prefixed to the coordinated items in a list or a dictionary\n",
        "def get_prefix(data, trace):\n",
        "    if isinstance(data, dict) or isinstance(data, list):\n",
        "        return trace[-2:]\n",
        "\n",
        "# traverse the json object recursively,\n",
        "# to find the source sentence of the triple\n",
        "def find_tri_sent(data, triple, trace=[], ls=[], prefix=[]):\n",
        "    # Parse the json file recursively and return a list of source sentences\n",
        "    if isinstance(data, dict):\n",
        "        for i, key in enumerate(data.keys()):\n",
        "            if key != \"from sentence\":\n",
        "                if prefix and i != 0:\n",
        "                    trace += prefix\n",
        "                trace.append(key)\n",
        "                find_tri_sent(data[key], triple, trace, ls,\n",
        "                              get_prefix(data[key], trace))\n",
        "            else:\n",
        "                if is_contained(trace, triple):\n",
        "                    ls.append(data[key].strip())\n",
        "    elif isinstance(data, list):\n",
        "        for i, item in enumerate(data):\n",
        "            if prefix and i != 0:\n",
        "                trace += prefix\n",
        "            find_tri_sent(item, triple, trace, ls, prefix)\n",
        "    elif isinstance(data, str):\n",
        "        trace.append(data)\n",
        "    return ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyqoUEFsEMtV",
        "outputId": "3d66a0af-5027-4074-a21d-064a2596da78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not find the phrase 'https://github.com/allenai/scibert/' in the 9th sentence of 'relation-classification' paper 9\n",
            "\n",
            "\"all_sent.csv\" and \"pos_sent.csv\" have been saved to ./interim\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Data preprocessing and cleaning\n",
        "get a dataframe of all sentences, together with relevant information to the tasks\n",
        "'''\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/Scispace/training_data'\n",
        "sep = os.path.sep\n",
        "\n",
        "def get_dir(topic_ls=None, paper_ls=None):\n",
        "    # Get the list of paper directories\n",
        "    dir_ls = []\n",
        "    if topic_ls is None:\n",
        "        topic_ls = os.listdir(base_dir)\n",
        "        topic_ls.remove('train-README.md')\n",
        "        topic_ls.remove('trial-README.md')\n",
        "    if paper_ls is None:\n",
        "        for topic in topic_ls:\n",
        "            paper_ls = os.listdir(os.path.join(base_dir, topic))\n",
        "            for i in paper_ls:\n",
        "                dir_ls.append(os.path.join(base_dir, topic, i))\n",
        "    else:\n",
        "        for topic in topic_ls:\n",
        "            for i in paper_ls:\n",
        "                dir_ls.append(os.path.join(base_dir, topic, str(i)))\n",
        "    return dir_ls\n",
        "\n",
        "def get_file_path(dirs):\n",
        "    # Get the relevant files from each directory of paper.\n",
        "    rx = '(.*Stanza-out.txt$)|(^sentences.txt$)'\n",
        "    file_path = []\n",
        "    for dir in dirs:\n",
        "        new = ['', '']  # stores the paths of the sentence file and the label file\n",
        "        for file in os.listdir(dir):\n",
        "            res = re.match(rx, file)\n",
        "            if res:\n",
        "                if res.group(1):\n",
        "                    new[0] = os.path.join(dir, file)\n",
        "                if res.group(2):\n",
        "                    new[1] = os.path.join(dir, file)\n",
        "        file_path.append(new)\n",
        "    return file_path\n",
        "\n",
        "def is_heading(line):\n",
        "    # Determine if a line is a heading\n",
        "    ls = line.split(' ')\n",
        "    # Titles rarely end with these words\n",
        "    False_end = ['by', 'as', 'in', 'and', 'that']\n",
        "    if len(ls) < 10 and ls[-1] not in False_end:\n",
        "        rx = '^[A-Z][^?]*[^?:]$|^title$|^abstract$'  # regex heuristic rules\n",
        "        res = re.match(rx, line)\n",
        "        return True if res else False\n",
        "    return False\n",
        "\n",
        "def is_main_heading(line, judge_mask=False):\n",
        "    '''\n",
        "    Assume that the line is a heading, determine if it is a main heading\n",
        "    A main heading is either a typical main section heading, or it contains lexical cues that are considered important for judgement.\n",
        "    '''\n",
        "    if len(line.split(' ')) <= 4:\n",
        "        if judge_mask:    # if the aim is to judge whether the sentence should be skipped\n",
        "            lex_cue = 'background|related|conclusion'  # |related work\n",
        "        else:\n",
        "            lex_cue = 'title|abstract|introduction|background|related|conclusion|model|models|method|methods|approach|architecture|system|application|experiment|experiments|experimental setup|implementation|hyperparameters|training|result|results|ablation|baseline|evaluation'  # |related work\n",
        "        exp = re.compile(lex_cue)\n",
        "        # Decide if it is a main heading\n",
        "        return True if exp.search(line.lower()) else False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Determin if a sentence conforms to a specific case method.\n",
        "# There are three case methods in all, eg: Attention Is All You Need; ATTENTION IS ALL YOU NEED; Attention is all you need\n",
        "\n",
        "def check_case(line, flag):\n",
        "    if flag == 1:\n",
        "        match = re.search(r'[a-z]', line)\n",
        "        if match:\n",
        "            return False\n",
        "        return True\n",
        "    else:\n",
        "        wd_num = 0\n",
        "        words = line.split(' ')\n",
        "        if flag == 0:\n",
        "            stp_wd = ['a', 'an', 'and', 'the', 'or', 'if', 'by', 'as', 'to', \n",
        "            'of', 'for', 'in', 'on', 'but', 'via', 'nor', 'with']\n",
        "            if not words[0].istitle():\n",
        "                wd_num += 1\n",
        "            if len(words) > 1:\n",
        "                if not words[-1].istitle():\n",
        "                    wd_num += 1\n",
        "                for word in words[1:-1]:\n",
        "                    if not word.istitle() and word not in stp_wd:\n",
        "                        wd_num += 1\n",
        "            return wd_num <= math.ceil(len(words)/5)\n",
        "        if flag == 2:\n",
        "            if not words[0].istitle():\n",
        "                wd_num += 1\n",
        "            for word in words[1:]:\n",
        "                if re.match(r'[A-Z]', word):\n",
        "                    wd_num += 1\n",
        "            return wd_num <= math.ceil(len(words)/3)\n",
        "\n",
        "# read the relevant files from the folder of one paper, and produce a data table for that paper.\n",
        "def load_paper_sentence(sent_path, label_path):\n",
        "    sent = []\n",
        "    count = [0, 0, 0]\n",
        "    task, index = sent_path.split(sep)[-3:-1]\n",
        "    # Decide the case type of the titles in this paper, by counting over the main headings and find the maximum\n",
        "    with open(sent_path, 'r') as f:\n",
        "        while(True):\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                if is_heading(line) and is_main_heading(line):\n",
        "                    for m in range(3):\n",
        "                        if check_case(line, m):\n",
        "                            count[m] += 1\n",
        "            else:\n",
        "                break\n",
        "    ocr_path = sent_path[:-14]+'Grobid-out.txt'\n",
        "    with open(ocr_path, 'r') as f:\n",
        "        fl=f.readlines()\n",
        "    title_ls = []\n",
        "    for i in range(len(fl)):\n",
        "        if fl[i]=='\\n':\n",
        "            if i<(len(fl)-1):\n",
        "                title_ls.append(fl[i+1].rstrip())\n",
        "        if fl[i].rstrip().lower() in ['title','abstract','introduction']:\n",
        "            title_ls.append(fl[i].rstrip())\n",
        "\n",
        "    with open(sent_path, 'r') as f:\n",
        "        i = 0\n",
        "        flg = count.index(max(count))\n",
        "        # two string buffers, storing the heading and the main heading respectively\n",
        "        heading, main_h = '', ''\n",
        "        ofs1 = ofs3 = 0\n",
        "        while(True):\n",
        "            i += 1\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                if line in title_ls:\n",
        "                    ofs3 = 0\n",
        "                else:\n",
        "                    ofs3 += 1\n",
        "                if is_heading(line) and check_case(line, flg):\n",
        "                    heading = line    # update the heading buffer\n",
        "                    if is_main_heading(line):\n",
        "                        ofs1 = 0\n",
        "                        main_h = line    # update the main heading buffer too\n",
        "                        # The line itself is a main heading, no heading needs to be stored.\n",
        "                        sent.append(\n",
        "                            [i, line, '', '', task, index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 1, 0, None])\n",
        "                    else:\n",
        "                        ofs1 += 1\n",
        "                        # for plain headings, store the main heading it belongs to.\n",
        "                        # judge if it should be masked\n",
        "                        if is_main_heading(main_h, judge_mask=True):\n",
        "                            sent.append([i, line, main_h, '', task,\n",
        "                                         index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 0, 0, None])\n",
        "                        else:\n",
        "                            sent.append([i, line, main_h, '', task,\n",
        "                                         index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 1, 0, None])\n",
        "                else:\n",
        "                    # For plain text line, store both the heading and the main heading.\n",
        "                    ofs1 += 1\n",
        "                    if is_main_heading(main_h, judge_mask=True):\n",
        "                        sent.append([i, line, main_h, heading, \n",
        "                                     task, index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 0, 0, None])\n",
        "                    else:\n",
        "                        sent.append([i, line, main_h, heading, \n",
        "                                     task, index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 1, 0, None])\n",
        "            else:\n",
        "                break\n",
        "    for i in range(1,len(sent)):\n",
        "        if sent[i][9]==0:\n",
        "            sof = sent[i-1][9]\n",
        "            if sof>1:\n",
        "                for j in range(i-sof,i):\n",
        "                    sent[j][10] = sent[j][9]/sof\n",
        "        if sent[i][13] == 0:\n",
        "            sof = sent[i-1][13]\n",
        "            if sof>1:\n",
        "                for j in range(i-sof, i):\n",
        "                    sent[j][14] = sent[j][13]/sof\n",
        "        if i == len(sent)-1:\n",
        "            sof = sent[i][9]\n",
        "            if sof > 1:\n",
        "                for j in range(i-sof+1, i+1):\n",
        "                    sent[j][10] = sent[j][9]/sof\n",
        "            sof = sent[i][13]\n",
        "            if sof > 1:\n",
        "                for j in range(i-sof+1, i+1):\n",
        "                    sent[j][14] = sent[j][13]/sof\n",
        "        sent[i][12] = sent[i][11]/len(sent)\n",
        "\n",
        "    # slice the sentence with the span of characters, returns the span of words\n",
        "    def get_word_idx(sent, start, end):\n",
        "        ls = sent.split(' ')\n",
        "        if isinstance(start, str):\n",
        "            start = int(start)\n",
        "        if isinstance(end, str):\n",
        "            end = int(end)\n",
        "        # if the span of characters doesn't conform to word boundaries, 'st' and 'en' will remain 0.\n",
        "        st, en = 0, 0\n",
        "        length = [len(word) for word in ls]\n",
        "        count = 0\n",
        "        for i in range(len(ls)):\n",
        "            if start == count:\n",
        "                st = i\n",
        "                break\n",
        "            count += (length[i]+1)\n",
        "        for j in range(st, len(ls)):\n",
        "            count += (length[j]+1)\n",
        "            if end == (count-1):\n",
        "                en = j + 1\n",
        "                break\n",
        "        return st, en\n",
        "\n",
        "    # Mark the label of contribution-ralated sentences, and initialize their BIO tag sequences.\n",
        "    with open(label_path, 'r') as f:\n",
        "        while(True):\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                sent[int(line)-1][-2] = 1\n",
        "                sent[int(line)-1][6] = ['O'] * \\\n",
        "                    len(sent[int(line)-1][1].split(' '))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # go over the entities and change the corresponding part of BIO sequences\n",
        "    ent_path = sep.join(label_path.split(sep)[:-1]+['entities.txt'])\n",
        "    with open(ent_path, 'r') as f:\n",
        "        while(True):\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                info = line.split('\\t')\n",
        "                sentence = sent[int(info[0])-1][1]\n",
        "                # if sentence.split(' ')[0].lower()[1:] == sentence.split(' ')[0][1:]:\n",
        "                # sentence = sentence[0].lower() + sentence[1:]\n",
        "                st, en = get_word_idx(sentence, info[1], info[2])\n",
        "                phrase = info[3].strip()\n",
        "                # If the span of characters does not match the given phrase, use the given phrase instead\n",
        "                if ' '.join(sentence.split(' ')[st: en]).strip() != phrase:\n",
        "                    st_char = (' ' + sentence).find(' ' + phrase + ' ')\n",
        "                    st, en = get_word_idx(\n",
        "                        sentence, st_char, st_char + len(phrase))\n",
        "                    if st == 0 and en == 0:\n",
        "                        print(\n",
        "                            f'Could not find the phrase \\'{info[3]}\\' in the {int(info[0])}th sentence of \\'{task}\\' paper {index}')\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(\n",
        "                            f'In the {int(info[0])}th sentence of \\'{task}\\' paper {index}, the entity \\'{info[3]}\\' is not in the span ({info[1]}, {info[2]})')\n",
        "                for j in range(st, en):\n",
        "                    if sent[int(info[0])-1][6] is None:\n",
        "                        print(\n",
        "                            f'A phrase exists in the {int(info[0])}th sentence of \\'{task}\\' paper {index}, which is not labeled as a contribution sentence.')\n",
        "                        sent[int(info[0])-1][6] = ['O'] * \\\n",
        "                            len(sent[int(info[0])-1][1].split(' '))\n",
        "                    if j == st:\n",
        "                        sent[int(info[0])-1][6][j] = 'B'\n",
        "                    else:\n",
        "                        sent[int(info[0])-1][6][j] = 'I'\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # decide which information unit each positive sentence belongs to.\n",
        "    j_dir = sep.join(sent_path.split(sep)[:-1]) + sep + 'info-units'\n",
        "    for unit in os.listdir(j_dir):  # For each json file representing an information unit\n",
        "        js_file = os.path.join(j_dir, unit)\n",
        "        try:\n",
        "            with open(js_file, 'r') as f:\n",
        "                data = json.load(f, strict=False)\n",
        "            lst = find_source(data, [])\n",
        "            if \"TITLE\" in lst:  # When the title is a source sentence, sometimes it is abbreviated as 'TITLE'\n",
        "                sent[1][-1] = unit[:-5]\n",
        "            for j in range(len(sent)):\n",
        "                if sent[j][1] in lst:\n",
        "                    sent[j][-1] = unit[:-5]\n",
        "        except json.JSONDecodeError as e:\n",
        "            js_position = sep.join(js_file.split(sep)[-4:])\n",
        "            print(f'JSONDecodeError in {js_position}\\n', e)\n",
        "            continue\n",
        "\n",
        "    # given a sequence of BIO tags, get the list of tuples representing spans of entities\n",
        "    def get_entity_spans(ls):\n",
        "        spans = []\n",
        "        for i in range(len(ls)):\n",
        "            st, ed = 0, 0\n",
        "            if ls[i] == 'B':\n",
        "                st, ed = i, i + 1\n",
        "                for j in range(i+1, len(ls)):\n",
        "                    if ls[j] == 'I':\n",
        "                        ed += 1\n",
        "                    else:\n",
        "                        break\n",
        "                spans.append((st, ed))\n",
        "        return spans\n",
        "\n",
        "    for i in range(len(sent)):\n",
        "        if sent[i][6] is not None:\n",
        "            sent[i][8] = sent[i][7] = sent[i][6]\n",
        "\n",
        "    # try to find the SPO(Subject, Predicate, Object) type of each phrase\n",
        "    aux = []\n",
        "    for i in range(len(sent)):\n",
        "        if sent[i][6] is not None:\n",
        "            tup_ls = get_entity_spans(sent[i][6])\n",
        "            # use three booleans to indicate if the phrase has ever been a subject, predicate or object\n",
        "            tuple_ls = [[0, 0, 0, tup] for tup in tup_ls]\n",
        "            word_ls = sent[i][1].split(' ')\n",
        "            phrase_ls = [' '.join(word_ls[st:en]) for st, en in tup_ls]\n",
        "            # store the sentence idx, tuple_ls, and phrase_ls.\n",
        "            aux.append([i, tuple_ls, phrase_ls, []])        \n",
        "    t_dir = sep.join(sent_path.split(sep)[:-1]) + sep + 'triples'\n",
        "    paper_triple_stat = [0] * 5\n",
        "    for unit in os.listdir(t_dir):\n",
        "        t_file = os.path.join(t_dir, unit)\n",
        "        js_file = os.path.join(j_dir, unit.replace('.txt', '.json'))\n",
        "        try:\n",
        "            with open(js_file,'r') as g:\n",
        "                js = json.load(g, strict=False)\n",
        "                js = {'Contribution': js}\n",
        "        except json.JSONDecodeError as e:\n",
        "            js_position = sep.join(js_file.split(sep)[-4:])\n",
        "            print(f'JSONDecodeError in {js_position}\\n', e)\n",
        "            continue\n",
        "        except FileNotFoundError as fe:\n",
        "            print(fe)\n",
        "            continue\n",
        "        with open(t_file, 'r') as f:\n",
        "            while(True):\n",
        "                line = f.readline().rstrip(\"\\n\")\n",
        "                if line:\n",
        "                    # empty the temporary buffer\n",
        "                    for a in range(len(aux)):\n",
        "                        aux[a][3] = []\n",
        "                    if line[0] == '(':\n",
        "                        line = line[1:]\n",
        "                    if line[-1] == ')':\n",
        "                        line = line[:-1]\n",
        "                    triple = line.split('||')\n",
        "                    evidence = find_tri_sent(\n",
        "                        js, triple, [], [], [])  # unit[:-4]\n",
        "                    if not evidence:\n",
        "                        js_position = sep.join(js_file.split(sep)[-4:]) #\n",
        "                        paper_triple_stat[0] += 1\n",
        "                        print(f'the triple \\'{triple}\\' not found in {js_position}')\n",
        "                    else:                       \n",
        "                        cands = evidence[0].split('\\n')\n",
        "                        for i in range(len(cands)):\n",
        "                            for j in range(len(aux)):\n",
        "                                if cands[i].strip() == sent[aux[j][0]][1]:\n",
        "                                    for w in range(3):\n",
        "                                        for k in range(len(aux[j][2])):\n",
        "                                            if aux[j][2][k] == triple[w]:\n",
        "                                                aux[j][3].append((w, k))\n",
        "                                                break\n",
        "                                    break\n",
        "                        lens = [len(aux[j][3]) for j in range(len(aux))]\n",
        "                        try:\n",
        "                            paper_triple_stat[max(lens)] += 1\n",
        "                        except IndexError:\n",
        "                            print(f'List index out of range. The actual number of max is {max(lens)} for triple \\'{triple}\\' in\\n', t_file)\n",
        "                        if max(lens)!=0:\n",
        "                            idx = lens.index(max(lens))\n",
        "                            found = [0, 0, 0]\n",
        "                            for t in range(len(aux[idx][3])):\n",
        "                                w, k = aux[idx][3][t]\n",
        "                                aux[idx][1][k][w] = 1\n",
        "                                found[w] = 1\n",
        "                            for i in range(3):\n",
        "                                if found[i] == 0:\n",
        "                                    for j in range(len(aux)):\n",
        "                                        for w, k in aux[j][3]:\n",
        "                                            if w == i and triple[w] == aux[j][2][k]:\n",
        "                                                aux[j][1][k][w] = 1\n",
        "                                                break\n",
        "                                        else:\n",
        "                                            continue\n",
        "                                        break\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "    # An S-P-O type corresponds to a combination of boolean indicators\n",
        "    # The 4 keys stand for 'predicate', 'subject', 'object', 'both subject and object' respectively.\n",
        "    good_state = {'p': [0, 1, 0], 's': [1, 0, 0],\n",
        "                    'ob': [0, 0, 1], 'b': [1, 0, 1]}\n",
        "    for i in range(len(aux)):\n",
        "        for item in aux[i][1]:\n",
        "            if item[:3] not in good_state.values():\n",
        "                # if the label of any phrase in the sentence cannot be decided,\n",
        "                # delete the tag sequence to filter out this sentence\n",
        "                sent[aux[i][0]][7] = sent[aux[i][0]][8] = None\n",
        "                break\n",
        "\n",
        "    for i in range(len(aux)):\n",
        "        '''\n",
        "        interprete the boolean states to phrase types according to the BIO_type setting,\n",
        "        and change the corresponding parts in BIO sequences\n",
        "        BIO_type=1: decide whether it is a predicate\n",
        "        BIO_type=2: decide which of the four keys in 'good_state' it belongs to\n",
        "        '''\n",
        "        if sent[aux[i][0]][7] is not None:\n",
        "            sent[aux[i][0]][7] = ['O']*len(sent[aux[i][0]][7])\n",
        "            sent[aux[i][0]][8] = ['O']*len(sent[aux[i][0]][8])\n",
        "            for item in aux[i][1]:\n",
        "                st, en = item[3]\n",
        "                if item[:3] == good_state['p']:\n",
        "                    sent[aux[i][0]][7][st] = 'B-p'\n",
        "                    for j in range(st+1, en):\n",
        "                        sent[aux[i][0]][7][j] = 'I-p'\n",
        "                else:\n",
        "                    sent[aux[i][0]][7][st] = 'B-n'\n",
        "                    for j in range(st+1, en):\n",
        "                        sent[aux[i][0]][7][j] = 'I-n'\n",
        "                for key, value in good_state.items():\n",
        "                    if item[:3] == value:\n",
        "                        sent[aux[i][0]][8][st] = 'B-'+key\n",
        "                        for j in range(st+1, en):\n",
        "                            sent[aux[i][0]][8][j] = 'I-'+key\n",
        "    # print(f'paper triple stat: {paper_triple_stat}')\n",
        "    return sent, paper_triple_stat\n",
        "\n",
        "def load_data_sentence(file_path):\n",
        "    # Get the data table of all the papers in file_path\n",
        "    triple_stat = [0] * 5\n",
        "    data = []\n",
        "    for tuple in file_path:\n",
        "        sentence_path, label_path = tuple\n",
        "        paper_data, paper_triple_stat = load_paper_sentence(\n",
        "        sentence_path, label_path)\n",
        "        for i in range(5):\n",
        "            triple_stat[i] += paper_triple_stat[i]\n",
        "        data += paper_data\n",
        "    return data\n",
        "\n",
        "dirs = get_dir()\n",
        "file_path = get_file_path(dirs)\n",
        "data = load_data_sentence(file_path)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.columns = ['idx', 'text', 'main_heading', 'heading',\n",
        "              'topic', 'paper_idx', 'BIO', 'BIO_1', 'BIO_2', 'offset1', 'pro1', 'offset2', 'pro2', 'offset3', 'pro3', 'mask', 'bi_labels', 'labels']\n",
        "\n",
        "df.to_csv('all_sent.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-6e2WzPIVbju"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('all_sent.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n9X7L6nZWIEu",
        "outputId": "131dacc3-6d04-4c87-e3b7-e424f41d16e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-701f3125-8020-4df7-859d-d3d9c67fa531\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>main_heading</th>\n",
              "      <th>heading</th>\n",
              "      <th>topic</th>\n",
              "      <th>paper_idx</th>\n",
              "      <th>BIO</th>\n",
              "      <th>BIO_1</th>\n",
              "      <th>BIO_2</th>\n",
              "      <th>offset1</th>\n",
              "      <th>pro1</th>\n",
              "      <th>offset2</th>\n",
              "      <th>pro2</th>\n",
              "      <th>offset3</th>\n",
              "      <th>pro3</th>\n",
              "      <th>mask</th>\n",
              "      <th>bi_labels</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>title</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Universal Sentence Encoder</td>\n",
              "      <td>title</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>['B', 'I', 'I']</td>\n",
              "      <td>['B-n', 'I-n', 'I-n']</td>\n",
              "      <td>['B-ob', 'I-ob', 'I-ob']</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>research-problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>abstract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013514</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We present models for encoding sentences into ...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', ...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', '...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>3</td>\n",
              "      <td>0.020270</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>research-problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>The models are efficient and result in accurat...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>4</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1875</th>\n",
              "      <td>268</td>\n",
              "      <td>We use almost the same biattention classificat...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>46</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>267</td>\n",
              "      <td>0.981618</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1876</th>\n",
              "      <td>269</td>\n",
              "      <td>A BCN model with a batch - normalized maxout n...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>47</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>268</td>\n",
              "      <td>0.985294</td>\n",
              "      <td>2</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1877</th>\n",
              "      <td>270</td>\n",
              "      <td>To match the CoVe training setup , we only tra...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>48</td>\n",
              "      <td>0.960000</td>\n",
              "      <td>269</td>\n",
              "      <td>0.988971</td>\n",
              "      <td>3</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1878</th>\n",
              "      <td>271</td>\n",
              "      <td>We use 300 -d hidden states for the biLSTM and...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>49</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>270</td>\n",
              "      <td>0.992647</td>\n",
              "      <td>4</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1879</th>\n",
              "      <td>272</td>\n",
              "      <td>The trainable biLM layer weights are regulariz...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>271</td>\n",
              "      <td>0.996324</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1880 rows Ã— 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-701f3125-8020-4df7-859d-d3d9c67fa531')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-701f3125-8020-4df7-859d-d3d9c67fa531 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-701f3125-8020-4df7-859d-d3d9c67fa531');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      idx                                               text main_heading  \\\n",
              "0       1                                              title          NaN   \n",
              "1       2                         Universal Sentence Encoder        title   \n",
              "2       3                                           abstract          NaN   \n",
              "3       4  We present models for encoding sentences into ...     abstract   \n",
              "4       5  The models are efficient and result in accurat...     abstract   \n",
              "...   ...                                                ...          ...   \n",
              "1875  268  We use almost the same biattention classificat...        Model   \n",
              "1876  269  A BCN model with a batch - normalized maxout n...        Model   \n",
              "1877  270  To match the CoVe training setup , we only tra...        Model   \n",
              "1878  271  We use 300 -d hidden states for the biLSTM and...        Model   \n",
              "1879  272  The trainable biLM layer weights are regulariz...        Model   \n",
              "\n",
              "                           heading                     topic  paper_idx  \\\n",
              "0                              NaN       text-classification          6   \n",
              "1                              NaN       text-classification          6   \n",
              "2                              NaN       text-classification          6   \n",
              "3                         abstract       text-classification          6   \n",
              "4                         abstract       text-classification          6   \n",
              "...                            ...                       ...        ...   \n",
              "1875  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "1876  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "1877  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "1878  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "1879  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "\n",
              "                                                    BIO  \\\n",
              "0                                                   NaN   \n",
              "1                                       ['B', 'I', 'I']   \n",
              "2                                                   NaN   \n",
              "3     ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', ...   \n",
              "4                                                   NaN   \n",
              "...                                                 ...   \n",
              "1875                                                NaN   \n",
              "1876                                                NaN   \n",
              "1877                                                NaN   \n",
              "1878                                                NaN   \n",
              "1879                                                NaN   \n",
              "\n",
              "                                                  BIO_1  \\\n",
              "0                                                   NaN   \n",
              "1                                 ['B-n', 'I-n', 'I-n']   \n",
              "2                                                   NaN   \n",
              "3     ['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n...   \n",
              "4                                                   NaN   \n",
              "...                                                 ...   \n",
              "1875                                                NaN   \n",
              "1876                                                NaN   \n",
              "1877                                                NaN   \n",
              "1878                                                NaN   \n",
              "1879                                                NaN   \n",
              "\n",
              "                                                  BIO_2  offset1      pro1  \\\n",
              "0                                                   NaN        0  0.000000   \n",
              "1                              ['B-ob', 'I-ob', 'I-ob']        1  0.000000   \n",
              "2                                                   NaN        0  0.000000   \n",
              "3     ['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', '...        1  0.111111   \n",
              "4                                                   NaN        2  0.222222   \n",
              "...                                                 ...      ...       ...   \n",
              "1875                                                NaN       46  0.920000   \n",
              "1876                                                NaN       47  0.940000   \n",
              "1877                                                NaN       48  0.960000   \n",
              "1878                                                NaN       49  0.980000   \n",
              "1879                                                NaN       50  1.000000   \n",
              "\n",
              "      offset2      pro2  offset3      pro3  mask  bi_labels            labels  \n",
              "0           0  0.000000        0  0.000000     1          0               NaN  \n",
              "1           1  0.006757        1  0.000000     1          1  research-problem  \n",
              "2           2  0.013514        0  0.000000     1          0               NaN  \n",
              "3           3  0.020270        1  0.111111     1          1  research-problem  \n",
              "4           4  0.027027        2  0.222222     1          0               NaN  \n",
              "...       ...       ...      ...       ...   ...        ...               ...  \n",
              "1875      267  0.981618        1  0.200000     1          0               NaN  \n",
              "1876      268  0.985294        2  0.400000     1          0               NaN  \n",
              "1877      269  0.988971        3  0.600000     1          0               NaN  \n",
              "1878      270  0.992647        4  0.800000     1          0               NaN  \n",
              "1879      271  0.996324        5  1.000000     1          0               NaN  \n",
              "\n",
              "[1880 rows x 18 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "H5RIqT-lWO2V",
        "outputId": "d4af17de-e08f-4b51-aabd-502d8a65aa56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0a95e8ae-de67-41af-a074-ada22bd68a00\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>main_heading</th>\n",
              "      <th>heading</th>\n",
              "      <th>topic</th>\n",
              "      <th>paper_idx</th>\n",
              "      <th>BIO</th>\n",
              "      <th>BIO_1</th>\n",
              "      <th>BIO_2</th>\n",
              "      <th>offset1</th>\n",
              "      <th>pro1</th>\n",
              "      <th>offset2</th>\n",
              "      <th>pro2</th>\n",
              "      <th>offset3</th>\n",
              "      <th>pro3</th>\n",
              "      <th>mask</th>\n",
              "      <th>bi_labels</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>title</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Universal Sentence Encoder</td>\n",
              "      <td>title</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>['B', 'I', 'I']</td>\n",
              "      <td>['B-n', 'I-n', 'I-n']</td>\n",
              "      <td>['B-ob', 'I-ob', 'I-ob']</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>research-problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>abstract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013514</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We present models for encoding sentences into ...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', ...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', '...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>3</td>\n",
              "      <td>0.020270</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>research-problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>The models are efficient and result in accurat...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>4</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a95e8ae-de67-41af-a074-ada22bd68a00')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a95e8ae-de67-41af-a074-ada22bd68a00 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a95e8ae-de67-41af-a074-ada22bd68a00');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   idx                                               text main_heading  \\\n",
              "0    1                                              title          NaN   \n",
              "1    2                         Universal Sentence Encoder        title   \n",
              "2    3                                           abstract          NaN   \n",
              "3    4  We present models for encoding sentences into ...     abstract   \n",
              "4    5  The models are efficient and result in accurat...     abstract   \n",
              "\n",
              "    heading                topic  paper_idx  \\\n",
              "0       NaN  text-classification          6   \n",
              "1       NaN  text-classification          6   \n",
              "2       NaN  text-classification          6   \n",
              "3  abstract  text-classification          6   \n",
              "4  abstract  text-classification          6   \n",
              "\n",
              "                                                 BIO  \\\n",
              "0                                                NaN   \n",
              "1                                    ['B', 'I', 'I']   \n",
              "2                                                NaN   \n",
              "3  ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', ...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                               BIO_1  \\\n",
              "0                                                NaN   \n",
              "1                              ['B-n', 'I-n', 'I-n']   \n",
              "2                                                NaN   \n",
              "3  ['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                               BIO_2  offset1      pro1  \\\n",
              "0                                                NaN        0  0.000000   \n",
              "1                           ['B-ob', 'I-ob', 'I-ob']        1  0.000000   \n",
              "2                                                NaN        0  0.000000   \n",
              "3  ['O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', '...        1  0.111111   \n",
              "4                                                NaN        2  0.222222   \n",
              "\n",
              "   offset2      pro2  offset3      pro3  mask  bi_labels            labels  \n",
              "0        0  0.000000        0  0.000000     1          0               NaN  \n",
              "1        1  0.006757        1  0.000000     1          1  research-problem  \n",
              "2        2  0.013514        0  0.000000     1          0               NaN  \n",
              "3        3  0.020270        1  0.111111     1          1  research-problem  \n",
              "4        4  0.027027        2  0.222222     1          0               NaN  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wJLAB9tNWZGO"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop(columns=['BIO','BIO_1','BIO_2','offset1','pro1','offset2','pro2','offset3','pro3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "SAsAHLZnC6kv",
        "outputId": "743e22e0-bc69-4f1c-ce15-614dccf10ce7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eefe44f4-c85f-4167-956f-7e1706852393\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>main_heading</th>\n",
              "      <th>heading</th>\n",
              "      <th>topic</th>\n",
              "      <th>paper_idx</th>\n",
              "      <th>mask</th>\n",
              "      <th>bi_labels</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>title</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>text_generation</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Improved Variational Autoencoders for Text Mod...</td>\n",
              "      <td>title</td>\n",
              "      <td>title</td>\n",
              "      <td>text_generation</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>research-problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>abstract</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>text_generation</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Recent work on generative text modeling has fo...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text_generation</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>research-problem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>This negative result is so far poorly understo...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text_generation</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57076</th>\n",
              "      <td>268</td>\n",
              "      <td>We use almost the same biattention classificat...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57077</th>\n",
              "      <td>269</td>\n",
              "      <td>A BCN model with a batch - normalized maxout n...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57078</th>\n",
              "      <td>270</td>\n",
              "      <td>To match the CoVe training setup , we only tra...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57079</th>\n",
              "      <td>271</td>\n",
              "      <td>We use 300 -d hidden states for the biLSTM and...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57080</th>\n",
              "      <td>272</td>\n",
              "      <td>The trainable biLM layer weights are regulariz...</td>\n",
              "      <td>Model</td>\n",
              "      <td>A.8 Sentiment classification</td>\n",
              "      <td>named-entity-recognition</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57081 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eefe44f4-c85f-4167-956f-7e1706852393')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eefe44f4-c85f-4167-956f-7e1706852393 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eefe44f4-c85f-4167-956f-7e1706852393');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       idx                                               text main_heading  \\\n",
              "0        1                                              title          nan   \n",
              "1        2  Improved Variational Autoencoders for Text Mod...        title   \n",
              "2        3                                           abstract          nan   \n",
              "3        4  Recent work on generative text modeling has fo...     abstract   \n",
              "4        5  This negative result is so far poorly understo...     abstract   \n",
              "...    ...                                                ...          ...   \n",
              "57076  268  We use almost the same biattention classificat...        Model   \n",
              "57077  269  A BCN model with a batch - normalized maxout n...        Model   \n",
              "57078  270  To match the CoVe training setup , we only tra...        Model   \n",
              "57079  271  We use 300 -d hidden states for the biLSTM and...        Model   \n",
              "57080  272  The trainable biLM layer weights are regulariz...        Model   \n",
              "\n",
              "                            heading                     topic  paper_idx  \\\n",
              "0                               nan           text_generation          5   \n",
              "1                             title           text_generation          5   \n",
              "2                               nan           text_generation          5   \n",
              "3                          abstract           text_generation          5   \n",
              "4                          abstract           text_generation          5   \n",
              "...                             ...                       ...        ...   \n",
              "57076  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "57077  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "57078  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "57079  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "57080  A.8 Sentiment classification  named-entity-recognition          4   \n",
              "\n",
              "       mask  bi_labels            labels  \n",
              "0         1          0               nan  \n",
              "1         1          1  research-problem  \n",
              "2         1          0               nan  \n",
              "3         1          1  research-problem  \n",
              "4         1          0               nan  \n",
              "...     ...        ...               ...  \n",
              "57076     1          0               nan  \n",
              "57077     1          0               nan  \n",
              "57078     1          0               nan  \n",
              "57079     1          0               nan  \n",
              "57080     1          0               nan  \n",
              "\n",
              "[57081 rows x 9 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.fillna(\"nan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8LijCjLXnC8",
        "outputId": "4a79cb9b-57e5-41df-b61f-b47153de4079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improved Variational Autoencoders for Text Modeling using Dilated ConvolutionsRecent work on generative text modeling has found that variational autoencoders ( VAE ) with LSTM decoders perform worse than simpler LSTM language models ( Bowman et al. , 2015 ) .We propose the use of a dilated CNN as a decoder in VAE , inspired by the recent success of using CNNs for audio , image and language modeling ( van den .In contrast with prior work where extremely large CNNs are used , we exploit the dilated CNN for its flexibility in varying the amount of conditioning context .We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .For CNNs , we explore several different configurations .We set the convolution filter size to be 3 and gradually increase the depth and dilation from [ 1 , 2 , 4 ] , ] to .We use Gumbel - softmax to sample y from q ( y|x ) .We use a vocabulary size of 20 k for both data sets and set the word embedding dimension to be 512 .The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .We use Adam to optimize all models and the learning rate is selected from [ 2e - 3 , 1 e - 3 , 7.5 e - 4 ] and ?Empirically , we find learning rate 1e - 3 and ?1 = 0.5 to perform the best .We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .For the CNN decoder , we use a dropout ratio of 0.1 at each layer .We use batch size of 32 and all model are trained for 40 epochs .Following , we use KL cost annealing strategy .We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .The results for language modeling are shown in .For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .When LCNN is used as the decoder , we obtain an optimal trade off between using contextual information and latent representation .LCNN - VAE achieves a NLL of 333.9 , which improves over LSTM - LM with NLL of 334.9 .We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .On the other hand , LCNN - VAE - Semi has the best NLL result .\n",
            "An Auto - Encoder Matching Model for Learning Utterance - Level Semantic Dependency in Dialogue GenerationAutomatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .However , conversation generation is a much more complex and flexible task as there are less \" word - to - words \" relations between inputs and responses .To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .For dialogue generation , we set the maximum length to 15 words for each generated sentence .Based on the performance on the validation set , we set the hidden size to 512 , embedding size to 64 and vocabulary size to 40 K for baseline models and the proposed model .The parameters are updated by the Adam algorithm ( Kingma and Ba , 2014 ) and initialized by sampling from the uniform distribution ( [? 0.1 , 0.1 ] ) .The initial learning rate is 0.002 and the model is trained in minibatches with a batch size of 256 . ? 1 and ?The proposed AEM model significantly outperforms the Seq2Seq model .It demonstrates the effectiveness of utterance - level dependency on improving the quality of generated text .The improvement from the AEM model to the AEM + Attention model 2 is 0.68 BLEU - 4 point .We find that the AEM model achieves significant improvement on the diversity of generated text .Also , it should be noticed that the attention mechanism performs almost the same compared to the AEM model ( 31.2 K vs. 34.6 K in terms of Dist - 3 ) , which indicates that the utterance - level dependency and the word - level dependency are both indispensable for dialogue generation .Therefore , by combining the two dependencies together , the AEM + Attention model achieves the best results .shows the results of human evaluation .The inter-annotator agreement is satisfactory considering the difficulty of human evaluation .The Pearson 's correlation coefficient is 0.69 on coherence and 0.57 on fluency , with p < 0.0001 .First , it is clear that the AEM model outperforms the Seq2Seq model with a large margin , which proves the effectiveness of the AEM model on generating high quality responses .Second , it is interesting to note that with the attention mechanism , the coherence is decreased slightly in the Seq2Seq model but increased significantly in the AEM model .Therefore , it is expected that the AEM + Attention model achieves the best G-score .\n",
            "Adversarial Ranking for Language GenerationIn this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .Specifically , the proposed new adversarial network consists of two neural network models , a generator and a ranker .As opposed to performing a binary classification task , we propose to train the ranker to rank the machine - written sentences lower than human - written sentences with respect to a reference sentence which is human-written .Accordingly , we train the generator to synthesize sentences which confuse the ranker so that machine - written sentences are ranked higher than human - written sentences in regard to the reference .During learning , we adopt the policy gradient technique to overcome the non-differentiable problem .Consequently , by viewing a set of data samples collectively and evaluating their quality through relative ranking , the discriminator is able to make better assessment of the quality of the samples , which in turn helps the generator to learn better .Our method is suitable for language learning in comparison to conventional GANs .Simulation on synthetic dataIt can be seen that the proposed RankGAN performs more favourably against the compared methods .While MLE , PG - BLEU and SeqGAN tend to converge after 200 training epochs , the proposed RankGAN consistently improves the language generator and achieves relatively lower NLL score .It is worth noting that the proposed RankGAN achieves better performance than that of PG - BLEU .Results on Chinese poems compositionFollowing the evaluation protocol in , we compute the BLEU - 2 score and estimate the similarity between the human - written poem and the machine - created one .It can be seen that the proposed Rank GAN performs more favourably compared to the state - of - the - art methods in terms of BLEU - 2 score .RankGAN outperforms the compared method in terms of the human evaluation score .Results on COCO image captionsRankGAN achieves better performance than the other methods in terms of different BLEU scores .These examples show that our model is able to generate fluent , novel sentences that are not existing in the training set .As can be seen , the human - written sentences get the highest score comparing to the language models .Among the GANs approaches , RankGAN receives better score than SeqGAN , which is consistent to the finding in the Chinese poem composition .Results on Shakespeare 's playsAs can be seen , the proposed method achieves consistently higher BLEU score than the other methods in terms of the different n-grams criteria .The results indicate the proposed RankGAN is able to capture the transition pattern among the words , even if the training sentences are novel , delicate and complicated .\n",
            "Long Text Generation via Adversarial Training with Leaked InformationAutomatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .In this paper , we propose a new algorithmic framework called Leak GAN to address both the non-informativeness and the sparsity issues .LeakGAN is a new way of providing richer information from the discriminator to the generator by borrowing the recent advances in hierarchical reinforcement learning .As illustrated in , we specifically introduce a hierarchical generator G , which consists of a high - level MANAGER module and a low - level WORKER module .The MANAGER is along shortterm memory network ( LSTM ) and serves as a mediator .In each step , it receives generator D 's high - level feature representation , e.g. , the feature map of the CNN , and uses it to form the guiding goal for the WORKER module in that timestep .Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .GAN Setting .For the discriminator , we choose the CNN architecture as the feature extractor and the binary classifier .For the synthetic data experiment , the CNN kernel size ranges from 1 to T .The number of each kernel is between 100 and 200 .In this case , the feature of text is a 1,720 dimensional vector .Dropout with the keep rate 0.75 and L2 regularization are performed to avoid overfitting .For the generator , we adopt LSTM as the architectures of MANAGER and WORKER to capture the sequence context information .The MANAGER produces the 16 - dimensional goal embedding feature vector wt using the feature map extracted by CNN .The goal duration time c is a hyperparameter set as 4 after some preliminary experiments .Synthetic Data Experiments( i ) In the pre-training stage , LeakGAN has already shown observable performance superiority compared to other models , which indicates that the proposed hierarchical architecture itself brings improvement over the previous ones .( ii ) In the adversarial training stage , Leak GAN shows a better speed of convergence , and the local minimum it explores is significantly better than previous results .Long Text Generation : EMNLP2017 WMT NewsIn all measured metrics , LeakGAN shows significant performance gain compared to baseline models .Middle Text Generation : COCO Image CaptionsThe results of the BLEU scores on the COCO dataset indicate that Leak GAN performs significantly better than baseline models in mid-length text generation task .Short Text Generation : Chinese PoemsThe results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks .Turing Test and Generated SamplesThe performance on two datasets indicates that the generated sentences of Leak GAN are of higher global consistency and better readability than those of SeqGAN .\n",
            "Generating Text through Adversarial Training using Skip - Thought VectorsAttempts have been made for utilizing GANs with word embeddings for text generation .Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .This work pro-Code available at : https://github.com/enigmaeth/skip-thought-gan poses an approach for text generation using Generative Adversarial Networks with Skip - Thought vectors .The Skip - Thought encoder for the model encodes sentences with length less than 30 words using 2400 GRU units with word vector dimensionality of 620 to produce 4800 - dimensional combineskip vectors . .The combine - skip vectors , with the first 2400 dimensions being uni-skip model and the last 2400 bi-skip model , are used as they have been found to be the best performing in the experiments\n",
            "As a new way of training generative models , Generative Adversarial Net ( GAN ) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real - valued data .However , it has limitations when the goal is for generating sequences of discrete tokens .Generating sequential synthetic data that mimics the real one is an important problem in unsupervised learning .In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .In our policy gradient , we employ Monte Carlo ( MC ) search to approximate the state - action value .We directly train the policy ( generative model ) via policy gradient , which naturally avoids the differentiation difficulty for discrete data in a conventional GAN .To setup the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution N ( 0 , 1 ) as the oracle describing the real data distribution G oracle ( x t |x 1 , . . . , x t?1 ) .In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the labelFor different tasks , one should design specific structure for the convolutional layer and in our synthetic data experiments , the kernel size is from 1 to T and the number of each kernel size is between 100 to 200 3 . Dropout ) and L2 regularization are used to avoid over-fitting .The first model is a random token generation .The second one is the MLE trained LSTM G ? .The third one is scheduled sampling .The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) .In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .A curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones .Since the evaluation metric is fundamentally instructive , we can see the impact of SeqGAN , which outperforms other baselines significantly .A significance T - test on the NLL oracle score distribution of the generated sequences from the compared models is also performed , which demonstrates the significant improvement of SeqGAN over all compared models .After about 150 training epochs , both the maximum likelihood estimation and the schedule sampling methods converge to a relatively high NLL oracle score , whereas SeqGAN can improve the limit of the generator with the same structure as the baselines significantly .This indicates the prospect of applying adversarial training strategies to discrete sequence generative models to breakthrough the limitations of MLE .Additionally , SeqGAN outperforms PG - BLEU , which means the discriminative signal in GAN is more general and effective than a predefined score ( e.g. BLEU ) to guide the generative policy to capture the underlying distribution of the sequence data .\n",
            "Bottom - Up Abstractive SummarizationText summarization systems aim to generate natural language summaries that compress the information in a longer text .Current state - of - the - art neural abstractive summarization models combine extractive and abstractive techniques by using pointergenerator style models which can copy words from the source document .Motivated by this approach , we consider bottom - up attention for neural abstractive summarization .Our approach first selects a selection mask for the source document and then constrains a standard neural model by this mask .Our full model incorporates a separate content selection system to decide on relevant aspects of the source document .We frame this selection task as a sequence - tagging problem , with the objective of identifying tokens from a document that are part of its summary .To incorporate bottom - up attention into abstractive summarization models , we employ masking to constrain copying words to the selected parts of the text , which produces grammatical outputs .All inference parameters are tuned on a 200 example subset of the validation set .Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .The coverage penalty parameter ? is set to 10 , and the copy attention normalization parameter ? to 2 for both approaches .We use AllenNLP for the content selector , and Open NMT - py for the abstractive models .3 . shows our main results on the CNN - DM corpus , with abstractive models shown in the top , and bottom - up attention methods at the bottom .We first observe that using a coverage inference penalty scores the same as a full coverage mechanism , without requiring any additional model parameters or model fine - tuning .The results with the CopyTransformer and coverage penalty indicate a slight improvement across all three scores , but we observe no significant difference between Pointer - Generator and CopyTransformer with bottom - up attention .\n",
            "Retrieve , Rerank and Rewrite : Soft Template Based Neural SummarizationIn this paper , we focus on an increasingly intriguing task , i.e. , abstractive sentence summarization , which generates a shorter version of a given sentence while attempting to preserve its original meaning .Due to the strong rewriting ability of the seq2seq framework , in this paper , we propose to combine the seq2seq and template based summarization approaches .We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .We utilize a widely - used Information Retrieval ( IR ) platform to find out candidate soft templates from the training corpus .Then , we extend the seq2seq model to jointly learn template saliency measurement ( Rerank ) and final summary generation ( Rewrite ) .Specifically , a Recurrent Neural Network ( RNN ) encoder is applied to convert the input sentence and each candidate template into hidden states .In Rerank , we measure the informativeness of a candidate template according to its hidden state relevance to the input sentence .The candidate template with the highest predicted informativeness is regarded as the actual soft template .In Rewrite , the summary is generated according to the hidden states of both the sentence and template .Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/We use the popular seq2seq framework Open - NMT 5 as the starting point .To make our model more general , we retain the default settings of Open NMT to build the network architecture .Specifically , the dimensions of word embeddings and RNN are both 500 , and the encoder and decoder structures are two - layer bidirectional Long Short Term Memory Networks ( LSTMs ) .On our computer ( GPU : GTX 1080 , Memory : 16G , CPU : i7-7700 K ) , the training spends about 2 days .During test , we use beam search of size 5 to generate summaries .We add the argument \" replace unk \" to replace the generated unknown words with the source word that holds the highest attention weight .Since the generated summaries are often shorter than the actual ones , we introduce an additional length penalty argument \" alpha 1 \" to encourage longer generation , like .OpenNMTWe also implement the standard attentional seq2seq model with OpenNMT .FTSum encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries .In addition , to evaluate the effectiveness of our joint learning framework , we develop a baseline named \" PIPELINE \" .However , it trains the Rerank module and Rewrite module in pipeline .We also examine the performance of directly regarding soft templates as output summaries .We introduce five types of different soft templates :As shown in , the performance of Random is terrible , indicating it is impossible to use one summary template to fit various actual summaries .Rerank largely outperforms First , which verifies the effectiveness of the Rerank module .Likewise , comparing Max and First , we observe that the improving capacity of the Retrieve module is high .Notice that Optimal greatly exceeds all the state - of - the - art approaches .We also measure the linguistic quality of generated summaries from various aspects , and the results are present in .As can be seen from the rows \" LEN DIF \" and \" LESS 3 \" , the performance of Re 3 Sum is almost the same as that of soft templates .In this section , we investigate how soft templates affect our model .As illustrated in , the more high - quality templates are provided , the higher ROUGE scores are achieved .Next , we manually inspect the summaries generated by different methods .We find the outputs of Re 3 Sum are usually longer and more flu - ent than the outputs of OpenNMT .As can be seen , with different templates given , our model is likely to generate dissimilar summaries .\n",
            "Entity Commonsense Representation for Neural Abstractive SummarizationText summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text .To this end , we present a method to effectively apply linked entities in sequence - tosequence models , called Entity2Topic ( E2T ) .E2T is a module that can be easily attached to any sequence - to - sequence based summarization model .The module encodes the entities extracted from the original text by an entity linking system ( ELS ) , constructs a vector representing the topic of the summary to be generated , and informs the decoder about the constructed topic vector .We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention .For both datasets , we further reduce the size of the input , output , and entity vocabularies to at most 50 K as suggested in and replace less frequent words to \" < unk > \" .We use 300D Glove 6 and 1000D wiki2vec 7 pre-trained vectors to initialize our word and entity vectors .For GRUs , we set the state size to 500 .For CNN , we set h = 3 , 4 , 5 with 400 , 300 , 300 feature maps , respectively .For firm attention , k is tuned by calculating the perplexity of the model starting with smaller values ( i.e. k = 1 , 2 , 5 , 10 , 20 , ... ) and stopping when the perplexity of the model becomes worse than the previous model .We use dropout on all non-linear connections with a dropout rate of 0.5 .We set the batch sizes of Gigaword and CNN datasets to 80 and 10 , respectively .Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule , with l 2 constraint ( Hinton et al. , 2012 ) of 3 .We perform early stopping using a subset of the given development dataset .We use beam search of size 10 to generate the summary .For the Gigaword dataset , we compare our models with the following abstractive baselines :ABS + is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder , Feat2s ( Nallapati et al. , 2016 ) is an RNN sequence - to - sequence model with lexical and statistical features in the encoder , Luong - NMT is a two - layer LSTM encoder - decoder model , RAS - Elman uses an attentive CNN encoder and an Elman RNN decoder , and SEASS uses BiGRU encoders and GRU decoders with selective encoding .For the CNN dataset , we compare our models with the following extractive and abstractive baselines :Lead - 3 is a strong baseline that extracts the first three sentences of the document as summary , LexRank extracts texts using LexRank , Bi - GRU is a non-hierarchical one - layer sequence - to - sequence abstractive baseline , Distraction - M3 uses a sequence - to - sequence abstractive model with distraction - based networks , and GBA is a graph - based attentional neural abstractive model .In Gigaword dataset where the texts are short , our best model achieves a comparable performance with the current state - of - the - art .In CNN dataset where the texts are longer , our best model outperforms all the previous models .Overall , E2T achieves a significant improvement over the baseline model BASE , with at least 2 ROUGE - 1 points increase in the Gigaword dataset and 6 ROUGE - 1 points increase in the CNN dataset .Among the model variants , the CNN - based encoder with selective disambiguation and firm attention performs the best .\n",
            "Coarse-to-Fine Attention Models for Document SummarizationTherefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .For document summarization , this means dividing the document into chunks of text , sparsely attending to one or a few chunks at a time using hard attention , then applying the usual full attention over those chunks - we call this method coarse - to - fine attention .We train with minibatch stochastic gradient descent ( SGD ) with batch size 20 for 20 epochs , renormalizing gradients below norm 5 .We initialize the learning rate to 0.1 for the top - level encoder and 1 for the rest of the model , and begin decaying it by a factor of 0.5 each epoch after the validation perplexity stops decreasing .We use 2 layer LSTMs with 500 hidden units , and we initialize word embeddings with 300 dimensional word2vec embeddings .We initialize all other parameters as uniform in the interval [ ? 0.1 , 0.1 ] .For convolutional layers , we use a kernel width of 6 and 600 filters .Positional embeddings have dimension 25 .We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize ( with dropout probability 0.3 ) .At test time , we run beam search to produce the summary with a beam size of 5 .Our models are implemented using Torch based on a past version of the Open NMT system4 . We ran our experiments on a 12GB Geforce GTX Titan X GPU .The ILP model ROUGE scores are surprisingly low .C2 F results are significantly worse than soft attention results .Sharpness of AttentionWe compute the entropy numbers by averaging over all generated words in the validation set .We note that the entropy of C2F is very low ( before taking the argmax at test time ) .This is exactly what we had hoped for - we will see that the model in fact learns to focus on only a few top - level chunks of the document over the course of generation .Attention HeatmapsIn HIER , we observe that the attention becomes washed out ( in accord with its high entropy ) and is essentially averaging all of the encoder hidden states .In C2 F , we see that we get very sharp attention on some rows as we had hoped .\n",
            "Concept Pointer Network for Abstractive SummarizationAbstractive summarization ( ABS ) has gained overwhelming success owing to a tremendous development of sequence - to - sequence ( seq2seq ) model and its variants .Hence , in this paper , we propose a novel model based on a concept pointer generator that encourages the generation of conceptual and abstract words .As a hidden benefit , the model also alleviates the OOV problems .Our model uses pointer network to capture the salient information from a source text , and then employs another pointer to generalize the detailed words according to their upper level of expressions .The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .We initialize word embeddings with 128 - d vectors and fine - tune them during training .The vocabulary size was set to 150 k for both the source and target text .The hidden state size was set to 256 .Our code is available on https :// github.com/wprojectsn/codes , and the vocabularies and candidate concepts are also included .We trained our models on a single GTX TI - TAN GPU machine .We used the Adagrad optimizer with a batch size of 64 to minimize the loss .The initial learning rate and the accumulator value were set to 0.15 and 0.1 , respectively .We used gradient clipping with a maximum gradient norm of 2 .We trained our concept pointer generator for 450 k iterations yielded the best performance , then took the optimization using RL rewards for RG - L at 95 K iterations on DUC - 2004 and at 50 K iterations on Gigaword .We took the distancesupervised training at 5 K iterations on DUC - 2004 and at 6.5 K iterations on Gigaword .ABS + is a tuned ABS model with additional features .RAS - Elman ) is a convolution encoder and an Elman RNN decoder with attention .Seq2seq + att is two - layer BiLSTM encoder and one - layer LSTM decoder equipped with attention .lvt5 k - lsent uses temporal attention to keep track of the past attentive weights of the decoder and restrains the repetition in later sequences .SEASS includes an additional selective gate to control information flow from the encoder to the decoder .Pointer - generator is an integrated pointer network and seq2seq model .CGU ) sets a convolutional gated unit and self - attention for global encoding .We observe that our model outperformed all the strong state of - the - art models on both datasets in all metrics except for RG - 2 on Gigaword .In terms of the pointer generator performance , the improvements made by our concept pointer are statistically significant ( p < 0.01 ) across all metrics .\n",
            "Deep Recurrent Generative Decoder for Abstractive Text SummarizationAutomatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .We employ Variational Auto - Encoders ( VAEs ) as the base model for our generative framework which can handle the inference problem associated with complex generative modeling .Inspired by , we add historical dependencies on the latent variables of VAEs and propose a deep recurrent generative decoder ( DRGD ) for latent structure modeling .Then the standard discriminative deterministic decoder and the recurrent generative decoder are integrated into a unified decoding framework .The target summaries will be decoded based on both the discriminative deterministic variables and the generative latent structural information .Automatic summarization is the process of automatically generating a summary that retains the most important content of the original text document .TOPIARY is the best on DUC2004 Task - 1 for compressive text summarization .It combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization .MOSES + uses a phrasebased statistical machine translation system trained on Gigaword to produce summaries .ABS and ABS + are both the neural network based models with local attention modeling for abstractive sentence summarization .ABS + is trained on the Gigaword corpus , but combined with an additional log - linear extractive summarization model with handcrafted features .RNN and RNN - context are two seq2seq architectures .Copy Net integrates a copying mechanism into the sequence - to sequence framework .RNN - distract uses a new attention mechanism by distracting the historical attention in the decoding steps .RAS - LSTM and RAS - Elman both consider words and word positions as input and use convolutional encoders to handle the source information .LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input .ASC+ FSC 1 ) uses a generative model with attention mechanism to conduct the sentence compression problem .lvt2k - 1sent and lvt5k - 1sent utilize a trick to control the vocabulary size to improve the training efficiency .For the experiments on the English dataset Gigawords , we set the dimension of word embeddings to 300 , and the dimension of hidden states and latent variables to 500 .The maximum length of documents and summaries is 100 and 50 respectively .The batch size of mini-batch training is 256 .For DUC - 2004 , the maximum length of summaries is 75 bytes .For the dataset of LCSTS , the dimension of word embeddings is 350 .We also set the dimension of hidden states and latent variables to 500 .The maximum length of documents and summaries is 120 and 25 respectively , and the batch size is also 256 .The beam size of the decoder was set to be 10 .Adadelta with hyperparameter ? = 0.95 and = 1 e ? 6 is used for gradient based optimization .Our neural network based framework is implemented using Theano ( Theano Development Team , 2016 ) .ROUGE EvaluationThe results on the Chinese dataset LCSTS are shown in .Our model DRGD also achieves the best performance .\n",
            "Ensure the Correctness of the Summary : Incorporate Entailment Knowledge into Abstractive Sentence SummarizationIn this paper , we investigate the sentence summarization task that produces a summary from a source sentence .To incorporate entailment knowledge into abstractive summarization models , we propose in this work an entailment - aware encoder and an entailment - aware decoder .We share the encoder of the summarization generation system with the entailment recognition system , so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships .Furthermore , we propose an entailment Reward Augmented Maximum Likelihood ( RAML ) training that encourages the decoder of the summarization system to produce summary entailed by the source .ABS . first apply the seq2seq model to abstractive sentence summarization .ABS +. propose a neural machine translation model with two - layer LSTMs for the encoder - decoder .Seq2seq .This is a standard seq2seq model with attention mechanism .Seq2seq + MTL .This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .Seq2seq + MTL ( Share decoder ) .propose a multi - task learning ( MTL ) framework in which the decoder is shared for summarization generation and entailment generation task .Seq2seq + ERAML .This is our proposed model with entailment - aware decoder , which conducts an Entailment Reward Augmented Maximum Likelihood ( ERAML ) training framework .Seq2seq + ROUGE -2 RAML .We apply ROUGE - 2 RAML training for seq2seq model .Seq2seq + RL .We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .Seq2seq + selective .employ a selective encoding model to control the information flow from encoder to decoder .Experimental Results : Gigaword CorpusOur model performs better than the previous works .Experimental Results : DUC 2004In , experimental results also show our Seq2seq + selective + MTL + ERAML model achieves significant improvements over baseline models , surpassing Feats2s by 0.98 % ROUGE - 1 , 0.78 % ROUGE - 2 and 0.65 % ROUGE - L without fine - tuning on DUC data .Does our summarization model learn entailment knowledge ?For the test set of , the average entailment score for the reference is 0.72 , while for the basic seq2seq model , the entailment score is only 0.46 .When we adopt entailmentbased strategies , the entailment score rises to 0.63 for seq2seq model .Note that the entailment score is 0.57 for seq2seq model with selective encoding , and we believe that the selective mechanism can filter out secondary information in the input , which will reduce the possibility to generate irrelevant information .Entailment - aware selective model achieves a high entailment reward of 0.71 .In part at least , we can conclude that our model has successfully learned entailment knowledge .Is it less abstractive for our model ?shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .However , when we exclude all the words not in the reference ( these words may lead to wrong information ) , our model generates more novel words , suggesting that our model provides a compromise solution for informativeness and correctness .6.6.3 Could the entailment recognition also be improved ?shows that our summarization model with MTL outperforms basic seq2seq model .As ? increases , the accuracy of entailment recognition improves and finally exceeds that of the model without MTL , which reveals the advantage of MTL framework .\n",
            "Structure - Infused Copy Mechanisms for Abstractive SummarizationSeq2seq learning has produced promising results on summarization .In this paper we seek to address this problem by incorporating source syntactic structure in neural sentence summarization to help the system identify summary - worthy content and compose summaries that preserve the important meaning of the source texts .We present structure - infused copy mechanisms to facilitate copying source words and relations to the summary based on their semantic and structural importance in the source sentences .We first report results on the Gigaword valid - 2000 dataset in .We present R - 1 , R - 2 , and R - L scores ) that respectively measures the overlapped unigrams , bigrams , and longest common subsequences between the system and reference summaries 3 .Overall , we observe that models equipped with the structure - infused copy mechanisms are superior to the baseline , suggesting that combining source syntactic structure with the copy mechanism is effective .We found that the \" Struct + Hidden \" architecture , which directly concatenates structural embeddings with the encoder hidden states , outperforms \" Struct + Input \" despite that the latter requires more parameters .\" Struct + 2 Way + Word \" also demonstrates strong performance , achieving 43.21 % , 21. 84 % , and 40.86 % F 1 scores , for R - 1 , R - 2 , and R - L respectively .\n",
            "Abstractive Sentence Summarization with Attentive Recurrent Neural NetworksGenerating a condensed version of a passage while preserving its meaning is known as text summarization .Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .In addition , at every time step the decoder also takes a conditioning input which is the output of an encoder module .Depending on the current state of the RNN , the encoder computes scores over the words in the input sentence .Both the decoder and encoder are jointly trained on a data set consisting of sentence - summary pairs .Lastly , our encoder uses a convolutional network to encode input words .We implemented our models in the Torch library ( http://torch.ch/)2 . To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .Our final Elman architecture ( RAS - Elman ) uses a single layer with H = 512 , ? = 0.5 , ? = 2 , and ? = 10 . The LSTM model ( RAS - LSTM ) also has a single layer with H = 512 , ? = 0.1 , ? = 2 , and ? = 10 .shows that both our RAS - Elman and RAS - LSTM models achieve lower perplexity than ABS as well as other models reported in .The RAS - LSTM performs slightly worse than RAS - Elman , most likely due to over-fitting .The ROUGE results show that our models comfortably outperform both ABS and ABS + by a wide margin on all metrics .On DUC - 2004 we report recall ROUGE as is customary on this dataset .The results ( Table 3 ) show that our models are better than ABS + .\n",
            "Cutting - off Redundant Repeating Generations for Neural Abstractive SummarizationThe RNN - based encoder - decoder ( EncDec ) approach has recently been providing significant progress in various natural language generation ( NLG ) tasks , i.e. , machine translation ( MT ) and abstractive summarization ( ABS ) .The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .The WFE sub-model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .\n",
            "Global Encoding for Abstractive SummarizationTherefore , sequence - to - sequence learning can be applied to neural abstractive summarization , whose model consists of an encoder and a decoder .To tackle this problem , we propose a model of global encoding for abstractive summarization .We set a convolutional gated unit to perform global encoding on the source context .The gate based on convolutional neural network ( CNN ) filters each encoder output based on the global context due to the parameter sharing , so that the representations at each time step are refined with consideration of the global context .We implement our experiments in PyTorch on an NVIDIA 1080 Ti GPU .The word embedding dimension and the number of hidden units are both 512 .In both experiments , the batch size is set to 64 .We use Adam optimizer ( Kingma and Ba , 2014 ) with the default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 .The learning rate is halved every epoch .Gradient clipping is applied with range [ - 10 , 10 ] .Baselines for LCSTS are introduced in the following .RNN and RNN - context are the RNNbased seq2seq models , without and with attention mechanism respectively .Copy - Net is the attention - based seq2seq model with the copy mechanism .SRB is a model that improves semantic relevance between source text and summary .DRGD is the conventional seq2seq with a deep recurrent generative decoder .As to the baselines for Gigaword , ABS and ABS + are the models with local attention and handcrafted features .Feats is a fully RNN seq2seq model with some specific methods to control the vocabulary size .RAS - LSTM and RAS - Elman are seq2seq models with a convolutional encoder and an LSTM decoder and an Elman RNN decoder respectively .SEASS is a seq2seq model with a selective gate mechanism .DRGD is also a baseline for Gigaword .In the experiments on the two datasets , our model achieves advantages of ROUGE score over the baselines , and the advantages of ROUGE score on the LCSTS are significant .Compared with the conventional seq2seq model , our model owns an advantage of ROUGE - 2 score 3.7 and 1.5 on the LCSTS and Gigaword respectively .\n",
            "Mixture Content Selection for Diverse Sequence GenerationGenerating diverse sequences is important in many NLP applications such as question generation or summarization that exhibit semantically one - to - many relationships between source and the target sequences .Generating target sequences given a source sequence has applications in a wide range of problems in NLP with different types of relationships between the source and target sequences .Encoder - decoder models are widely used for sequence generation , most notably in machine translation where neural models are now often almost as good as human translators in some language pairs .In this paper , we present a method for diverse generation that separates diversification and generation stages .The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .We present a generic module called SELECTOR that is specialized for diversification .This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .Beam SearchThis baseline keeps K hypotheses with highest log-probability scores at each decoding step .Truncated SamplingThis baseline randomly samples words from top - 10 candidates of the distribution at the decoding step .Mixture DecoderThis baseline constructs a hard - MoE of K decoders with uniform mixing coefficient ( referred as hMup in ) and conducts parallel greedy decoding .Mixture Selector ( Ours )We construct a hard - MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence .For all experiments , we tie the weights of the encoder embedding , the decoder embedding , and the decoder output layers .We train up to 20 epochs and select the checkpoint with the best oracle metric .We use Adam ( Kingma and Ba , 2015 ) optimizer with learning rate 0.001 and momentum parmeters ? 1 = 0.9 and ? 2 = 0.999 . Minibatch size is 64 and 32 for question generation and abstractive summarization .All models are implemented in PyTorch and trained on single Tesla P40 GPU , based on NAVER Smart Machine Learning ( NSML ) platform .Diversity vs. Accuracy Trade - off compare our method with different diversitypromoting techniques in question generation and abstractive summarization .The tables show that our mixture SELECTOR method outperforms all baselines in Top - 1 and oracle metrics and achieves the best trade - off between diversity and accuracy .Notably , our method scores state - of - the - art BLEU - 4 in question generation on SQuAD and ROUGE comparable to state - of - the - art methods in abstractive summarization in CNN - DM ( See also for state - of - the - art results in CNN - DM ) .Diversity vs. Number of MixturesHere we compare the effect of number of mixtures in our SELECTOR and Mixture Decoder .show that pairwise similarity increases ( diversity ?) when the number of mixtures increases for Mixture Decoder .\n",
            "Abstractive Text Summarization by Incorporating Reader CommentsIn neural abstractive summarization field , conventional sequence - to - sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect .To tackle this problem , we propose the task of reader - aware abstractive summary generation , which utilizes the reader comments to help the model produce better summary about the main aspect .Unlike traditional abstractive summarization task , reader - aware summarization confronts two main challenges :In this paper , we propose a summarization framework named reader - aware summary generator ( RASG ) that incorporates reader comments to improve the summarization performance .Specifically , a seq2seq architecture with attention mechanism is employed as the basic summary generator .We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the \" reader focused aspect \" .Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , \" decoder focused aspect \" .After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .The training of our framework RASG is conducted in an adversarial way .( 1 ) S2S : Sequence - to - sequence framework has been proposed for language generation task .( 2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .( 3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .( 4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .( 5 ) TextRank : propose to build a graph , then add each sentence as a vertex and use link to represent semantic similarity .We implement our experiments in TensorFlow ) on an NVIDIA P40 GPU .The word embedding dimension is set to 256 and the number of hidden units is 512 .We use Adagrad optimizer as our optimizing algorithm .We employ beam search with beam size 5 to generate more fluency summary sentence .We see that RASG achieves a 11.0 % , 9.1 % and 6.6 % increment over the state - of - the - art method CGU in terms of ROUGE - 1 , ROUGE - 2 and ROUGE - L respectively .It is worth noticing that the baseline model S2SR achieves better performance than S2S which demonstrates the effectiveness of incorporating reader focused aspect in summary generation .The discriminator provides the scalar training signal L g c for generator training and the feature vector F ( m t ) for goal tracker .Consequently , there is an increment of 17.51 % from RASG w / o GTD to RASG w / o GT in terms of ROUGE - L , which demonstrates the effectiveness of discriminator .As for the effectiveness of goal tracker , compared with RASG and RASG w / o GT , RASG w/ o GTD offers a decrease of 45. 23 % and 17.88 % in terms of ROUGE - 1 , respectively .Finally , RASG w/o DM offers a decrease of 10 . 22 % compared with RASG in terms of ROUGE - L , which demonstrates the effectiveness of denoising module .\n",
            "Soft Layer - Specific Multi - Task Summarization with Entailment and Question GenerationWe improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .In this work , we improve abstractive text summarization via soft , high - level ( semantic ) layerspecific multi-task learning with two relevant auxiliary tasks .Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .Pointer + Coverage Baseline4 On Gigaword dataset , our baseline model ( with pointer only , since coverage not needed for this single - sentence summarization task ) performs better than all previous works , as shown in .Multi - Task with Entailment Generation4 . shows that this multi-task setting is better than our strong baseline models and the improvements are statistically significant on all metrics 5 on both CNN / DailyMail ( p < 0.01 in ROUGE - 1 / ROUGE - L / METEOR and p < 0.05 in ROUGE - 2 ) and Gigaword ( p < 0.01 on all metrics ) datasets , showing that entailment generation task is inducing useful inference skills to the summarization task ( also see analysis examples in Sec. 7 ) .For multi-task learning with question generation , the improvements are statistically significant in ROUGE - 1 ( p < 0.01 ) , ROUGE - L ( p < 0.05 ) , and METEOR ( p < 0.01 ) for CNN / DailyMail and in all metrics ( p < 0.01 ) for Gigaword , compared to the respective baseline models .Soft - sharing vs. Hard - sharingAs described in Sec. 4.2 , we choose soft - sharing over hard - sharing because of the more expressive parameter sharing it provides to the model .Empirical results in 8 prove that soft - sharing method is statistically significantly better than hard - sharing with p < 0.001 in all metrics .Quantitative Improvements in EntailmentWe found that our 2 - way MTL model with entailment generation reduces this extraneous count by 17.2 % w.r.t. the baseline .Quantitative Improvements in Saliency DetectionThe results are shown in Table 10 , where the 2 - way - QG MTL model ( with question generation ) versus baseline improvement is stat. significant ( p < 0.01 ) .Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .Hence , our 3 - way multi-task model generates summaries that are both better at logical entailment and contain more salient information .\n",
            "Selective Encoding for Abstractive Sentence SummarizationThe second level representation is tailored for sentence summarization task , which leads to better performance .In this paper we propose Selective Encoding for Abstractive Sentence Summarization ( SEASS ) .We treat the sentence summarization as a threephase task : encoding , selection , and decoding .It consists of a sentence encoder , a selective gate network , and a summary decoder .First , the sentence encoder reads the input words through an RNN unit to construct the first level sentence representation .Then the selective gate network selects the encoded information to construct the second level sentence representation .The selective mechanism controls the information flow from encoder to decoder by applying a gate network according to the sentence information , which helps improve encoding effectiveness and release the burden of the decoder .Finally , the attention - equipped decoder generates the summary using the second level sentence representation .We initialize model parameters randomly using a Gaussian distribution with Xavier scheme .We use Adam as our optimizing algorithm .For the hyperparameters of Adam optimizer , we set the learning rate ? = 0.001 , two momentum parameters ? 1 = 0.9 and ? 2 = 0.999 respectively , and = 10 ?8 .During training , we test the model performance ( ROUGE - 2 F1 ) on development set for every 2,000 batches .We also apply gradient clipping with range [ ? 5 , 5 ] during training .To both speedup the training and converge quickly , we use mini-batch size 64 by grid search .ABS + Based on ABS model , further with two - layer LSTMs for the encoder - decoder with 500 hidden units in each layer implemented in .s 2 s+ attWe also implement a sequence - to sequence model with attention as our baseline and denote it as \" s2 s + att \" .Our SEASS model with beam search outperforms all baseline models by a large margin .Even for greedy search , our model still performs better than other methods which used beam search .For the popular ROUGE - 2 metric , our SEASS model achieves 17.54 F1 score and performs better than the previous works .Compared to the ABS model , our model has a 6.22 ROUGE - 2 F1 relative gain .Compared to the highest CAs 2s baseline , our model achieves 1.57 ROUGE - 2 F1 improvement and passes the significant test according to the official ROUGE script .DUC 2004As summarized in , our SEASS outperforms all the baseline methods and achieves 29.21 , 9.56 and 25.51 for ROUGE 1 , 2 and L recall .Compared to the ABS + model which is tuned using DUC 2003 data , our model performs significantly better by 1.07 ROUGE - 2 recall score and is trained only with English Gigaword sentence - summary data without being tuned using DUC data .\n",
            "Learning document embeddings along with their uncertaintiesWe also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .L EARNING word and document embeddings have proven to be useful in wide range of information retrieval , speech and natural language processing applications -.In this paper , we present Bayesian subspace multinomial model ( Bayesian SMM ) as a generative model for bag - ofwords representation of documents .We show that our model can learn to represent each document in the form of a Gaussian distribution , thereby encoding the uncertainty in its covariance .Further , we propose a generative Gaussian classifier that exploits this uncertainty for topic identification ( ID ) .The proposed VB framework can be extended in a straightforward way for subspace n-gram model , that can model n-gram distribution of words in sentences .The embedding dimension was chosen from K = { 100 , . . . , 800 } , and regularization weight from ? = { 0.0001 , . . . , 10.0 }.1 ) NVDM : Since NVDM and our proposed Bayesian SMM share similarities , we chose to extract the embeddings from NVDM and use them for training linear classifiers .2 ) SMM : Our second baseline system is non-Bayesian SMM with 1 regularization over the rows in T matrix , i.e. , 1 SMM .3 ) ULMFiT : The third baseline system is the universal language model fine - tuned for classification ( ULMFiT ) .4 ) TF - IDF :The fourth baseline system is a standard term frequency - inverse document frequency ( TF - IDF ) based document representation , followed by multi-class logistic regression ( LR ) .presents the classification results on Fisher speech corpora with manual and automatic transcriptions , where the first two rows are the results from earlier published works .We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .presents classification results on 20 Newsgroups dataset .We see that the topic ID systems based on Bayesian SMM and logistic regression is better than all the other models , except for the purely discriminative CNN model .We can also see that all the topic ID systems based on Bayesian SMM are consistently better than variational auto encoder inspired NVDM , and ( non-Bayesian ) SMM .\n",
            "Deep Learning For Smile RecognitionInspired by recent successes of deep learning in computer vision , we propose a novel application of deep convolutional neural networks to facial expression recognition , in particular smile recognition .The input images are fed into a convolution comprising a convolutional and a subsampling layer .That convolution maybe followed by more convolutions to become gradually more invariant to distortions in the input .In the second stage , a regular neural network follows the convolutions in order to discriminate the features learned by the convolutions .The output layer consists of two units for smile or no smile .The novelty of this approach is that the exact number of convolutions , number of hidden layers and size of hidden layers are not fixed but subject to extensive model selection in Sec. 4.3 .Due to training time constraints , some parameters have been fixed to reasonable and empirical values , such as the size of convolutions ( 5 5 pixels , 32 feature maps ) and the size of subsamplings ( 2 2 pixels using max pooling ) .All layers use ReLU units , except of softmax being used in the output layer .The learning rate is fixed to ? = 0.01 and not subject to model selection as it would significantly prolong the model selection .The same considerations apply to the momentum , which is fixed to = 0.9 .The entire database has been randomly split into a 60% / 20 % / 20 % training / validation / test ratio .The model is implemented using Lasagne 4 and the generated CUDA code is executed on a Tesla K40c 9 as training on a GPU allows to perform a comprehensive model selection in a feasible amount of time .Stochastic gradient descent with a batch size of 500 is used .contains the four parameters to be optimized : the number of convolutions , the number of hidden layers , the number of units per hidden layer and the dropout factor .Each model was trained for 50 epochs in the model selection .\n",
            "Graphical Abstract A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction Highlights A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term Extraction A Multi-task Learning Model for Chinese - oriented Aspect Polarity Classification and Aspect Term ExtractionAspect - based sentiment analysis ( ABSA ) task is a multi - grained task of natural language processing and consists of two subtasks : aspect term extraction ( ATE ) and aspect polarity classification ( APC ) .Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .By integrating the domain - adapted BERT model , the LCF - ATEPC model achieved the state - of the - art performance of aspect term extraction and aspect polarity classification in four Chinese review datasets .Aspect - based sentiment analysis ; Pontiki , Galanis , Papageorgiou , Androutsopoulos , Manandhar , AL - Smadi , Al - Ayyoub , Zhao , Qin , De Clercq , Hoste , Apidianaki , Tannier , Loukachevitch , Kotelnikov , Bel , Jimnez - Zafra and Eryigit ( 2016 ) ( ABSA ) is a fine - grained task compared with traditional sentiment analysis , which requires the model to be able to automatic extract the aspects and predict the polarities of all the aspects .The APC task is a kind of classification problem .The researches concerning APC tasks is more abundant than the ATE task , and a large number of deep learning - based models have been proposed to solve APC problems , such as the models ; ; ; based on long short - term memory ( LSTM ) and the methodologies based on transformer .Aiming to automatically extract aspects from the text efficiently and analyze the sentiment polarity of aspects simultaneously , this paper proposes a multi-task learning model for aspect - based sentiment analysis .The LCF - ATEPC 3 model proposed in this paper is a novel multilingual and multi-task - oriented model .The proposed model is based on multi-head self - attention ( MHSA ) and integrates the pre-trained and the local context focus mechanism , namely LCF - ATEPC .By training on a small amount of annotated data of aspect and their polarity , the model can be adapted to a large - scale dataset , automatically extracting the aspects and predicting the sentiment polarities .The codes for this paper are available at https://github.com/yangheng95/LCF-ATEPCATAE - LSTM is a classical LSTM - based network for the APC task , which applies the attention mechanism to focus on the important words in the context .ATSM -S Peng et al.is a baseline model of the ATSM variations for Chinese language - oriented ABSA task .GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs .AEN - is an attentional encoder network based on the pretrained BERT model , which aims to solve the aspect polarity classification .BERT - is a BERT - adapted model for Review Reading Comprehension ( RRC ) task , a task inspired by machine reading comprehension ( MRC ) , it could be adapted to aspect - level sentiment classification task .BERT - BASE is the basic pretrained BERT model .We adapt it to ABSA multi-task learning , which equips the same ability to automatically extract aspect terms and classify aspects polarity as LCF - ATEPC model .BERT - ADAis a domain - adapted BERT - based model proposed for the APC task , which finetuned the BERT - BASE model on task - related corpus .LCF - ATEPC 5 is the multi -task learning model for the ATE and APC tasks , which is based on the the BERT - SPC model and local context focus mechanism .LCF - ATE are the variations of the LCF - ATEPC model which only optimize for the ATE task .LCF - APC are the variations of LCF - ATEPC and it only optimize for the APC task during training process .The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .After optimizing the model parameters according to the empirical result , the joint model based on BERT - BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets , such as BERT - PT , AEN - BERT , SDGCN - BERT , and soon .Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .The experimental results show that its performance is also better than the existing BERT - based models .\n",
            "Fine - grained Sentiment Classification using BERTSentiment classification is an important process in understanding people 's perception towards a product , service , or topic .In this paper , we use the pretrained BERT model and finetune it for the fine - grained sentiment classification task on the Stanford Sentiment Treebank ( SST ) dataset .1 ) Word embeddings :In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .2 ) Recursive networks :Various types of recursive neural networks ( RNN ) have been applied on SST .3 ) Recurrent networks :Sophisticated recurrent networks such as left - to - right and bidrectional LSTM networks have also been applied on SST .4 ) Convolutional networks :In this approach , the input sequences were passed through a 1 - dimensional convolutional neural network as feature extractors .We can see that our model , despite being a simple architecture , performs better in terms of accuracy than many popular and sophisticated NLP models .\n",
            "MULTI - MODAL EMOTION RECOGNITION ON IEMOCAP WITH NEURAL NETWORKSEmotion recognition has become an important field of research in human computer interactions and there is a growing need for automatic emotion recognition systems .We explore various deep learning based architectures to first get the best individual detection accuracy from each of the different modes .We then combine them in an ensemble based architecture to allow for training across the different modalities using the variations of the better individual models .Our ensemble consists of Long Short Term Memory networks , Convolution Neural Networks , fully connected Multi - Layer Perceptrons and we complement them using techniques such as Dropout , adaptive optimizers such as Adam , pretrained word - embedding models and Attention based RNN decoders .This allows us to individually target each modality and only perform feature fusion at the final stage .For the text transcript of each of the utterance we use pretrained Glove embeddings of dimension 300 , along with the maximum sequence length of 500 to obtain a ( 500,300 ) vector for each utterance .For the Mocap data , for each different mode such as face , hand , head rotation we sample all the feature values between the start and finish time values and split them into 200 partitioned arrays .We then average each of the 200 arrays along the columns ( 165 for faces , 18 for hands , and 6 for rotation ) , and finally concatenate all of them to obtain ( 200,189 ) dimension vector for each utterance .Our performance matches the prior state of the art , however the comparison is not fair .\n",
            "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion RecognitionInspired by this study , in this paper , we propose a novel bi-hemispheric discrepancy model ( BiHDM ) to learn the asymmetric differences between two hemispheres for electroencephalograph ( EEG ) emotion recognition .As the first step to make machines capture human emotions , emotion recognition has received substantial attention from human - machine - interaction ( HMI ) and pattern recognition research communities in recent years , , .Thus , in this paper , we propose a novel neural network model BiHDM to learn the bi-hemispheric discrepancy for EEG emotion recognition .BiHDM aims to obtain the deep discrepant features between the left and right hemispheres , which is expected to contain more discriminative information to recognize the EEG emotion signals .Hence , to avoid losing this intrinsic graph structural information of EEG data , we can simplify the graph structure learning process by using the horizontal and vertical traversing RNNs , which will construct a complete relationship graph and generate discriminative deep features for all the EEG electrodes .After obtaining these deep features of each electrodes , we can extract the asymmetric discrepancy information between two hemispheres by performing specific pairwise operations for any paired symmetric electrodes .We use the released handcrafted features , i.e. , the differential entropy ( DE ) in SEED and SEED - IV , and the Short - Time Fourier Transform ( STFT ) in MPED , as the input to feed our model .Thus the sizes d N of the input sample X t are 5 62 , 5 62 and 1 62 for these three datasets , respectively .Moreover , in the experiment , we respectively set the dimension d l of each electrode 's deep representation to 32 ; the parameters d g and K of the global high - level feature to 32 and 6 ; and the dimension do of the output feature to 16 without elaborate traversal .Specifically , we implemented BiHDM using Tensor Flow on one Nvidia 1080 Ti GPU .The learning rate , momentum and weight decay rate are set as 0.003 , 0.9 and 0.95 respectively .The network is trained using SGD with batch size of 200 .In addition , we adopt the subtraction as the pairwise operation of the BiHDM model in the experiment section , and discuss the other two types of operations in section III - D.1 ) The subject - dependent experiment :To validate the superiority of BiHDM , we also conduct the same experiments using twelve methods , including linear support vector machine ( SVM ) , random forest ( RF ) , canonical correlation analysis ( CCA ) , group sparse canonical correlation analysis ( GSCCA ) , deep believe network ( DBN ) , graph regularization sparse linear regression ( GRSLR ) , graph convolutional neural network ( GCNN ) , dynamical graph convolutional neural network ( DGCNN ) , domain adversarial neural networks ( DANN ) , bi-hemisphere domain adversarial neural network ( BiDANN ) , EmotionMeter , and attention - long short - term memory ( A - LSTM ) .From , we can see that the proposed BiHDM model outperforms all the compared methods on all the three public EEG emotional datasets , which verifies the effectiveness of BiHDM .Especially for the result on SEED - IV , the proposed method improves over the state - of - the - art method Emotion - Meter by 4 % .Besides , we can see that the compared method BiDANN , which also considers the bi-hemispheric asymmetry , achieves a comparable performance .shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .2 ) The subject - independent experiment :In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .The results are shown in From , it can be clearly seen that the proposed BiHDM method achieves the best performance in the three public datasets , which verifies the effectiveness of BiHDM in dealing with subject - independent EEG emotion recognition .For the three datasets , the improvements on accuracy are 2.2 % , 3.5 % and 2.4 % , respectively , when compared with the existing state - of - the - art methods .shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .\n",
            "A Multi-sentiment - resource Enhanced Attention Network for Sentiment ClassificationIn this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .Then , we propose a multisentiment - resource attention module to learn more comprehensive and meaningful sentiment - specific sentence representation by using the three types of sentiment resource words as attention sources attending to the context words respectively .In this way , we can attend to different sentimentrelevant information from different representation subspaces implied by different types of sentiment sources and capture the over all semantics of the sentiment , negation and intensity words for sentiment prediction .RNTN : Recursive Tensor Neural Network ) is used to model correlations between different dimensions of child nodes vectors .LSTM / Bi-LSTM : Cho et al. ( 2014 ) employs Long Short - Term Memory and the bidirectional variant to capture sequential information .Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .CNN : Convolutional Neural Networks ) is applied to generate task - specific sentence representation .NCSL : designs a Neural Context - Sensitive Lexicon ( NSCL ) to obtain prior sentiment scores of words in the sentence .LR - Bi-LSTM : imposes linguistic roles into neural networks by applying linguistic regularization on intermediate outputs with KL divergence .Self - attention : proposes a selfattention mechanism to learn structured sentence embedding .ID - LSTM : uses reinforcement learning to learn structured sentence representation for sentiment classification .In our experiments , the dimensions of characterlevel embedding and word embedding ( Glo Ve ) are both set to 300 .Kernel sizes of multi-gram convolution for Char - CNN are set to 2 , 3 , respectively .All the weight matrices are initialized as random orthogonal matrices , and we set all the bias vectors as zero vectors .We optimize the proposed model with RMSprop algorithm , using mini-batch training .The size of mini-batch is 60 .The dropout rate is 0.5 , and the coefficient ?of L 2 normalization is set to 10 ?5 . is set to 10 ? 4 . ?First , our model brings a substantial improvement over the methods that do not leverage sentiment linguistic knowledge ( e.g. , RNTN , LSTM , BiLSTM , CNN and ID - LSTM ) on both datasets .Second , our model also consistently outperforms LR - Bi - LSTM which integrates linguistic roles of sentiment , negation and intensity words into neural networks via the linguistic regularization .For example , our model achieves 2.4 % improvements over the MR dataset and 0.8 % improvements over the SST dataset compared to LR - Bi - LSTM .\n",
            "Context - Dependent Sentiment Analysis in User- Generated VideosMultimodal sentiment analysis is a developing area of research , which involves the identification of sentiments in videos .Sentiment analysis is a ' suitcase ' research problem that requires tackling many NLP sub - tasks , e.g. , aspect extraction , named entity recognition , concept extraction , sarcasm detection , personality recognition , and more .Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .Recently , a number of approaches to multimodal sentiment analysis , producing interesting results , have been proposed .In this paper , we discard such an oversimplifying hypothesis and develop a framework based on long shortterm memory ( LSTM ) that takes a sequence of utterances as input and extracts contextual utterancelevel features .Our model preserves the sequential order of utterances and enables consecutive utterances to share information , thus providing contextual information to the utterance - level sentiment classification process .As expected , trained contextual unimodal features help the hierarchical fusion framework to outperform the non-hierarchical framework .The non-hierarchical model outperforms the baseline uni - SVM , which confirms that it is the contextsensitive learning paradigm that plays the key role in improving performance over the baseline .It is to be noted that both sc - LSTM and bc - LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets .Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .The performance improvement is in the range of 0.3 % to 1.5 % on MOSI and MOUD datasets .On the IEMOCAP dataset , the performance improvement of bc - LSTM and sc - LSTM over h- LSTM is in the range of 1 % to 5 % .Every LSTM network variant has outperformed the baseline uni - SVM on all the datasets by the margin of 2 % to 5 % ( see ) .Experimental results in show that the proposed method outperformes by a significant margin .\n",
            "Exploiting Document Knowledge for Aspect - level Sentiment ClassificationSpecifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .Our source code can be obtained from https://github.com/ruidan/Aspect-level-sentiment.In all experiments , 300 - dimension Glo Ve vectors are used to initialize E and E when pretraining is not conducted for weight initialization .These vectors are also used for initializing E in the pretraining phase .We randomly sample 20 % of the original training data from the aspectlevel dataset as the development set and only use the remaining 80 % for training .For all experiments , the dimension of LSTM hidden vectors is set to 300 , ?is set to 0.1 , and we use dropout with probability 0.5 on sentence / document representations before the output layer .We use RMSProp as the optimizer with the decay rate set to 0.9 and the base learning rate set to 0.001 .The mini - batch size is set to 32 .We observe that PRET is very helpful , and consistently gives a 1 - 3 % increase in accuracy over LSTM + ATT across all datasets .MULT gives similar performance as LSTM + ATT on D1 and D2 , but improvements can be clearly observed for D3 and D4 .The combination ( PRET + MULT ) over all yields better results .( 2 ) The numbers of neutral examples in the test sets of D3 and D4 are very small .( 2 ) Overall , transfers of the LSTM and embedding layer are more useful than the output layer .( 3 ) Transfer of the embedding layer is more helpful on D3 and D4 .Sentiment information is not adequately captured by Glo Ve word embeddings .\n",
            "Left - Center - Right Separated Neural Network for Aspect - based Sentiment Analysis with Rotatory AttentionAspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .With the attempt to better address the two problems , in this paper we propose a left - center - right separated neural network with rotatory attention mechanism ( LCR - Rot ) .Specifically , we design a left - center - right separated LSTMs that contains three LSTMs , i.e. , left - , center - and right - LSTM , respectively modeling the three parts of a review ( left context , target phrase and right context ) .On this basis , we further propose a rotatory attention mechanism to take into account the interaction between targets and contexts to better represent targets and contexts .The target2context attention is used to capture the most indicative sentiment words in left / right contexts .Subsequently , the context2target attention is used to capture the most important word in the target .This leads to a two - side representation of the target : left - aware target and right - aware target .Finally , we concatenate the component representations as the final representation of the sentence and feed it into a softmax layer to predict the sentiment polarity .In our work , the dimension of word embedding vectors and hidden state vectors is 300 .We use GloVe 2 vectors with 300 dimensions to initialize the word embeddings , the same as .All out - ofvocabulary words and weight matrices are randomly initialized by a uniform distribution U ( - 0.1 , 0.1 ) , and all bias are set to zero .Tensor Flow is used for implementing our neural network model .In model training , the learning rate is set to 0.1 , the weight for L 2 - norm regularization is set to 1 e - 5 , and dropout rate is set to 0.5 .We train the model use stochastic gradient descent optimizer with momentum of 0.9 .The paired t- test is used for the significance testing .Majority assigns the sentiment polarity that has the largest probability in the training set ; 2 . Simple SVM is a SVM classifier with simple features such as unigrams and bigrams ; 3 . Feature - enhanced SVM is a SVM classifier with a state - of - the - art feature template which contains n-gram features , parse features and lexicon features ; 4 . LSTM represents a standard LSTM for aspect - based sentiment classification task ; 5 . TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; 74.30 66.50 66.50 TD- LSTM 75.60 68.10 70.80 AE - LSTM 76.60 68.90 - ATAE - LSTM 77.20 68.70 - GRNN- G3 79.55 * 71.47 * 70.09 * MemNet 79.98 * 70.33 * 70.52 * IAN 78.60 72.10 - LCR - Rot ( our approach ) 81.34 75.24 72.69 : The performance ( classification accuracy ) of different methods on three datasets .6 . AE - LSTM is an upgraded version of LSTM .7 . ATAE - LSTM is developed based on AE - LSTM .8 . GRNN - G3 adopts a Gated - RNN to represent sentence and use a three - way structure to leverage contexts .MemNet is a deep memory network which considers the content and position of target .IAN interactively learns attentions in the contexts and targets , and generate the representations for targets and contexts separately .We can find that the Majority method is the worst , which means the majority sentiment polarity occupies 53.50 % , 65.00 % and 50 % of all samples on the Restaurant , Laptop and Twitter testing datasets respectively .The Simple SVM model performs better than Majority .With the help of feature engineering , the Feature - enhanced SVM achieves much better results .Our model achieves significantly better results than feature - enhanced SVM .Among LSTM based neural networks described in this paper , the basic LSTM approach performs the worst .TD - LSTM obtains an improvement of 1 - 2 % over LSTM when target signals are taken into consideration .MemNet achieves better results than other models on the Restaurant dataset , since it considers not only the contexts of targets but also the position of each context word related to the target .IAN considers separate representations of targets and obtains better result on the Laptop dataset .GRNN - G3 achieves competitive results on all datasets because of its three - way structure and special gated - RNN model .In the contrast , our LCR - Rot model achieves the best results on the all datasets among all models .\n",
            "Multimodal Speech Emotion Recognition and Ambiguity ResolutionIdentifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .In this work , we explore the implication of hand - crafted features for SER and compare the performance of lighter machine learning models with the heavily data - reliant deep learning models .Furthermore , we also combine features from the textual modality to understand the correlation between different modalities and aid ambiguity resolution .For both the approaches , we first extract handcrafted features from the time domain of the audio signal and train the respective models .In the first approach , we train traditional machine learning classifiers , namely , Random Forests , Gradient Boosting , Support Vector Machines , Naive - Bayes and Logistic Regression .In the second approach , we build a Multi - Layer Perceptron and an LSTM classifier to recognize emotion given a speech signal .We use librosa , a Python library , to process the audio files and extract features from them .We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .We use PyTorch to implement the LSTM classifiers described earlier .In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .We stop the training when we do not see any improvement in validation performance for > 10 epochs .From , we can see that our simpler and lighter ML models either outperform or are comparable to the much heavier current state - of - the art on this dataset .Audio - only results :Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .Text - only results :We observe that the performance of all the models for this setting is similar .c) Audio + Text results :We see that combining audio and text features gives us a boost of ? 14 % for all the metrics .Overall , we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .\n",
            "Variational Semi-supervised Aspect - term Sentiment Analysis via TransformerAspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .The variational autoencoder offers the flexibility to customize the model structure .By regarding the aspect sentiment polarity of the unlabeled data as the discrete latent variable , the model implicitly induces the sentiment polarity via the variational inference .Specifically , the representation of the lexical context is extracted by the encoder and the aspect - term sentiment polarity is inferred from the specific ATSA classifier .In addition , by separating the representation of the input sentence , the classifier becomes an independent module in our framework , which endows the method with the ability to integrate different classifiers .The number of units in the encoder and the decoder is 100 and the latent variable is of size 50 and the number of layers of both Transformer blocks is 2 , the number of selfattention heads is 8 .In this work , the KL weight is set to be 1e - 4 .TC - LSTM : Two LSTMs are used to model the left and right context of the target separately , then the concatenation of two representations is used to predict the label .MemNet : It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence , the vector of the final round is used for the prediction .IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and makes use of a special gate layer to combine these two representations .TNet - AS : Without using an attention module , TNet adopts a convolutional layer to get salient features from the transformed word representations originated from a bidirectional LSTM layer .From the , the ASVAET is able to improve supervised performance consistently for all classifiers .For the MemNet , the test accuracy can be improved by about 2 % by the TSSVAE , and so as the Macro - averaged F1 .The TNet - AS outperforms the other three models .Compared with the other two semi-supervised methods , the ASVAET also shows better results .The ASVAET outperforms the compared semisupervised methods evidently .The adoption of indomain pre-trained word vectors is beneficial for the performance compared with the Glove vectors .\n",
            "Contextual Inter-modal Attention for Multi-modal Sentiment AnalysisTraditionally , sentiment analysis has been applied to a wide variety of texts .In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context .The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .The model facilitates this modality selection by attending over the contextual utterances and thus generates better multimodal feature representation when these modalities from the context are combined with the modalities of the target utterance .We use Bi-directional GRUs having 300 neurons , each followed by a dense layer consisting of 100 neurons .Utilizing the dense layer , we project the input features of all the three modalities to the same dimensions .We set dropout = 0.5 ( MOSI ) & 0.3 ( MOSEI ) as a measure of regularization .In addition , we also use dropout = 0.4 ( MOSI ) & 0.3 ( MOSEI ) for the Bi - GRU layers .We employ ReLu activation function in the dense layers , and softmax activation in the final classification layer .For training the network we set the batch size = 32 , use Adam optimizer with cross - entropy loss function and train for 50 epochs .For MOSEI dataset , we obtain better performance with text .For text - acoustic input pairs , we obtain the highest accuracies with 79. 74 % , 79.60 % and 79.32 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .Finally , we experiment with tri-modal inputs and observe an improved performance of 79. 80 % , 79.76 % and 79.63 % for MMMU - BA , MMUU - SA and MU - SA frameworks , respectively .The performance improvement was also found to be statistically significant ( T-test ) than the bimodality and uni-modality inputs .Further , we observe that the MMMU - BA framework reports the best accuracy of 79 . 80 % for the MOSEI dataset , thus supporting our claim that multi-modal attention framework ( i.e. MMMU - BA ) captures more information than the self - attention frameworks ( i.e. MMUU - SA & MU - SA ) .\n",
            "Aspect Level Sentiment Classification with Deep Memory NetworkAspect level sentiment classification is a fundamental task in the field of sentiment analysis .In pursuit of this goal , we develop deep memory network for aspect level sentiment classification , which is inspired by the recent success of computational models with attention mechanism and explicit memory .Our approach is data - driven , computationally efficient and does not rely on syntactic parser or sentiment lexicon .The approach consists of multiple computational layers with shared parameters .Each layer is a content - and location - based attention model , which first learns the importance / weight of each context word and then utilizes this information to calculate continuous text representation .The text representation in the last layer is regarded as the feature for sentiment classification .As every component is differentiable , the entire model could be efficiently trained end - toend with gradient descent , where the loss function is the cross - entropy error of sentiment classification .( 1 ) Majority is a basic baseline method , which assigns the majority sentiment label in training set to each instance in the test set .( 2 ) Feature - based SVM performs state - of - the - art on aspect level sentiment classification .( 3 ) We compare with three LSTM models ( Tang et al. , 2015 a ) ) .In LSTM , a LSTM based recurrent model is applied from the start to the end of a sentence , and the last hidden vector is used as the sentence representation .TDLSTM extends LSTM by taking into account of the aspect , and uses two LSTM networks , a forward one and a backward one , towards the aspect .TDLSTM + ATT extends TDLSTM by incorporating an attention mechanism ( Bahdanau et al. , 2015 ) over the hidden vectors .We use the same Glove word vectors for fair comparison .( 4 ) We also implement ContextAVG , a simplistic version of our approach .We can find that feature - based SVM is an extremely strong performer and substantially outperforms other baseline methods , which demonstrates the importance of a powerful feature representation for aspect level sentiment classification .Among three recurrent models , TDLSTM performs better than LSTM , which indicates that taking into account of the aspect information is helpful .We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .Among all our models from single hop to nine hops , we can observe that using more computational layers could generally lead to better performance , especially when the number of hops is less than six .The best performances are achieved when the model contains seven and nine hops , respectively .On both datasets , the proposed approach could obtain comparable accuracy compared to the state - of - art feature - based SVM system .We can find that using multiple computational layers could consistently improve the classification accuracy in all these models .All these models perform comparably when the number of hops is larger than five .\n",
            "Multi - grained Attention Network for Aspect - Level Sentiment ClassificationWe propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .In this paper , we propose a multi -grained attention network to address the above two issues in aspect level sentiment classification .Specifically , we propose a fine - grained attention mechanism ( i.e. F- Aspect2Context and F - Context2Aspect ) , which is employed to characterize the word - level interactions between aspect and context words , and relieve the information loss occurred in coarse - grained attention mechanism .In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .In our experiments , word embeddings for both context and aspect words are initialized by Glove .The dimension of word embedding d v and hidden stated are 1 The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.set to 300 .The weight matrix and bias are initialized by sampling from a uniform distribution U ( 0.01 , 0.01 ) .The coefficient ? of L 2 regularization item is 10 ? 5 , the parameter ?of aspect alignment loss and dropout rate are set to 0.5 .Majority is the basic baseline , which chooses the largest sentiment polarity in the training set to each instance in the test set .MemNet applys multi-hop attentions on the word embeddings , learns the attention weights on context word vectors with respect to the averaged query vector .IAN interactively learns the coarse - grained attentions between the context and aspect , and concatenate the vectors for prediction .BILSTM - ATT -G ( Liu and Zhang , 2017 ) models left and right context with two attention - based LSTMs and utilizes gates to control the importance of left context , right context and the entire sentence for prediction .RAM learns multi-hop attentions on the hidden states of bidirectional LSTM networks for context words , and proposes to use GRU network to get the aggregated vector from the attentions .MGAN - C only employs the coarse - grained attentions for prediction , which is similar with IAN .MGAN - F only utilizes the proposed fine - grained attentions for prediction .MGAN - CF adopts both the coarse - grained and fine - grained attentions , while without applying the aspect alignment loss .MGAN is the complete multi-grained attention network model .( 1 ) Majority performs worst since it only utilizes the data distribution information .Our method MGAN outperforms Majority and Feature + SVM since MGAN could learn the high quality representation for prediction .( 2 ) ATAE - LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to generate the final representation .TD - LSTM performs slightly better than ATAE - LSTM , and it employs two LSTM networks to capture the left and right context of the aspect .TD - LSTM performs worse than our method MGAN since it could not properly pay more attentions on the important parts of the context .( 3 ) IAN achieves slightly better results with the previous LSTM - based methods , which interactively learns the attended aspect and context vector as final representation .Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN .BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves better performance than MemNet .RAM performs better than other baselines .Our proposed MGAN consistently performs better than MemNet , BILSTM - ATT - G and RAM on all three datasets .\n",
            "Representation learning ) plays a critical role in many modern machine learning systems .We focus in on the task of sentiment analysis and attempt to learn an unsupervised representation that accurately contains this concept .As an approach , we consider the popular research benchmark of byte ( character ) level language modelling due to its further simplicity and generality .We train on a very large corpus picked to have a similar distribution as our task of interest .Review Sentiment AnalysisThe representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .It matches the performance of baselines using as few as a dozen labeled examples and outperforms all previous results with only a few hundred labeled examples .Confusingly , despite a 16 % relative error reduction on the binary subtask , it does not reach the state of the art of 53.6 % on the fine - grained subtask , achieving 52.9 % .L1 regularization is known to reduce sample complexity when there are many irrelevant features .Fitting a threshold to this single unit achieves a test accuracy of 92.30 % which outperforms a strong supervised results on the dataset , the 91.87 % of NB - SVM trigram , but is still below the semi-supervised state of the art of 94.09 % .Using the full 4096 unit representation achieves 92.88 % .Capacity CeilingWe try our approach on the binary version of the Yelp Dataset Challenge in 2015 as introduced in .Using the full dataset , we achieve 95 . 22 % test accuracy .The observed capacity ceiling is an interesting phenomena and stumbling point for scaling our unsupervised representations .Additionally , there is a notable drop in the relative performance of our approach transitioning from sentence to document datasets .Finally , as the amount of labeled data increases , the performance of the simple linear model we train on top of our static representation will eventually saturate .\n",
            "ICON : Interactive Conversational Memory Network for Multimodal Emotion DetectionEmotion recognition in conversations is crucial for building empathetic machines .Analyzing emotional dynamics in conversations , however , poses complex challenges .We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .First , it extracts multimodal features from all utterancevideos .Next , given a test utterance to be classified , ICON considers the preceding utterances of both speakers falling within a context - window and models their self - emotional influences using local gated recurrent units .Furthermore , to incorporate inter -speaker influences , a global representation is generated using a GRU that intakes output of the local GRUs .For each instance in the context - window , the output of this global GRU is stored as a memory cell .These memories are then subjected to multiple read / write cycles that include attention mechanism for generating contextual summaries of the conversational history .At each iteration , the representation of the test utterance is improved with this summary representation and finally used for prediction .20 % of the training set is used as validation set for hyper - parameter tuning .We use the Adam optimizer ( Kingma and Ba , 2014 ) for training the parameters starting with an initial learning rate of 0.001 .Termination of the training - phase is decided by early - stopping with a patience of 10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs .The network is subjected to regularization in the form of Dropout and Gradient - clipping for a norm of 40 .Finally , the best hyper - parameters are decided using a gridsearch .For multimodal feature extraction , we explore different designs for the employed CNNs .For text , we find the single layer CNN to perform at par with deeper variants .For visual features , however , a deeper CNN provides better representations .We also find that contextually conditioned features perform better than context - less features .memnet is an end - toend memory network .cLSTM 4 classifies utterances using neighboring utterances ( of same speaker ) as context .TFN 5 models intra-and intermodality dynamics by explicitly aggregating uni - , bi- and trimodal interactions .MFN performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .CMN models separate contexts for both speaker and listener to an utterance .ICON performs better than the compared models with significant performance increase in emotions ( ? 2.1 % acc. ) .For each emotion , ICON outperforms all the compared models except for happiness emotion .However , its performance is still at par with c LSTM without a significant gap .Also , ICON manages to correctly identify the relatively similar excitement emotion by a large margin .In all the labels , ICON attains improved performance over its counterparts , suggesting the efficacy of its context - modeling scheme .presents the results for different combinations of modes used by ICON on IEMOCAP .As seen , the trimodal network provides the best performance which is preceded by the bimodal variants .Among unimodals , language modality performs the best , reaffirming its significance in multimodal systems .Interestingly , the audio and visual modality , on their own , do not provide good performance , but when used with text , complementary data is shared to improve over all performance .Self vs Dual History :Compared to the dual - history variants ( variants 3 , 5 , and 7 ) , these models provide lesser performance .DGIM prevents the storage of dynamic influences between speakers at each historical time step and leads to performance deterioration .Multi - hop vs No - hop : Variants 2 and 3 represent cases where multi-hop is omitted , i.e. , R = 1 . Performance for them are poorer than variants having multi-hop mechanism ( variants 4 - 7 ) .Also , removal of multi-hop leads to worse performance than the removal of DGIM .However , best performance is achieved by variant 6 which contains all the proposed modules in its pipeline .\n",
            "Attention - based LSTM for Aspect - level Sentiment ClassificationAspect - level sentiment classification is a finegrained task in sentiment analysis .In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .In this paper , we propose an attention mechanism to enforce the model to attend to the important part of a sentence , in response to a specific aspect .We design an aspect - tosentence attention mechanism that can concentrate on the key part of a sentence given the aspect .We explore the potential correlation of aspect and sentiment polarity in aspect - level sentiment classification .In order to capture important information in response to a given aspect , we design an attentionbased LSTM .We apply the proposed model to aspect - level sentiment classification .In our experiments , all word vectors are initialized by Glove 1 .The word embedding vectors are pre-trained on an unlabeled corpus whose size is about 840 billion .The other parameters are initialized by sampling from a uniform distribution U (?? , ? ) .The dimension of word vectors , aspect embeddings and the size of hidden layer are 300 .The length of attention weights is the same as the length of sentence .Theano is used for implementing our neural network models .We trained all models with a batch size of 25 examples , and a momentum of 0.9 , L 2 - regularization weight of 0.001 and initial learning rate of 0.01 for AdaGrad .LSTM : Standard LSTM can not capture any aspect information in sentence , so it must get the same ( a ) the aspect of this sentence : service ( b ) the aspect of this sentence : food : Attention Visualizations .Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .TD - LSTM : TD - LSTM can improve the performance of sentiment classifier by treating an aspect as a target .Since there is no attention mechanism in TD - LSTM , it can not \" know \" which words are important for a given aspect .It is worth noting that TC - LSTM performs worse than LSTM and TD - LSTM in .ATAE - LSTM not only addresses the shortcoming of the unconformity between word vectors and aspect embeddings , but also can capture the most important information in response to a given aspect .\n",
            "Utilizing BERT for Aspect - Based Sentiment Analysis via Constructing Auxiliary SentenceAspect - based sentiment analysis ( ABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect , is a challenging subtask of sentiment analysis ( SA ) .Both SA and ABSA are sentence - level or document - level tasks , but one comment may refer to more than one object , and sentence - level tasks can not handle sentences with multiple targets .Therefore , introduce the task of targeted aspect - based sentiment analysis ( TABSA ) , which aims to identify fine - grained opinion polarity towards a specific aspect associated with a given target .In this paper , we investigate several methods of constructing an auxiliary sentence and transform ( T ) ABSA into a sentence - pair classification task .We fine - tune the pre-trained model from BERT and achieve new state - of - the - art results on ( T ) ABSA task .We use the pre-trained uncased BERT - base model 5 for fine - tuning .The number of Transformer blocks is 12 , the hidden layer size is 768 , the number of self - attention heads is 12 , and the total number of parameters for the pretrained model is 110M .the dropout probability at 0.1 , set the number of epochs to 4 .The initial learning rate is 2 e - 5 , and the batch size is 24 .LR : a logistic regression classifier with n-gram and pos-tag features .LSTM - Final ) : a biLSTM model with the final state as a representation .LSTM - Loc ) : a biLSTM model with the state associated with the target position as a representation .LSTM + TA + SA ) : a biLSTM model which introduces complex target - level and sentence - level attention mechanisms .SenticLSTM : an upgraded version of the LSTM + TA + SA model which introduces external information from Sentic - Net .Dmu - Entnet : a bidirectional EntNet with external \" memory chains \" with a delayed memory update mechanism to track entities .We find that BERT - single has achieved better results on these two subtasks , and BERT - pair has achieved further improvements over BERT - single .The BERT - pair - NLI - B model achieves the best performance for aspect category detection .For aspect category polarity , BERTpair - QA - B performs best on all 4 - way , 3 - way , and binary settings .\n",
            "Recurrent Attention Network on Memory for Aspect Sentiment AnalysisIn this paper , we propose a novel framework to solve the above problems in target sentiment analysis .Specifically , our framework first adopts a bidirectional LSTM ( BLSTM ) to produce the memory ( i.e. the states of time steps generated by LSTM ) from the input , as bidirectional recurrent neural networks ( RNNs ) were found effective for a similar purpose in machine translation .The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .Our framework introduces a novel way of applying multiple - attention mechanism to synthesize important features in difficult sentence structures .Average Context :There are two versions of this method .The first one , named AC - S , averages the word vectors before the target and the word vectors after the target separately .The second one , named AC , averages the word vectors of the full context .SVM : The traditional state - of - the - art method using SVMs on surface features , lexicon features and parsing features , which is the best team in SemEval 2014 .Rec - NN : It firstly uses rules to transform the dependency tree and put the opinion target at the root , and then performs semantic composition with Recursive NNs for sentiment prediction .TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the information before and after the target .TD - LSTM - A : We developed TD - LSTM to make it have one attention on the outputs of 3 https://github.com/svn2github/word2vecMemNet : It applies attention multiple times on the word embeddings , and the last attention 's output is fed to softmax for prediction , without combining the results of different attentions .As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .AC and AC - S perform poorly , because averaging context is equivalent to paying identical attention to each word which would hide the true sentiment word .Rec - NN is better than TD - LSTM but not as good as our method .TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .MemNet adopts multiple attentions in order to improve the attention results , given the assumption that the result of an attention at a later hop should be better than that at the beginning .\n",
            "SentiHood : Targeted Aspect Based Sentiment Analysis Dataset for Urban NeighbourhoodsIn this paper , we introduce the task of targeted aspect - based sentiment analysis .Sentiment analysis is an important task in natural language processing .Aspect - based sentiment analysis ( ABSA ) relates to the task of extracting fine - grained information by identifying the polarity towards different aspects of an entity in the same unit of text , and recognizing the polarity associated with each aspect separately .Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences ( often a tweet ) .SentiHood currently contains annotated sentences containing one or two location entity mentions .2 Sen-tiHood contains 5215 sentences with 3862 sentences containing a single location and 1353 sentences containing multiple ( two ) locations .\" Positive \" sentiment is dominant for aspects such as dining and shopping .The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences .Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .Location entity names are masked by location1 and location 2 in the whole dataset , so the task does not involve identification and segmentation of the named entities .Logistic RegressionMany existing works in the aspect - based sentiment analysis task , 3 use a classifier , such as logistic regression or SVM , based on linguistic features such as n-grams , POS information or more hand - engineered features .More concretely , we define the following sparse representations of locations :Mask target entity n-grams :Left - right n- grams : we create an n-gram representation for both the right and the left context around each location mention .Left right pooling :Long Short - Term Memory ( LSTM )Inspired by the recent success of applying deep neural networks on language tasks , we use a bidirectional LSTM to learn a classifier for each of the aspects .Representations for a location ( e l ) are obtained using one of the following two approaches :Final output state ( LSTM - Final ) : e l is the output embedding of the bidirectional LSTM .Location output state ( LSTM - Location ) :As we can see , the n-gram representation with location masking achieves slightly better results over the left - right context .Also , by adding POS information , we gain an increase in the performance .Separating the left and the right context ( LR - Left - Right ) for BoW representation , does not improve the performance .Amongst the two variations of LSTM , the model with final state embeddings does slightly better than the model where we use the embeddings at the location index , however they are not significantly different ( with a p valueless than 0.01 ) .It is interesting to note that the best LSTM model is not superior to logistic regression model , especially in terms of AUC .Another interesting observation is that the F 1 measure for logistic regression model with n-grams and POS information is very low while this model 's performance is superior to other models in terms of AUC .\n",
            "IARM : Inter-Aspect Relation Modeling with Memory Networks in Aspect - Based Sentiment AnalysisSentiment analysis has immense implications in modern businesses through user-feedback mining .Aspect - based sentiment analysis ( ABSA ) caters to these needs .The aim of the ABSA classifier is to learn these connections between the aspects and their sentiment bearing phrases .To model these scenarios , firstly , following , we independently generate aspect - aware sentence representations for all the aspects using gated recurrent unit ( GRU ) and attention mechanism .Then , we employ memory networks to repeatedly match the target aspect representation with the other aspects to generate more accurate representation of the target aspect .This refined representation is fed to a softmax classifier for final classification .LSTMFollowing , the sentence is fed to along short - term memory ( LSTM ) network to propagate context among the constituent words .TD- LSTMFollowing , sequence of words preceding ( left context ) and succeeding ( right context ) target aspect term are fed to two different LSTMs .This representation is fed to softmax classifier. , ATAE - LSTM is identical to AE - LSTM , except the LSTM is fed with the concatenation of aspect - term representation and word representation . , target - aspect and its context are sent to two distinct LSTMs and the means of the hidden outputs are taken as intermediate aspect representation and context representation respectively .It is evident from the results that our IARM model outperforms all the baseline models , including the state of the art , in both of the domains .We obtained bigger improvement in laptop domain , of 1.7 % , compared to restaurant domain , of 1.4 % .\n",
            "Exploring Joint Neural Model for Sentence Level Discourse Parsing and Sentiment AnalysisOur framework consists of three main sub parts .Given a segmented sentence , the first step is to create meaningful vector representations for all the EDUs .Next , we devise three different Recursive Neural Net models , each designed for one of discourse structure prediction , discourse relation prediction and sentiment analysis .Finally , we join these Neural Nets in two different ways : Multitasking and Pre-training .All the neural models presented in this paper were implemented using the Tensor Flow python pack - .We minimize the crossentropy error using the Adam optimizer and L2regularization on the set of weights .For the individual models ( before joining ) , we use 200 training epochs and a batch size of 100 .From the results , we see some improvement on Discourse Structure prediction when we are using a joint model but the improvement is statistically significant only for the Nuclearity and Relation predictions .The improvements on the Relation predictions were mainly on the Contrastive set , specifically the class of Contrast , Comparison and Cause relations as .In the fine grained setting we compute the accuracy of exact match across five classes .\n",
            "Mazajak : An Online Arabic Sentiment AnalyserSentiment analysis ( SA ) is one of the most useful natural language processing applications .Sentiment analysis is one of the vital approaches to extract public opinion from large corpora of text .Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .The system is available as an online API that can be used by other researchers .The best performing system in the SemEval 2017 task is the one described in which achieved an F P N of 0.61 .For the ASTD , the best reported results are by who used an ensemble system combining output of CNN and Bi - LSTM architectures , which achieved an F P N of 0.71 .As shown in the table , Mazajak model outperformed the current state - of - the - art models on the SemEval and ASTD datasets .In addition , it achieved a high performance on the ArSAS dataset .Our reported scores are higher than current top systems for all the evaluation scores , including average recall , F P N , and accuracy .These results confirm that our model choice for our tool represents the current state - of - the - art for Arabic SA .\n",
            "Parameterized Convolutional Neural Networks for Aspect Level Sentiment ClassificationContinuous growing of user generated text in social media platforms such as Twitter drives sentiment classification increasingly popular .Differing from general sentiment classification , aspect level sentiment classification identifies opinions from text about specific entities and their aspects .In the present work , we propose two simple yet effective convolutional neural networks with aspect information incorporated .Specifically , we design two novel neural units that take target aspects into account .One is parameterized filter , the other is parameterized gate .These units both are generated from aspect - specific features and are further applied on the sentence .We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 .Parameterized filters and gates have the same size and number as normal filters .They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively .The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .We apply dropout on the final classification features of PG - CNN .The dropout rate is chosen as 0.3 .Training is done through mini-batch stochastic gradient descent with Adam update rule .The initial learning rate is 0.001 .If the training loss does not drop after every three epochs , we decrease the learning rate by half .We adopt early stopping based on the validation loss on development sets .TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .AT - LSTM combines the sentence hidden states from a LSTM with the aspect term embedding to generate the attention vector .ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .AF - LSTM introduces a word - aspect fusion attention to learn associative relationships between aspect and context words .CNN uses the architecture proposed in without explicitly considering aspect .Our two models achieve the best performance when compared to these baselines as shown in , which shows that our proposed neural units effectively captures the aspect - specific features .Compared to one recently proposed model AF - LSTM , our method achieve 2 % - 5 % improvements .Surprisingly , a vanilla CNN works quite well on this problem .It even beats these welldesigned LSTM models , which further proves that using CNN - based methods is a direction worth exploring .\n",
            "Convolutional Neural Networks with Recurrent Neural FiltersIn this work , we model convolution filters with RNNs that naturally capture compositionality and long - term dependencies in language .To overcome this , we propose to employ recurrent neural networks ( RNNs ) as convolution filters of CNN systems for various NLP tasks .Our recurrent neural filters ( RNFs ) can naturally deal with language compositionality with a recurrent function that models word relations , and they are also able to implicitly model long - term dependencies .RNFs are typically applied to word sequences of moderate lengths , which alleviates some well - known drawbacks of RNNs , including their vulnerability to the gradient vanishing and exploding problems .As in conventional CNNs , the computation of the convolution operation with RNFs can be easily parallelized .As a result , RNF - based CNN models can be 3 - 8 x faster than their RNN counterparts .We present two RNF - based CNN architectures for sentence classification and answer sentence selection problems .We consider CNN variants with linear filters and RNFs.For RNFs , we adopt two implementations based on GRUs and LSTMs respectively .We also compare against the following RNN variants : GRU , LSTM , GRU with max pooling , and LSTM with max pooling .In particular , CNN - RNF - LSTM achieves 53.4 % and 90.0 % accuracies on the fine - grained and binary sentiment classification tasks respectively , which match the state - of the - art results on the Stanford Sentiment Treebank .CNN - RNF - LSTM also obtains competitive results on answer sentence selection datasets , despite the simple model architecture compared to state - of - the - art systems .Conventional RNN models clearly benefit from max pooling , especially on the task of answer sentence selection .As a result , RNF - based CNN models perform consistently better than max - pooled RNN models .\n",
            "Transformation Networks for Target - Oriented Sentiment ClassificationWe propose a new architecture , named Target - Specific Transformation Networks ( TNet ) , to solve the above issues in the task of target sentiment classification .TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs .To integrate the target information into the word representations , TNet introduces a novel Target - Specific Transformation ( TST ) component for generating the target - specific word representations .Contrary to the previous attention - based approaches which apply the same target representation to determine the attention scores of individual context words , TST firstly generates different representations of the target conditioned on individual context words , then it consolidates each context word with its tailor - made target representation to obtain the transformed word representation .As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST , we design a contextpreserving mechanism to contextualize the generated target - specific word representations .To help the CNN feature extractor locate sentiment indicators more accurately , we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target .SVM : It is a traditional support vector machine based model with extensive feature engineering ;AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ;AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ;IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ;CNN - ASP :It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ;TD - LSTM :It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ;MemNet : It applies attention mechanism over the word embeddings multiple times and predicts sentiments based on the top - most sentence representations ;BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and introduces gates to measure the importance of left context , right context , and the entire sentence for the prediction ;RAM : RAM is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .LSTM - based models relying on sequential information can perform well for formal sentences by capturing more useful context features ;For ungrammatical text , CNN - based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns .After removing the deep transformation ( i.e. , the techniques introduced in Section 2.2 ) , both TNet - LF and TNet - AS are reduced to TNet w/o transformation ( where position relevance is kept ) , and their results in both accuracy and F 1 measure are incomparable with those of TNet .It shows that the integration of target information into the word - level representations is crucial for good performance .Comparing the results of TNet and TNet w/o context ( where TST and position relevance are kept ) , we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST 7 , while on TWITTER , TNet w/o context performs very competitive ( p- values with TNet - LF and TNet - AS are 0.066 and 0.053 respectively for Accuracy ) .TNet w/o context performs consistently better than TNet w/o transformation , which verifies the efficacy of the target specific transformation ( TST ) , before applying context - preserving .All of the produced p-values are less than 0.05 , suggesting that the improvements brought in by position information are significant .\n",
            "Interactive Attention Networks for Aspect - Level Sentiment ClassificationPrevious approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .Based on the two points analyzed above , we propose an interactive attention network ( IAN ) model which is based on long - short term memory networks ( LSTM ) and attention mechanism .IAN utilizes the attention mechanism associated with a target to get important information from the context and compute context representation for sentiment classification .Further , IAN makes use of the interactive information from context to supervise the modeling of the target which is helpful to judging sentiment .Finally , with both target representation and context representation concatenated , IAN predicts the sentiment polarity for the target within its context .In our experiments , all word embeddings from context and target are initialized by GloVe 2 , and all out - of - vocabulary words are initialized by sampling from the uniform distribution U ( ?0.1 , 0.1 ) .All weight matrices are given their initial values by sampling from uniform distribution U ( ?0.1 , 0.1 ) , and all biases are set to zeros .The dimensions of word embeddings , attention vectors and LSTM hidden states are set to 300 as in .To train the parameters of IAN , we employ the Momentum , which adds a fraction ? of the update vector in the prior step to the current update vector .The coefficient of L 2 normalization in the objective function is set to 10 ?5 , and the dropout rate is set to 0.5 .Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .LSTM only uses one LSTM network to model the context and get the hidden state of each word .TD - LSTM adopts two long short - term memory ( LSTM ) networks to model the left context with target and the right context with target respectively .AE - LSTM represents targets with aspect embeddings .ATAE - LSTM is developed based on AE - LSTM .All the other methods are based on LSTM models and better than the Majority method , showing that LSTM has potentials in automatically generating representations and can all bring performance improvement for sentiment classification .The LSTM method gets the worst performance of all the neural network baseline methods , because it treats targets equally with other context words and does not make full use of the target information .TD - LSTM outperforms LSTM over 1 percent and 2 percent on the Restaurant and Laptop category respectively , since it develops from the standard LSTM and processes the left and right contexts with targets .Further , both AE - LSTM and ATAE - LSTM stably exceed the TD - LSTM method because of the introduction of attention mechanism .For AE - LSTM and ATAE - LSTM , they capture important information in the context with the supervision of target and generate more reasonable representations for aspect - level sentiment classification .We can also see that AE - LSTM and ATAE - LSTM further emphasize the modeling of targets via the addition of the aspect embedding , which is also the reason of performance improvement .Compared with AE - LSTM , ATAE - LSTM especially enhance the interaction between the context words and target and thus has a better performance than AE - LSTM .We can see that IAN achieves the best performance among all baselines .Compared with ATAE - LSTM model , IAN improves the performance about 1.4 % and 3.2 % on the Restaurant and Laptop categories respectively .The more attentions are paid to targets , the higher accuracy the system achieves .\n",
            "Aspect Level Sentiment Classification with Attention - over - Attention Neural NetworksAspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .Previous LSTM - based methods mainly focus on modeling texts separately , while our approach models aspects and texts simultaneously using LSTMs .Furthermore , the target representation and text representation generated from LSTMs interact with each other by an attention - over - attention ( AOA ) module .AOA automatically generates mutual attentions not only from aspect - to - text but also text - to - aspect .That is why we choose AOA to attend to the most important parts in both aspect and sentence .In experiments , we first randomly select 20 % of training data as validation set to tune the hyperparameters .All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .The dimension of LSTM hidden states is set to 150 .The initial learning rate is 0.01 for the Adam optimizer .If the training loss does not drop after every three epochs , we decrease the learning rate by half .The batch size is set as 25 .Majority is a basic baseline method , which assigns the largest sentiment polarity in the training set to each sample in the test set .LSTM uses one LSTM network to model the sentence , and the last hidden state is used as the sentence representation for the final classification .TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .AT - LSTM first models the sentence via a LSTM model .ATAE - LSTM further extends AT - LSTM by appending the aspect embedding into each word vector .IAN uses two LSTM networks to model the sentence and aspect term respectively .In our implementation , we found that the performance fluctuates with different random initialization , which is a well - known issue in training neural networks .On average , our algorithm is better than these baseline methods and our best trained model outperforms them in a large margin .\n",
            "Exploiting Coarse - to - Fine Task Transfer for Aspect - level Sentiment ClassificationAspect - level sentiment classification ( ASC ) aims at identifying sentiment polarities towards aspects in a sentence , where the aspect can behave as a general Aspect Category ( AC ) or a specific Aspect Term ( AT ) .To model aspect - oriented sentiment analysis , equipping Recurrent Neural Networks ( RNNs ) with the attention Copyright c 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .First , to reduce the task discrepancy between domains , i.e. , modeling the two tasks at the same fine - grained level , we propose a novel Coarse2 Fine ( C2F ) attention module to help the source task automatically capture the corresponding aspect term in the context towards the given aspect category ( e.g. , \" salmon \" to the \" food \" ) .To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .The CFA considers both semantic alignment by maximally ensuring the equivalent distributions from different domains but the same class , and semantic separation by guaranteeing distributions from both different classes and domains to be as dissimilar as possible .The word embeddings are initialized with 200 - dimension GloVE vectors and fine - tuned during the training .The fc layer size is 300 .The Adam ( Kingma and Ba 2014 ) is used as the optimizer with the initial learning rate 10 ? 4 .Gradients with the 2 norm larger than 40 are normalized to be 40 .All weights in networks are randomly initialized from a uniform distribution U ( ? 0.01 , 0.01 ) .The batch sizes are 64 and 32 for source and target domains , respectively .To alleviate overfitting , we apply dropout on the word embeddings of the context with dropout rate 0.5 .We also perform early stopping on the validation set during the training process .The hyperparameters are tuned on 10 % randomly held - out training data of the target domain in R1?L task and are fixed to be used in all transfer pairs .Non-TransferTo demonstrate the benefits from coarse - tofine task transfer , we compare with the following state - of the - art AT - level methods without transfer : Target Network ( TN ) :It is our proposed base model ( BiLSTM + C2A + Pas ) trained on D t for the target task .TransferSource- only ( SO ) :It uses a source network trained on D s to initialize a target network and then tests it on D t .Fine-tuning ( FT ) : It advances SO with further finetuning the target network on D t .M- DAN : It is a multi-adversarial version of Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .M - MMD : Similar with M - DAN , M - MMD aligns different class distributions between domains based on multiple Maximum Mean Discrepancy ( MMD ) ) .Comparison with Non - Transfer( 2 ) MGAN consistently outperforms the MGAN w / o C2 F , where C2F module of the source network is removed and the source position information is missed ( we set all p s i to 1 ) , by 1.41 % , 1.03 % , 1.09 % for accuracy and 1.79 % , 3.62 % and 1.16 % for Macro - F1 on average .The MGAN w / o PI , which does not utilize the position information , performs very poorly .Comparison with TransferSO performs poorly due to no adaptation applied .The popular technique FT can not achieve satisfactory results since fine - tuning may cause the oblivion of useful knowledge from the source task .The full model MGAN outperforms M - DAN and M - MMD by 1.80 % and 1.33 % for accuracy and 1.90 % and 1.66 % for Marco - F1 on average , respectively .Remarkably , MGAN considers both of them in a point - wise surrogate , which altogether improves the performance of our method .Besides , MGAN outperforms its ablation MGAN w/ o SS removing the semantic separation loss of the CFA by 0.81 % for accuracy and 1.00 % for Macro - F1 on average , which implies that the semantic separation plays an important role in alleviating false alignment .Effect of C2F Attention ModuleThen , compared with MGAN w / o C2F , MGAN further uses C2F to capture more specific aspect terms from the context towards the aspect category , such as \" shells \" to food seafood sea , which helps the source task capture more fine - grained semantics of aspect category and detailed position information like the target task , such that the sentiment attention can be positionaware and identify more relevant sentiment features towards the aspect .While MGAN w / o C2F locates wrong sentiment contexts and fails in ( c ) .As such , benefited from distilled knowledge from the source task , MGAN can better model the complicated relatedness between the context and aspect term for the target domain L , but MGAN w / o C2F performs poorly though it make true predictions in ( d ) and ( e ) .\n",
            "Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect - based Sentiment AnalysisWhile neural networks have been shown to achieve impressive results for sentence - level sentiment analysis , targeted aspect - based sentiment analysis ( TABSA ) - extraction of finegrained opinion polarity w.r.t. a pre-defined set of aspects - remains a difficult task .In this work , we propose a novel model architecture for TABSA , augmented with multiple \" memory chains \" , and equipped with a delayed memory update mechanism , to keep track of numerous entities independently .We initialise our model with GloVe ( 300 - D , trained on 42B tokens , 1.9 M vocab , not updated during training : ) 4 and pre-process the corpus with tokenisation using NLTK ) and case folding .Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .Lastly , to curb overfitting , we regularise the last layer ( Equation ) with an L 2 penalty on its weights : ?We empirically set the number of memory chains to 6 , with the keys of two of them set to the same embeddings as the target words LOC1 and LOC2 , resp. , and the other 4 chains with free key embeddings which are updated during training , and therefore free to capture any entities .Our model achieves state - of - the - art results for both aspect detection and sentiment classification .It is impressive that the proposed model , equipped only with domainindependent general - purpose GloVe embeddings , outperforms Sentic LSTM , an approach heavily reliant on external knowledge bases and domainspecific embeddings .Ent Net vs. our model .We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .\n",
            "Attentional Encoder Network for Targeted Sentiment ClassificationTargeted sentiment classification is a fine - grained sentiment analysis task , which aims at determining the sentiment polarities ( e.g. , negative , neutral , or positive ) of a sentence over \" opinion targets \" that explicitly appear in the sentence .However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .This paper propose an attention based model to solve the problems above .Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .shows the number of training and test instances in each category .Word embeddings in AEN - Glo Ve do not get updated in the learning process , but we fine - tune pre-trained BERT 3 in AEN - BERT .Embedding dimension d dim is 300 for GloVe and is 768 for pretrained BERT .Dimension of hidden states d hid is set to 300 .The weights of our model are initialized with Glorot initialization .During training , we set label smoothing parameter to 0.2 , the coefficient ? of L 2 regularization item is 10 ? 5 and dropout rate is 0.1 .Adam optimizer ( Kingma and Ba , 2014 ) is applied to update all the parameters .We also design a basic BERT - based model to evaluate the performance of AEN - BERT .Non - RNN based baselines :Feature - based SVM is a traditional support vector machine based model with extensive feature engineering .Rec - NN firstly uses rules to transform the dependency tree and put the opinion target at the root , and then learns the sentence representation toward target via semantic composition using Recursive NNs .MemNet uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word .RNN based baselines :TD - LSTM extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively .ATAE - LSTM( Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .IAN learns the representations of the target and context with two LSTMs and attentions interactively , which generates the representations for targets and contexts with respect to each other .RAM strengthens Mem - Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation .AEN - Glo Ve ablations :AEN - GloVe w/ o PCT ablates PCT module .AEN - GloVe w/ o MHA ablates MHA module .AEN - GloVe w/ o LSR ablates label smoothing regularization .AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM .Basic BERT - based model :BERT - SPC feeds sequence \" [ CLS ] + context + [ SEP ] + target + [ SEP ] \" into the basic BERT model for sentence pair classification task .The over all performance of TD - LSTM is not good since it only makes a rough treatment of the target words .ATAE - LSTM , IAN and RAM are attention based models , they stably exceed the TD - LSTM method on Restaurant and Laptop datasets .RAM is better than other RNN based models , but it does not perform well on Twitter dataset , which might because bidirectional LSTM is not good at modeling small and ungrammatical text .Feature - based SVM is still a competitive baseline , but relying on manually - designed features .Rec - NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments .Like AEN , Mem Net also eschews recurrence , but its over all performance is not good since it does not model the hidden semantic of embeddings , and the result of the last attention is essentially a linear combination of word embeddings .Comparing the results of AEN - GloVe and AEN - Glo Ve w / o LSR , we observe that the accuracy of AEN - Glo Ve w / o LSR drops significantly on all three datasets .The over all performance of AEN - GloVe and AEN - Glo Ve - BiLSTM is relatively close , AEN - Glo Ve performs better on the Restaurant dataset .More importantly , AEN - Glo Ve has fewer parameters and is easier to parallelize .AEN - Glo Ve 's lightweight level ranks second , since it takes some more parameters than MemNet in modeling hidden states of sequences .As a comparison , the model size of AEN - Glo Ve - BiLSTM is more than twice that of AEN - GloVe , but does not bring any performance improvements .\n",
            "Knowledge - Enriched Transformer for Emotion Detection in Textual ConversationsThe task of detecting emotions in textual conversations leads to a wide range of applications such as opinion mining in social networks .This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .To this end , we propose a Knowledge - Enriched Transformer ( KET ) to effectively incorporate contextual information and external knowledge bases to address the aforementioned challenges .The self - attention and cross-attention modules in the Transformer capture the intra-sentence and inter-sentence correlations , respectively .The shorter path of information flow in these two modules compared to gated RNNs and CNNs allows KET to model contextual information more efficiently .In addition , we propose a hierarchical self - attention mechanism allowing KET to model the hierarchical structure of conversations .Our model separates context and response into the encoder and decoder , respectively , which is different from other Transformer - based models , e.g. , BERT , which directly concatenate context and response , and then train language models using only the encoder part .Moreover , to exploit commonsense knowledge , we leverage external knowledge bases to facilitate the understanding of each word in the utterances by referring to related knowledge entities .The referring process is dynamic and balances between relatedness and affectiveness of the retrieved knowledge entities using a context - aware affective graph attention mechanism .c LSTM : A contextual LSTM model .An utterance - level bidirectional LSTM is used to encode each utterance .A context - level unidirectional LSTM is used to encode the context .CNN+cLSTM : An CNN is used to extract utterance features .An c LSTM is then applied to learn context representations .BERT BASE :We treat each utterance with its context as a single document .We limit the document length to the last 100 tokens to allow larger batch size .DialogueRNN : The stateof - the - art model for emotion detection in textual conversations .It models both context and speakers information .KET SingleSelfAttn :We replace the hierarchical self - attention by a single self - attention layer to learn context representations .Contextual utterances are concatenated together prior to the single self - attention layer .KET StdAttn :We replace the dynamic contextaware affective graph attention by the standard graph attention .We preprocessed all datasets by lower - casing and tokenization using Spacy 2 .We use the released code for BERT BASE and DialogueRNN .For each dataset , all models are fine - tuned based on their performance on the validation set .For our model in all datasets , we use Adam optimization ( Kingma and Ba , 2014 ) with a batch size of 64 and learning rate of 0.0001 throughout the training process .We use Glo Ve embedding for initialization in the word and concept embedding layersFor the class weights in cross - entropy loss for each dataset , we set them as the ratio of the class distribution in the validation set to the class distribution in the training set .c LSTM performs reasonably well on short conversations ( i.e. , EC and DailyDialog ) , but the worst on long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP ) .In contrast , when the utterance - level LSTM in c LSTM is replaced by features extracted by CNN , i.e. , the CNN + c LSTM , the model performs significantly better than c LSTM on long conversations , which further validates that modelling long conversations using only RNN models may not be sufficient .BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi-directional context modelling using the Transformer .In particular , DialogueRNN performs better than our model on IEMOCAP , which maybe attributed to its detailed speaker information for modelling the emotion dynamics in each speaker as the conversation flows .This finding indicates that our model is robust across datasets with varying training sizes , context lengths and domains .Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP .However , both variants perform noticeably worse than KET on all datasets except EC , validating the importance of our proposed hierarchical self - attention and dynamic context - aware affective graph attention mechanism .One observation worth mentioning is that these two variants perform on a par with the KET model on EC .It is clear that both context and knowledge are essential to the strong performance of KET on all datasets .Note that removing context has a greater impact on long conversations than short conversations , which is expected because more contextual information is lost in long conversations .\n",
            "Discriminative Neural Sentence Modeling by Tree - Based ConvolutionIn this paper , we propose a novel neural architecture for discriminative sentence modeling , called the Tree - Based Convolutional Neural Network ( TBCNN ) .Our models can leverage different sentence parsing trees , e.g. , constituency trees and dependency trees .The model variants are denoted as c- TBCNN and d - TBCNN , respectively .The idea of tree - based convolution is to apply a set of subtree feature detectors , sliding over the entire parsing tree of a sentence ; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension .In our d-TBCNN model , the number of units is 300 for convolution and 200 for the last hidden layer .Word embeddings are 300 dimensional , pretrained ourselves using word2vec To train our model , we compute gradient by back - propagation and apply stochastic gradient descent with mini-batch 200 .We use ReLU as the activation function .For regularization , we add 2 penalty for weights with a coefficient of 10 ?5 .Dropout is further applied to both weights and embeddings .All hidden layers are dropped out by 50 % , and embeddings 40 % .Nonetheless , our d-TBCNN model achieves87.9 % accuracy , ranking third in the list .In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform \" flat \" CNNs by more than 10 % .We also observe d- TBCNN achieves higher performance than c - TBCNN .\n",
            "A Position - aware Bidirectional Attention Network for Aspect - level Sentiment AnalysisSentiment analysis , also known as opinion mining , is a vital task in Natural Language Processing ( NLP ) .Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding .2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively .3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence .In our experiments , all word embedding are initialized by the pre-trained Glove vector 2 .All the weight matrices are given the initial value by sampling from the uniform distribution U ( ?0.1 , 0.1 ) , and all the biases are set to zero .The dimension of the word embedding and aspect term embedding are set to 300 , and the number of the hidden units are set to 200 .The dimension of position embedding is set to 100 , which is randomly initialized and updated during the training process .We use Tensorflow to implement our proposed model and employ the Momentum as the training method , whose momentum parameter ? is set to 0.9 , ? is set to 10 ? 6 , and the initial learning rate is set to 0.01 .LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .Then it regards the average value of all hidden states as the representation of sentence , and puts it into softmax layer to predict the probability of each sentiment polarity .AE - LSTM : AE - LSTM first models the words in sentence via LSTM network and concatenate the aspect embedding to the hidden contextual representation for calculating the attention weights , which are employed to produce the final representation for the input sentence to judge the sentiment polarity .ATAE - LSTM : ATAE - LSTM extended AE - LSTM by appending the aspect embedding to each word embedding so as to represent the input sentence , which highlights the role of aspect embedding .IAN : IAN considers the separate modeling of aspect terms and sentences respectively .MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .We can observe that our proposed PBAN model achieves the best performance among all methods .Generally speaking , by integrating the position information and the bidirectional attention mechanism , PBAN achieves the state - of - the - art performances , and it can effectively judge the sentiment polarity of different aspect term in its corresponding sentence so as to improve the classification accuracy .\n",
            "A Helping Hand : Transfer Learning for Deep Sentiment AnalysisOver the past decades , sentiment analysis has grown from an academic endeavour to an essential analytics tool .In recent years , deep neural architectures based on convolutional or recurrent layers have become established as the preeminent models for supervised sentiment polarity classification .In this paper , we investigate how extrinsic signals can be incorporated into deep neural networks for sentiment analysis .In our paper , we instead consider word embeddings specifically specialized for the task of sentiment analysis , studying how they can lead to stronger and more consistent gains , despite the fact that the embeddings were obtained using out - of - domain data .We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .Embeddings .The standard pre-trained word vectors used for English are the GloVe ones trained on 840 billion tokens of Common Crawl data 1 , while for other languages , we rely on the Facebook fastText Wikipedia embeddings as input representations .All of these are 300 - dimensional .The vectors are either fed to the CNN , or to the convolutional module of the DM - MCNN during initialization , while unknown words are initialized with zeros .All words , including the unknown ones , are fine - tuned during the training process .For our transfer learning approach , our experiments rely on the multi-domain sentiment dataset by , collected from Amazon customers reviews .Specifically , we train linear SVMs using scikit - learn to extract word coefficients in each domain and also for the union of all domains together , yielding a 26 - dimensional sentiment embedding .For comparison and analysis , we also consider several alternative forms of infusing external cues .We consider a recent sentiment lexicon called VADER .These contain separate domain - specific scores for 250 different Reddit communities , and hence result in 250 - dimensional embeddings .For cross - lingual projection , we extract links between words from a 2017 dump of the English edition of Wiktionary .We restrict the vocabulary link set to include the languages in , mining corresponding translation , synonymy , derivation , and etymological links from Wiktionary .For CNNs , we make use of the well - known CNN - non-static architecture and hyperparameters proposed by , with a learning rate of 0.0006 , obtained by tuning on the validation data .For our DM - MCNN models , the configuration of the convolutional module is the same as for CNNs , and the remaining hyperparameter values were as well tuned on the validation sets .For greater efficiency and better convergence properties , the training relies on mini-batches .Our implementation considers the maximal sentence length in each mini-batch and zero - pads all other sentences to this length under convolutional module , thus enabling uniform and fast processing of each mini-batch .All neural network architectures are implemented using the PyTorch framework 2 .Comparing this to CNNs with GloVe / fastText embeddings , where Glo Ve is used for English , and fastText is used for all other languages , we observe substantial improvements across all datasets .This shows that word vectors do tend to convey pertinent word semantics signals that enable models to generalize better .Note also that the accuracy using GloVe on the English movies review dataset is consistent with numbers reported in previous work .Next , we consider our DM - MCNNs with their dual - module mechanism to take advantage of transfer learning .We observe fairly consistent and sometimes quite substan - tial gains over CNNs with just the GloVe / fastText vectors .Although the automatically projected cross - lingual embeddings are very noisy and limited in their coverage , particularly with respect to inflected forms , our model succeeds in exploiting them to obtain substantial gains in several different languages and domains .\n",
            "Sentiment Classification using Document Embeddings trained with Cosine SimilarityIn document - level sentiment classification , each document must be mapped to a fixed length vector .In document classification tasks such as sentiment classification ( in this paper we focus on binary sentiment classification of long movie reviews , i.e. determining whether each review is positive or negative ) , the choice of document representation is usually more important than the choice of classifier .This paper aims to improve existing document embedding models by training document embeddings using cosine similarity instead of dot product .For example , in the basic model of trying to predict given a document - the words / n - grams in the document , instead of trying to maximize the dot product between a document vector and vectors of the words / n - grams in the document over the training set , we 'll be trying to maximize the cosine similarity instead .Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.Grid search was performed using 20 % of the training data as a validation set in order to determine the optimal hyperparameters as well as whether to use a constant learning rate or learning rate annealing .We did however tune the number of iterations from , learning rate from [ 0.25 , 0.025 , 0.0025 , 0.001 ] and ? from .In the case of using L2 regularized dot product , ? ( regularization strength ) was chosen from [ 1 , 0.1 , 0.01 ] .The optimal learning rate in the case of cosine similarity is extremely small , suggesting a chaotic error surface .The model in turn requires a larger number of epochs for convergence .For the distribution for sampling negative words , we used the n-gram distribution raised to the 3 / 4 th power in accordance with .The weights of the networks were initialized from a uniform distribution in the range of [ - 0.001 , 0.001 ] .From here we see that using cosine similarity instead of dot product improves accuracy across the board .However it is not to suggest that switching from dot product to cosine similarity alone improves accuracy as other minor ad - justments and hyperparameter tuning as explained was done .As seen during grid search , whenever the initial learning rate was 0.25 , accuracy was always poor .Introducing L2 regularization to dot product improves accuracy for all cases except a depreciation in the case of using unigrams only , lucikily cosine similarity does not suffer from this same depreciation .\n",
            "Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment AnalysisAspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .Based on the analysis above , in this paper , we propose a hierarchical attention based positionaware network ( HAPN ) for aspect - level sentiment classification .A position - aware encoding layer is introduced for modelling the sentence to achieve the position - aware abstract representation of each word .On this basis , a succinct fusion mechanism is further proposed to fuse the information of aspects and the contexts , achieving the final sentence representation .Finally , we feed the achieved sentence representation into a softmax layer to predict the sentiment polarity .We make our source code public at https://github.com/DUT-LiuYang/Aspect-Sentiment-Analysis.We use 300 - dimension word vectors pre-trained by GloVe ( whose vocabulary size is 1.9M ) for our experiments , as previous works did .All out - of - vocabulary words are initialized as zero vectors , and all biases are set to zero .The dimensions of hidden states and fused embeddings are set to 300 .The dimension of position embeddings is set to 50 .Keras is used for implementing our neural network model .In model training , we set the learning rate to 0.001 , the batch size to 64 , and dropout rate to 0.5 .The paired t- test is used for the significance testing .Majority assigns the sentiment polarity with most frequent occurrences in the training set to each sample in test set .Bi - LSTM and Bi - GRU adopt a Bi - LSTM and a Bi - GRU network to model the sentence and use the hidden state of the final word for prediction respectively .TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; It takes the hidden states of LSTM at last time - step to represent the sentence for prediction .MemNet applies attention multiple times on the word embeddings , and the output of last attention is fed to softmax for prediction .IAN interactively learns attentions in the contexts and targets , and generates the representations for targets and contexts separately .RAM ) is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .LCR - Rot employs three Bi- LSTMs to model the left context , the target and the right context .AOA - LSTM introduces an attention - over- attention ( AOA ) based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences .( 2 ) The TD - LSTM model , which has been shown to be better than LSTM , gets the worst performance of all RNN based models and the accuracy achieved by TD - LSTM is 2.94 % and 2.4 % lower than those by Bi - LSTM on the two datasets respectively .( 3 ) Compared with the state - of - the - art methods , our model achieves the best performance , which illustrates the effectiveness of the proposed approach .Our method achieves accuracies of 82.23 % as well as 77 . 27 % on the Restaurant and Laptop dataset respectively , which are 0.89 % and 2.03 % higher than the current best method .After introducing the position embeddings , the accuracy has an increase of 0.62 % and 2.67 % on two datasets .In addition , another observation is that Bi - GRU - PW performs even worse than Bi - GRU .The accuracy achieved by Bi - GRU - PW is 0.72 % as well as 1.41 % lower than that by Bi - GRU on the Restaurant and Laptop dataset respectively .HAPN achieves improvement of 0.35 % and 0.78 % on accuracy respectively on the two dataset .( 1 ) The information fusion operation is only used to calculate the Source2context attention value .The output of Source2aspect attention is only used for information fusion .And the achieved model is \" Bi - GRU - PE \" reported in the , achieving the accuracies of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .\n",
            "DataStories at SemEval-2017 Task 4 : Deep LSTM with Attention for Message - level and Topic - based Sentiment AnalysisIn this paper we present two deep - learning systems that competed at SemEval - 2017 Task 4 \" Sentiment Analysis in Twitter \" .Sentiment analysis is an area in Natural Language Processing ( NLP ) , studying the identification and quantification of the sentiment expressed in text .In this paper , we present two deep - learning systems that competed at SemEval - 2017 Task 4 .Our first model is designed for addressing the problem of messagelevel sentiment analysis .We employ a 2 - layer Bidirectional LSTM , equipped with an attention mechanism .For the topic - based sentiment analysis tasks , we propose a Siamese Bidirectional LSTM with a contextaware attention mechanism .MSA Model ( message - level )TSA Model ( topic - based )The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .We add Gaussian noise with ? = 0.2 and dropout of 0.3 at the embedding layer , dropout of 0.5 at the LSTM layers and dropout of 0.25 at the recurrent connections of the LSTM .Finally , we add L 2 regularization of 0.0001 at the loss function .The size of the embedding layer is 300 , and the LSTM layers 64 ( 128 for BiLSTM ) .We insert Gaussian noise with ? = 0.2 at the embedding layer of both inputs and dropout of 0.3 at the embedding layer of the message , dropout of 0.2 at the LSTM layer and the recurrent connection of the LSTM layer and dropout of 0.3 at the attention layer and the Maxout layer .Finally , we add L 2 regularization of 0.001 at the loss function .Our official ranking is 1/38 ( tie ) in Subtask A , 2/24 in Subtask B , 2/16 in Subtask C , 2/16 in Subtask D and 11/12 in Subtask E.\n",
            "Improved Sentence Modeling using Suffix Bidirectional LSTMRecurrent neural networks have become ubiquitous in computing representations of sequential data , especially textual data in natural language processing .Using SuBiLSTM we achieve new state - of - the - art results for fine - grained sentiment classification and question classification .Recurrent Neural Networks ( RNN ) ) have emerged as a powerful tool for modeling sequential data .In this paper , we propose a simple , general and effective technique to compute contextual representations that capture long range dependencies .For each token t , we encode both its prefix and suffix in both the forward and reverse direction .Further , we combine the prefix and suffix representations by a simple max - pooling operation to produce a richer contextual representation of t in both the forward and reverse direction .We call our model Suffix BiLSTM or SuBiLSTM in short .For each of the tasks , we compare SuBiLSTM and SuBiLSTM - Tied with a single - layer BiLSTM and a 2 - layer BiLSTM encoder with the same hidden dimension .The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .SuBiLSTM - Tied works better on small datasets ( SST and TREC ) , probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes .For the larger datasets ( SNLI and QUORA ) , SuBILSTM slightly edges out the tied version owing to its larger capacity .The training complexity for both the models is similar and hence , with half the parameters , SuBILSTM - Tied should be the more favored model for sentence modeling tasks .\n",
            "Modeling Sentiment Dependencies with Graph Convolutional Networks for Aspect - level Sentiment ClassificationIt is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .In this paper , we propose a novel method to model Sentiment Dependencies with Graph Convolutional Networks ( SDGCN ) for aspect - level sentiment classification .GCN is a simple and effective convolutional neural network operating on graphs , which can catch inter-dependent information from rich relational data .For every node in graph , GCN encodes relevant information about its neighborhoods as a new feature representation vector .In our case , an aspect is treated as a node , and an edge represents the sentiment dependency relation of two nodes .Our model learns the sentiment dependencies of aspects via this graph structure .As far as we know , our work is the first to consider the sentiment dependencies between aspects in one sentence for aspect - level sentiment classification task .Furthermore , in order to capture the aspect - specific representations , our model applies bidirectional attention mechanism with position encoding before GCN .In our implementation , we respectively use the GloVe 3 word vector and the pre-trained language model word representation BERT 4 to initialize the word embeddings .The dimension of each word vector is 300 for GloVe and 768 for BERT .The number of LSTM hidden units is set to 300 , and the output dimension of GCN layer is set to 600 .The weight matrix of last fully connect layer is randomly initialized by a normal distribution N ( 0 , 1 ) .Besides the last fully connect layer , all the weight matrices are randomly initialized by a uniform distribution U ( ? 0.01 , 0.01 ) .In addition , we add L2-regularization to the last fully connect layer with a weight of 0.01 .During training , we set dropout to 0.5 , the batch size is set to 32 and the optimizer is Adam Optimizer with a learning rate of 0.001 .We implement our proposed model using Tensorflow 5 .TD - LSTM constructs aspect-specific representation by the left context with aspect and the right context with aspect , then employs two LSTMs to model them respectively .The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect .ATAE - LSTM first attaches the aspect embedding to each word embedding to capture aspect - dependent information , and then employs attention mechanism to get the sentence representation for final classification .Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .IAN generates the representations for aspect terms and contexts with two attention - based LSTM network separately .RAM [ 10 ] employs a gated recurrent unit network to model a multiple attention mechanism , and captures the relevance between each context word and the aspect .PBAN appends the position embedding into each word embedding .TSN is a two - stage framework for aspect - level sentiment analysis .AEN mainly consists of an embedding layer , an attentional encoder layer , an aspect - specific attention layer , and an output layer .AEN - BERT is AEN with BERT embedding .Among all the GloVe - based methods , the TD - LSTM approach performs worst because it takes the aspect information into consideration in a very coarse way .After taking the importance of the aspect into account with attention mechanism , they achieve a stable improvement comparing to the TD - LSTM .RAM achieves a better performance than other basic attention - based models , because it combines multiple attentions with a recurrent neural network to capture aspect - specific representations .PBAN achieves a similar performance as RAM by employing a position embedding .To be specific , PBAN is better than RAM on Restaurant dataset , but worse than RAN on Laptop dataset .Compared with RAM and PBAN , the over all performance of TSN is not perform well on both Restaurant dataset and Laptop dataset , which might because the framework of TSN is too simple to model the representations of context and aspect effectively .AEN is slightly better than TSN , but still worse than RAM and PBAN .Comparing the results of SDGCN - A w/o position and SDGCN - G w/o position , SDGCN - A and SDGCN - G , respectively , we observe that the GCN built with global - relation is slightly higher than built with adjacent - relation in both accuracy and Macro - F1 measure .Moreover , the two models ( SDGCN - A and SDGCN - G ) with position information gain a significant improvement compared to the two models without position information .Benefits from the power of pre-trained BERT , BERT - based models have shown huge superiority over GloVe - based models .Furthermore , compared with AEN - BERT , on the Restaurant dataset , SDGCN - BERT achieves absolute increases of 1.09 % and 1.86 % in accuracy and Macro - F1 measure respectively , and gains absolute increases of 1.42 % and 2.03 % in accuracy and Macro - F1 measure respectively on the Laptop dataset .\n",
            "An Interactive Multi - Task Learning Network for End - to - End Aspect - Based Sentiment AnalysisAspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .IMN introduces a novel message passing mechanism that allows informative interactions between tasks .Specifically , it sends useful information from different tasks back to a shared latent representation .The information is then combined with the shared latent representation and made available to all tasks for further processing .In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .In addition , IMN allows fined - grained tokenlevel classification tasks to be trained together with document - level classification tasks .We incorporated two document - level classification tasks - sentiment classification ( DS ) and domain classification ( DD ) - to be jointly trained with AE and AS , allowing the aspect - level tasks to benefit from document - level information .We adopt the multi - layer - CNN structure from as the CNN - based encoders in our proposed network .For word embedding initialization , we concatenate a general - purpose embedding matrix and a domain - specific embedding matrix 7 following .We adopt their released domainspecific embeddings for restaurant and laptop domains with 100 dimensions , which are trained on a large domain - specific corpus using fast Text .The general - purpose embeddings are pre-trained Glove vectors with 300 dimensions .We tune the maximum number of iterations T in the message passing mechanism by training IMN ?d via cross validation on D1 .It is set to 2 .We use Adam optimizer with learning rate set to 10 ? 4 , and we set batch size to 32 .Learning rate and batch size are set to conventional values without specific tuning for our task .At training phase , we randomly sample 20 % of the training data from the aspect - level dataset as the development set and only use the remaining 80 % for training .From , we observe that IMN ?d is able to significantly outperform other baselines on F1 - I .IMN further boosts the performance and outperforms the best F1 - I results from the baselines by 2.29 % , 1.77 % , and 2.61 % on D1 , D2 , and D3 .Specifically , for AE ( F1 - a and F1 - o ) , IMN ?d performs the best in most cases .For AS ( acc - s and F1 - s ) , IMN outperforms other methods by large margins .IMN wo DE performs only marginally below IMN .IMN ?d is more affected without domain - specific embeddings , while it still outperforms all other baselines except DECNN - d Trans .DECNN - dTrans is a very strong baseline as it exploits additional knowledge from larger corpora for both tasks .IMN ?d wo DE is competitive with DECNN - dTrans even without utilizing additional knowledge , which suggests the effectiveness of the proposed network structure .We observe that + Message passing - a and + Message passing - d contribute to the performance gains the most , which demonstrates the effectiveness of the proposed message passing mechanism .We also observe that simply adding documentlevel tasks ( + DS / DD ) with parameter sharing only marginally improves the performance of IMN ?d .However , + Message passing -d is still helpful with considerable performance gains , showing that aspect - level tasks can benefit from knowing predictions of the relevant document - level tasks .\n",
            "Aspect Based Sentiment Analysis with Gated Convolutional NetworksAspect based sentiment analysis ( ABSA ) can provide more detailed information than general sentiment analysis , because it aims to predict the sentiment polarities of the given aspects or entities in text .We summarize previous approaches into two subtasks : aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .A number of models have been developed for ABSA , but there are two different subtasks , namely aspect - category sentiment analysis ( ACSA ) and aspect - term sentiment analysis ( ATSA ) .The goal of ACSA is to predict the sentiment polarity with regard to the given aspect , which is one of a few predefined categories .On the other hand , the goal of ATSA is to identify the sentiment polarity concerning the target entities that appear in the text instead , which could be a multi-word phrase or a single word .In this paper , we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms , which has much less training time than LSTM based networks , but with better accuracy .For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .For ATSA task , where the aspect terms consist of multiple words , we extend our model to include another convolutional layer for the target expressions .In our experiments , word embedding vectors are initialized with 300 - dimension GloVe vectors which are pre-trained on unlabeled data of 840 billion tokens .Words out of the vocabulary of Glo Ve are randomly initialized with a uniform distribution U ( ? 0.25 , 0.25 ) .We use Adagrad with a batch size of 32 instances , default learning rate of 1 e ? 2 , and maximal epochs of 30 .We only fine tune early stopping with 5 - fold cross validation on training datasets .All neural models are implemented in PyTorch .NRC - Canada is the top method in SemEval 2014 Task 4 for ACSA and ATSA task .CNN is widely used on text classification task .TD - LSTM uses two LSTM networks to model the preceding and following contexts of the target to generate target - dependent representation for sentiment prediction .ATAE - LSTM is an attention - based LSTM for ACSA task .IAN stands for interactive attention network for ATSA task , which is also based on LSTM and attention mechanisms .RAM is a recurrent attention network for ATSA task , which uses LSTM and multiple attention mechanisms .GCN stands for gated convolutional neural network , in which GTRU does not have the aspect embedding as an additional input .LSTM based model ATAE - LSTM has the worst performance of all neural networks .GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .Without the large amount of sentiment lexicons , SVM perform worse than neural methods .With multiple sentiment lexicons , the performance is increased by 7.6 % .GCAE achieves 4 % higher accuracy than ATAE - LSTM on Restaurant - Large and 5 % higher on SemEval - 2014 on ACSA task .However , GCN , which does not have aspect modeling part , has higher score than GCAE on the original restaurant dataset .ATSAIAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .RAM also achieves good accuracy by combining multiple attentions with a recurrent neural network , but it needs more training time as shown in the following section .On the hard test dataset , GCAE has 1 % higher accuracy than RAM on restaurant data and 1.7 % higher on laptop data .Because of the gating mechanisms and the convolutional layer over aspect terms , GCAE outperforms other neural models and basic SVM .\n",
            "Effective Attention Modeling for Aspect - Level Sentiment ClassificationAspect - level sentiment classification is an important task in fine - grained sentiment analysis .We propose two novel approaches for improving the effectiveness of attention models .The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .To address this problem , inspired by , we instead model each target as a mixture of K aspect embeddings where we would like each embedded aspect to represent a cluster of closely related targets .We use an autoencoder structure to learn both the aspect embeddings as well as the representation of the target as a weighted combination of the aspect embeddings .The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .Our second approach exploits syntactic information to construct a syntax - based attention model .Instead , our syntax - based attention mechanism selectively focuses on a small subset of context words that are close to the target on the syntactic path which is obtained by applying a dependency parser on the review sentence .( 1 ) Feature - based SVM :We compare with the reported results of a top system in SemEval 2014 .( 2 ) LSTM : An LSTM network is built on top of word embeddings .1 ) Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .2 ) Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .3 ) Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .4 ) The integrated full model over all achieves the best performance compared to using only one of the two proposed approaches .5 ) The proposed target representation is more helpful on restaurant domain ( D1 , D3 , and D4 ) than laptop domain ( D2 ) .\n",
            "Target - Sensitive Memory Networks for Aspect Sentiment ClassificationAspect sentiment classification ( ASC ) is a fundamental task in sentiment analysis .However , we found an important problem with the current MNs in performing the ASC task .To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .AMN : A state - of - the - art memory network used for ASC .BL - MN : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .AE - LSTM : RNN / LSTM is another popular attention based neural model .Here we compare with a state - of - the - art attention - based LSTM for ASC , AE - LSTM .ATAE - LSTM :Another attention - based LSTM for ASC reported in .Target - sensitive Memory Networks ( TMNs ) :The six proposed techniques , NP , CNP , IT , CI , JCI , and JPI give six target - sensitive memory networks .We use the open - domain word embeddings 1 for the initialization of word vectors .We initialize other model parameters from a uniform distribution U ( - 0.05 , 0.05 ) .The dimension of the word embedding and the size of the hidden layers are 300 .The learning rate is set to 0.01 and the dropout rate is set to 0.1 .Stochastic gradient descent is used as our optimizer .We also compare the memory networks in their multiple computational layers version ( i.e. , multiple hops ) and the number of hops is set to 3 as used in the mentioned previous studies .We implemented all models in the TensorFlow environment using same input , embedding size , dropout rate , optimizer , etc.Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .2 . In the 3 - hop setting , TMNs achieve much better results on Restaurant .JCI , IT , and CI achieve the best scores , outperforming the strongest baseline AMN by 2.38 % , 2.18 % , and 2.03 % .On Laptop , BL - MN and most TMNs ( except CNP and JPI ) perform similarly .3 . Comparing all TMNs , we see that JCI works the best as it always obtains the top - three scores on two datasets and in two settings .CI and JPI also perform well in most cases .IT , NP , and CNP can achieve very good scores in some cases but are less stable .\n",
            "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue VideosEmotion recognition in conversations is crucial for the development of empathetic machines .Emotion detection from such resources can benefit numerous fields like counseling , public opinion mining , financial forecasting , and intelligent systems such as smart homes and chatbots .In this paper , we analyze emotion detection in videos of dyadic conversations .We propose a conversational memory network ( CMN ) , which uses a multimodal approach for emotion detection in utterances ( a unit of speech bound by breathes or pauses ) of such conversational videos .Our proposed CMN incorporates these factors by using emotional context information present in the conversation history .It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .CMN also models interplay of these memories to capture interspeaker dependencies .CMN first extracts multimodal features ( audio , visual , and text ) for all utterances in a video .We use 10 % of the training set as a held - out validation set for hyperparameter tuning .To optimize the parameters , we use Stochastic Gradient Descent ( SGD ) optimizer , starting with an initial learning Utterances whose history has atleast 3 similar emotion labels in either own history or the history of the other person , is counted in case 1 or 2 , respectively .An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .Gradient clipping is used for regularization with a norm set to 40 .Hyperparameters are decided using a Random Search .Based on validation performance , context window length K is set to be 40 and the number of hops R is fixed at 3 hops .The dimension size of the memory cells d is set as 50 .SVM - ensemble :A strong context - free benchmark model which uses similar multimodal approach on an ensemble of trees .bc - LSTM :A bi-directional LSTM equipped with hierarchical fusion , proposed by .Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .[ 1 , K ] } for ? ? {a , b}. CMN Self :In this baseline , we use only self history for classifying emotion of utterance u i .CMN N A : Single layer variant of the CMN with no attention module .This suggests that gathering contexts temporally through sequential processing is indeed a superior method over non-temporal memory representations .CMN self which uses only single history channel also provides lesser performance when compared to CMN .Overall , predictions on valence and arousal levels also show similar results which reinforce our hypothesis of CMN 's ability to model emotional dynamics .\n",
            "Progressive Self - Supervised Attention Learning for Aspect - Level Sentiment AnalysisIn aspect - level sentiment classification ( ASC ) , it is prevalent to equip dominant neural models with attention mechanisms , for the sake of acquiring the importance of each context word on the given aspect .In this paper , we propose a progressive self - supervised attention learning approach for neural ASC models , which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms .Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .However , the existing attention mechanism in ASC suffers from a major drawback .In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .We used pre-trained Glo Ve vectors to initialize the word embeddings with vector dimension 300 .For out - of - vocabulary words , we randomly sampled their embeddings from the uniform distribution , as implemented in .To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .Adam ( Kingma and Ba , 2015 ) was adopted as the optimizer with the learning rate 0.001 .When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .All hyper - parameters were tuned on 20 % randomly held - out training data .First , both of our reimplemented MN and TNet are comparable to their original models reported in .When we replace the CNN of TNet with an attention mechanism , TNet - ATT is slightly inferior to TNet .Moreover , when we perform additional K+1 - iteration of training on these models , their performance has not changed significantly , suggesting simply increasing training time is unable to enhance the performance of the neural ASC models .Finally , when we use both kinds of attention supervision information , no matter for which metric , MN ( + AS ) remarkably outperforms MN on all test sets .\n",
            "DialogueRNN : An Attentive RNN for Emotion Detection in ConversationsOur proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .The global GRU encodes corresponding party information while encoding an utterance .Attending over this GRU gives contextual representation that has information of all preceding utterances by different parties in the conversation .The speaker state depends on this context through attention and the speaker 's previous state .This ensures that at time t , the speaker state directly gets information from the speaker 's previous state and global GRU which has information on the preceding parties .Finally , the updated speaker state is fed into the emotion GRU to decode the emotion representation of the given utterance , which is used for emotion classification .At time t , the emotion GRU cell gets the emotion representation of t ? 1 and the speaker state of t .c - LSTM : Biredectional LSTM is used to capture the context from the surrounding utterances to generate contextaware utterance representation .c- LSTM+ Att :In this variant attention is applied applied to the c - LSTM output at each timestamp by following Eqs. and .TFN :This is specific to multimodal scenario .Tensor outer product is used to capture intermodality and intra-modality interactions .MFN ) : Specific to multimodal scenario , this model utilizes multi-view learning by modeling view - specific and cross - view interactions .CNN : This is identical to our textual feature extractor network ( Section 3.2 ) and it does not use contextual information from the surrounding utterances .Memnet : As described in , the current utterance is fed to a memory network , where the memories correspond to preceding utterances .The output from the memory network is used as the final utterance representation for emotion classification .CMN : This state - of - the - art method models utterance context from dialogue history using two distinct GRUs for two speakers .As expected , on average Di - alogue RNN outperforms all the baseline methods , including the state - of - the - art CMN , on both of the datasets .As evidenced by , for IEMOCAP dataset , our model surpasses the state - of - the - art method CMN by 2.77 % accuracy and 3.76 % f 1 - score on average .AVEC DialogueRNN outperforms CMN for valence , arousal , expectancy , and power attributes ; see .DialogueRNN vs. DialogueRNN VariantsDialogueRNN l :Following , using explicit listener state update yields slightly worse performance than regular DialogueRNN .BiDialogueRNN : Since BiDialogueRNN captures context from the future utterances , we expect improved performance from it over DialogueRNN .This is confirmed in , where BiDialogueRNN outperforms Dialogue RNN on average on both datasets .\n",
            "Emo2 Vec : Learning Generalized Emotion Representation by Multi- task TrainingThis work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment - related tasks compared to other word embeddings .1 ) We propose Emo2Vec 1 which are word - level representations that encode emotional semantics into fixed - sized , real - valued vectors .2 ) We propose to learn Emo2Vec with a multi-task learning framework by including six different emotion - related tasks .Pre-training Emo2VecEmo2 Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone .Parameters of T and CNN are randomly initialized and Adam is used for optimization .For the best model , we use the batch size of 16 , embedding size of 100 , 1024 filters and filter sizes are 1 , 3 ,5 and 7 respectively .Multi - task trainingWe tune our parameters of learning rate , L2 regularization , whether to pre-train our model and batch size with the average accuracy of the development set of all datasets .We early stop our model when the averaged dev accuracy stop increasing .Our best model uses learning rate of 0.001 , L2 regularization of 1.0 , batch size of 32 .We save the best model and take the embedding layer as Emo2Vec vectors .Compared with CNN embedding : Emo2 Vec works better than CNN embedding on 14 / 18 datasets , giving 2.6 % absolute accuracy improvement for the sentiment task and 1.6 % absolute f1score improvement on the other tasks .It shows multi-task training helps to create better generalized word emotion representations than just using a single task .Compared with SSWE : Emo2 Vec works much better on all datasets except SS - T datasets , which gives 3.3 % accuracy improvement and 4.7 % f 1 score improvement respectively on sentiment and other tasks .On average , it gives 1.3 % improvement in accuracy for the sentiment task and 1.1 % improvement of f 1 - score on the other tasks .Since Emo2 Vec is not trained by predicting contextual words , it is weak on capturing synthetic and semantic meaning .Here , we want to highlight that solely using a simple classifier with good word representation can achieve promising results .Compared with GloVe+ DeepMoji , GloVe + Emo2 Vec achieves same or better results on 11 / 14 datasets , which on average gives 1.0 % improvement .GloVe + Emo2 Vec achieves better performances on SOTA results on three datasets ( SE0714 , stress and tube tablet ) and comparable result to SOTA on dataset Previous SOTA resultsThus , to detect the corresponding emotion , more attention needs to be paid to words .\n",
            "EEG - Based Emotion Recognition Using Regularized Graph Neural NetworksE MOTION recognition is an important subarea of affective computing , which focuses on recognizing human emotions based on a variety of modalities , such as audio- visual expressions , body language , physiological signals , etc .In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .Inspired by , , we consider each channel in EEG signals as a node in our graph .Our RGNN model extends the simple graph convolution network ( SGC ) and leverages the topological structure of EEG signals , i.e. , according to the economy of brain network organization , we propose a biologically supported sparse adjacency matrix to capture both local and global inter-channel relations .Local interchannel relations connect nearby groups of neurons and may reveal anatomical connectivity at macroscale , .Global inter-channel relations connect distant groups of neurons between the left and right hemispheres and may reveal emotion - related functional connectivity , .For our RGNN in all experiments , we empirically set the number of convolutional layers L = 2 , dropout rate of 0.7 at the output fully - connected layer , and batch size of 16 .We use Adam optimization with default values , i.e. , ? 1 = 0.9 and ? 2 = 0.999 .We only tune the output feature dimension d , label noise level , learning rate ? , L1 regularization factor ? , and L2 regularization for each experiment .It is encouraging to see that our model achieves superior performance on both datasets as compared to all baselines including the stateof - the - art BiHDM when DE features from all frequency bands are used .It is worth noting that our model improves the accuracy of the state - of - the - art model on SEED - IV by around 5 % .In particular , our model performs better than DGCNN , which is another GNN - based model that leverages the topological structure in EEG signals .In subject - dependent experiments on SEED , STRNN achieves the highest accuracy in delta , theta and alpha bands , BiDANN performs best in beta band , and our model performs best in gamma band .In subject - independent experiments on SEED , BiDANN - S achieves the highest accuracy in theta and alpha bands , and our model performs best in delta , beta and gamma bands .For both subject - dependent and subjectindependent settings on SEED , we compare the performance of each model across different frequency bands .In general , most models including our model achieve better performance on beta and gamma bands than delta , theta and alpha bands , with one exception of STRNN , which performs the worst on gamma band .One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band , whereas other models perform comparably in both bands , indicating that gamma band maybe the most discriminative band for our model .The two major designs in our adjacency matrix A , i.e. , global connection and symmetric adjacency matrix designs , are helpful in recognizing emotions .The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .In addition , if NodeDAT is removed , the performance of our model has a greater variance , demonstrating the importance of NodeDAT in improving the robustness of our model against cross - subject variations .DL regularizer improves performance of our model by around 3 % in accuracy on both datasets .\n",
            "Improved Semantic Representations From Tree - Structured Long Short - Term Memory NetworksTree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .In this paper , we introduce a generalization of the standard LSTM architecture to tree - structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM .While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step , the tree - structured LSTM , or Tree - LSTM , composes its state from an input vector and the hidden states of arbitrarily many child units .Implementations of our models and experiments are available at https :// github.com/stanfordnlp/treelstm.The hyperparameters for our models were tuned on the development set for each task .We initialized our word representations using publicly available 300 - dimensional Glove vectorsFor the sentiment classification task , word representations were updated during training with a learning rate of 0.1 .For the semantic relatedness task , word representations were held fixed as we did not observe any significant improvement when the representations were tuned .Our models were trained using AdaGrad with a learning rate of 0.05 and a minibatch size of 25 .The model parameters were regularized with a per-minibatch L2 regularization strength of 10 ?4 .The sentiment classifier was additionally regularized using dropout with a dropout rate of 0.5 .\n",
            "BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment AnalysisTo show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .This work adopts BERT ) as the base model as it achieves the state - of the - art performance on MRC .To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre-trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC .This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness .We adopt BERT BASE ( uncased ) as the basis for all experiments10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .The maximum length of post -training is set to 320 with a batch size of 16 for each type of knowledge .The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G .We use Adam optimizer and set the learning rate to be 3e - 5 .We train 70,000 steps for the laptop domain and 140,000 steps for the restaurant domain , which roughly have one pass over the preprocessed data on the respective domain .To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge .Rest. Methods EM F1 EM F1 DrQA 38.26 50.99 49.52 63.73 DrQA+MRC 40 To answer RQ2 , to our surprise we found that the vanilla pre-trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks .To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains .For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post -training ( as indicated by BERT - MRC ) .We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .For ASC , we observed that large - scale annotated MRC data is very useful .The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question .For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) .ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) .\n",
            "Recursive Deep Models for Semantic Compositionality Over a Sentiment TreebankFurther progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .The corpus is based on the dataset introduced by and consists of 11,855 single sentences extracted from movie reviews .It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees , each annotated by 3 human judges .This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena .The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques .In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .Recursive Neural Tensor Networks take as input phrases of any length .They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30 .The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours .We use f = tanh in all experiments .We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs , as well as Naive Bayes with bag of bigram features .We also compare to a model that averages neural word vectors and ignores word order ( VecAvg ) .The sentences in the treebank were split into a train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 ) and these splits are made available with the data release .showed that a fine grained classification into 5 classes is a reasonable approximation to capture most of the data variation .The RNTN gets the highest performance , followed by the MV - RNN and RNN .The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .The combination of the new sentiment treebank and the RNTN pushes the state of the art on short phrases up to 85.4 % .The RNTN has the highest reversal accuracy , showing its ability to structurally learn negation of positive sentences .shows a typical case in which sentiment was made more positive by switching the main class from negative to neutral even though both not and dull were negative .Therefore we can conclude that the RNTN is best able to identify the effect of negations upon both positive and negative sentiment sentences . :\n",
            "MULTIMODAL SPEECH EMOTION RECOGNITION USING AUDIO AND TEXTTo overcome these limitations , we propose a model that uses high - level text transcription , as well as low - level audio signals , to utilize the information contained within low - resource datasets to a greater degree .In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .Among the variants of the RNN function , we use GRUs as they yield comparable performance to that of the LSTM and include a smaller number of weight parameters .We use a max encoder step of 750 for the audio input , based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts .The vocabulary size of the dataset is 3,747 , including the \" UNK \" token , which represents unknown words , and the \" PAD \" token , which is used to indicate padding information added while preparing mini-batch data .The number of hidden units and the number of layers in the RNN for each model ( ARE , TRE , MDRE and MDREA ) are selected based on extensive hyperparameter search experiments .The weights of the hidden units are initialized using orthogonal weights ] , and the text embedding layer is initialized from pretrained word - embedding vectors .In preparing the textual dataset , we first use the released transcripts of the IEMOCAP dataset for simplicity .First , our ARE model shows the baseline performance because we use minimal audio features , such as the MFCC and prosodic features with simple architectures .On the other hand , the TRE model shows higher performance gain compared to the ARE .From this result , we note that textual data are informative in emotion prediction tasks , and the recurrent encoder model is effective in understanding these types of sequential data .Second , the newly proposed model , MDRE , shows a substantial performance gain .It thus achieves the state - of - the - art performance with a WAP value of 0.718 .Lastly , the attention model , MDREA , also outperforms the best existing research results ( WAP 0.690 to 0.688 ) .The label accuracy of the processed transcripts is 5.53 % WER .The TRE - ASR , MDRE - ASR and MDREA - ASR models reflect degraded performance compared to that of the TRE , MDRE and MDREA models .The ARE model ( ) incorrectly classifies most instances of happy as neutral ( 43.51 % ) ; thus , it shows reduced accuracy ( 35.15 % ) in predicting the the happy class .Overall , most of the emotion classes are frequently confused with the neutral class .Interestingly , the TRE model ( ) shows greater prediction gains in predicting the happy class when compared to the ARE model ( 35.15 % to 75. 73 % ) .The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .Furthermore , the occurrence of the incorrect \" sad - to - happy \" cases in the TRE model is reduced from 16 . 20 % to 9.15 % .\n",
            "We present CATENA , a sieve - based system to perform temporal and causal relation extraction and classification from English texts , exploiting the interaction between the temporal and the causal model .The CATENA system includes two main classification modules , one for temporal and the other for causal relations between events .As shown in , they both take as input a document annotated with the so - called temporal entities according to TimeML guidelines , including the document creation time ( DCT ) , events and time expressions ( timexes ) .The output is the same document with temporal links ( TLINKs ) set between pairs of temporal entities , each assigned to one of the TimeML temporal relation types , such as or SIMULTANEOUS , which denotes the temporal ordering .The document is also annotated with causal relations ( CLINKs ) between event pairs .The modules for temporal and causal relation classification rely both on a sieve - based architecture , in which the remaining unlabelled pairs - after running a rule - based component and / or a transitive reasoner are fed into a supervised classifier .The evaluation shows that CATENA is the best performing system in both tasks , even if in Task C best precision and best recall are yielded by and , respectively .If we consider the different entity pairs , CATENA performs best on timex - timex and event - timex relations , while CAEVO still achieves the best results on event - DCT and event - event pairs .As expected , running a transitive closure module after the temporal rule - based sieve ( RB + TR ) results in improving recall , but the over all performance is still lacking ( less than .30 F1-score ) .Combining rule - based and machine - learned sieves ( RB + ML ) yields a slight improvement compared with enabling only the machine - learned sieve in the system ( ML ) .Introducing the temporal reasoner module between the two sieves ( RB + TR + ML ) proves to be even more beneficial .\n",
            "A Structured Learning Approach to Temporal Relation ExtractionIdentifying temporal relations between events is an essential step towards natural language understanding .The fundamental tasks in temporal processing , as identified in the TE workshops , are 1 ) time expression ( the so - called \" timex \" ) extraction and normalization and 2 ) temporal relation ( also known as TLINKs ) extraction .In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .The first is the regularized averaged perceptron ( AP ) implemented in the LBJava package and is a local method .On top of the first baseline , we performed global inference in Eq .Both of them used the same feature set ( i.e. , as designed in ) as in the proposed structured perceptron ( SP ) and CoDL for fair comparisons .TE3 Task C - Relation OnlyWe can see that UT - Time is about 3 % better than AP - 1 in the absolute value of F 1 , which is expected since UTTime included more advanced features derived from syntactic parse trees .On top of AP - 2 , a global inference step enforcing symmetry and transitivity constraints ( \" AP + ILP \" ) can further improve the F 1 score by 9.3 % , which is consistent with previous observations .SP + ILP further improved the performance in precision , recall , and F 1 significantly ( per the McNemar 's test with p < 0.0005 ) , reaching an F 1 score of 67.2 % .TE3 Task CThe improvement of SP + ILP ( line 4 ) over AP ( line 2 ) was small and AP + ILP ( line 3 ) was even worse than AP , which necessitates the use of a better approach towards vague TLINKs .By applying the postfiltering method proposed in Sec. 4 , we were able to achieve better performances using SP + ILP ( line 5 ) , which shows the effectiveness of this strategy .Comparison with CAEVOSP + ILP outperformed CAEVO and if additional unlabeled dataset TE3 - SV was used , CoDL + ILP achieved the best score with a relative improvement in F 1 score being 6.3 % .\n",
            "Transfer Learning from Speaker Verification to Multispeaker Text - To - Speech SynthesisWe describe a neural network - based system for text - to - speech ( TTS ) synthesis that is able to generate speech audio in the voice of different speakers , including those unseen during training .The goal of this work is to build a TTS system which can generate natural speech for a variety of speakers in a data efficient manner .Our approach is to decouple speaker modeling from speech synthesis by independently training a speaker - discriminative embedding network that captures the space of speaker characteristics and training a high quality TTS model on a smaller dataset conditioned on the representation learned by the first network .We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .Speech naturalnessThe proposed model achieved about 4.0 MOS in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .Most importantly , the audio generated by our model for unseen speakers is deemed to be at least as natural as that generated for seen speakers .Surprisingly , the MOS on unseen speakers is higher than that of seen speakers , by as much as 0.2 points on LibriSpeech .Speaker similarityThe scores for the VCTK model tend to be higher than those for LibriSpeech , reflecting the cleaner nature of the dataset .For seen speakers on VCTK , the proposed model performs about as well as the baseline which uses an embedding lookup table for speaker conditioning .Speaker verificationAs shown in , as long as the synthesizer was trained on a sufficiently large set of speakers , i.e. on LibriSpeech , the synthesized speech is typically most similar to the ground truth voices .On this 20 voice discrimination task we obtain an EER of 2.86 % , demonstrating that , while the synthetic speech tends to be close to the target speaker ( cosine similarity > 0.6 , and as in ) , it is nearly always even closer to other synthetic utterances for the same speaker ( similarity > 0.7 ) .Speaker embedding spaceThe PCA visualization ( left ) shows that synthesized utterances tend to lie very close to real speech from the same speaker in the embedding space .However , synthetic utterances are still easily distinguishable from the real human speech as demonstrated by the t - SNE visualization ( right ) where utterances from each synthetic speaker form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker .Number of speaker encoder training speakersAs the number of training speakers increases , both naturalness and similarity improve significantly .Fictitious speakersBypassing the speaker encoder network and conditioning the synthesizer on random points in the speaker embedding space results in speech from fictitious speakers which are not present in the train or test sets of either the synthesizer or the speaker encoder .\n",
            "Token - Level Ensemble Distillation for Grapheme - to - Phoneme ConversionGrapheme - to - phoneme ( G2P ) conversion is an important task in automatic speech recognition and text - to - speech systems .Inspired by the knowledge distillation in computer vision and natural language processing , in this work , we propose the token - level ensemble distillation for G2P conversion , to address the practical problems mentioned above .First , we use knowledge distillation to leverage the large amount of unlabeled words .Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .Second , we train a variety of models ( CNN , RNN and Transformer ) for ensemble to get higher accuracy , and transfer the knowledge of the ensemble models to a light - weight model that is suitable for online deployment , again by knowledge distillation .Besides , we adopt Transformer instead of RNN or CNN as the basic encoder - decoder model structure , since it demonstrates advantages in a variety of sequence to sequence tasks , such as neural machine translation , text summarization , automatic speech recognition .Ensemble ModelWe use 4 Transformer models , 3 CNN models and 3 Bi - LSTM models with different hyperparameters for ensemble , which give the best performance on the validation set .The 4 Transformer models share the same hidden size ( 256 ) but vary in the number of the encoder - decoder layers .For the 3 CNN models , they share the same hidden size ( 256 ) but vary in the number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ) respectively .For the 3 Bi - LSTM models , they share the same number of encoder - decoder layers ( 1 - 1 ) , but with different hidden sizes ( 256 , 384 and 512 ) .Student ModelWe choose Transformer as the student model and use the default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ) unless otherwise stated .We implement experiments with the fairseq - py 4 library in Py-Torch .We use Adam optimizer for all models and follow the learning rate schedule in .The dropout is 0.3 for Bi - LSTM and CNN models , while the residual dropout , attention dropout and ReLU dropout for Transformer models is 0.2 , 0.4 , 0.4 respectively .We train each model on 8 NVIDIA M40 GPUs .Each GPU contains roughly 4000 tokens in one mini-batch .We use beam search during inference and set beam size to 10 .We use WER ( word error rate ) and PER ( phoneme error rate ) to measure the accuracy of G2P conversion .We first compare our method with previous works on CMUDict 0.7 b dataset , as shown in .It can be seen that our method on 6 - layer encoder and 6 - layer decoder Transformer achieves the new state - of - the - art result of 19.88 % WER , outperforming NSGD by 4.22 % WER .We first study the effect of distilling from unlabeled source words , as shown in .It can be seen that unlabeled source words can boost the accuracy by nearly 1 % WER , demonstrating the effectiveness by introducing abundant unlabeled data into knowledge distillation .Furthermore , we study the effect of ensemble teacher model in knowledge distillation .As shown in , the ensemble teacher model can boost the accuracy by more than 1 % WER , compared with the single teacher model ( a Transformer model with 6 - layer encoder and 6 - layer decoder ) , which demonstrates the strong ensemble teacher model is essential to guarantee the performance of student model in knowledge distillation .At last , we compare Transformer with RNN and CNN based models , without using knowledge distillation and unlabeled data , as shown in .We can see that Transformer model outperforms the RNN and CNN based models used in previous works , demonstrating the advantage of Transformer model .We compare our method with the previous state - of - the - art CNN with NSGD ( which is reproduced by ourself ) on our internal dataset , as shown in .Our method outperforms CNN with NSGD by 3.52 % WER , which demonstrates the effectiveness of our method for G2P conversion .\n",
            "FastSpeech : Fast , Robust and Controllable Text to SpeechNeural network based end - to - end text to speech ( TTS ) has significantly improved the quality of synthesized speech .Text to speech ( TTS ) has attracted a lot of attention in recent years due to the advance in deep learning .Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .Considering the monotonous alignment between text and speech , to speedup mel- spectrogram generation , in this work , we propose a novel model , FastSpeech , which takes a text ( phoneme ) sequence as input and generates mel-spectrograms non-autoregressively .It adopts a feed - forward network based on the self - attention in Transformer and 1D convolution .Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .We first train the autoregressive Transformer TTS model on 4 NVIDIA V100 GPUs , with batchsize of 16 sentences on each GPU .We use the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 , ? = 10 ?9 and follow the same learning rate schedule in .In addition , we also leverage sequence - level knowledge distillation that has achieved good performance in non-autoregressive machine translation to transfer the knowledge from the teacher model to the student model .In the inference process , the output mel-spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow [ 20 ] 5 .Audio QualityWe conduct the MOS ( mean opinion score ) evaluation on the test set to measure the audio quality .RobustnessIt can be seen that Transformer TTS is not robust to these hard cases and gets 34 % error rate , while FastSpeech can effectively eliminate word repeating and skipping to improve intelligibility .Voice SpeedAs demonstrated by the samples , FastSpeech can adjust the voice speed from 0.5x to 1.5 x smoothly , with stable and almost unchanged pitch .1D Convolution in FFT BlockWe propose to replace the original fully connected layer ( adopted in Transformer ) with 1D convolution in FFT block , as described in Section 3.1 .As shown in , replacing 1D convolution with fully connected layer results in - 0.113 CMOS , which demonstrates the effectiveness of 1D convolution .Sequence - Level Knowledge DistillationWe find that removing sequence - level knowledge distillation results in - 0.325 CMOS , which demonstrates the effectiveness of sequence - level knowledge distillation .\n",
            "Sentence Compression by Deletion with LSTMsWe present an LSTM approach to deletion - based sentence compression where the task is to translate a sentence into a sequence of zeros and ones , corresponding to token deletion decisions .Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence .In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .There is a significant difference in performance of the MIRA baseline and the LSTM models , both in terms of F1 - score and in accuracy .More than 30 % of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20 % of MIRA .The differences in F- score between the three versions of LSTM are not significant , all scores are close to 0.81 .\n",
            "Improving sentence compression by learning to predict gazeWe go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .Our proposed model does not require that the gaze data and the compression data come from the same source .Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .Both the baseline and our systems are three - layer bi - LSTM models trained for 30 iterations with pretrained ( SENNA ) embeddings .The input and hidden layers are 50 dimensions , and at the output layer we predict sequences of two labels , indicating whether to delete the labeled word or not .Our baseline ( BASELINE - LSTM ) is a multi - task learning 1 http://groups.inf.ed.ac.uk/ccg/bi -LSTM predicting both CCG supertags and sentence compression ( word deletion ) at the outer layer .We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .\n",
            "A Language Model based Evaluator for Sentence CompressionWe herein present a language - modelbased evaluator for deletion - based sentence compression , and viewed this task as a series of deletion - and - evaluation operations using the evaluator .To answer the above questions , a syntax - based neural language model is trained on large - scale datasets as a readability evaluator .The neural language model is supposed to learn the correct word collocations in terms of both syntax and semantics .Subsequently , we formulate the deletionbased sentence compression as a series of trialand - error deletion operations through a reinforcement learning framework .The policy network performs either RETAIN or REMOVE action to form a compression , and receives a reward ( e.g. , readability score ) to update the network .We choose several strong baselines ; the first one is the dependency - tree - based method that considers the sentence compression task as an optimization problem by using integer linear programming 5 .The second method is the long short - term memory networks ( LSTMs ) which showed strong promise in sentence compression by .The embedding size for word , part - of - speech tag , and the dependency relation is 128 .We employed the vanilla RNN with a hidden size of 512 for both the policy network and neural language model .The mini - batch size was chosen from [ 5 , 50 , 100 ] .Vocabulary size was 50,000 .The learning rate for neural language model is 2.5 e - 4 , and 1e - 05 for the policy network .For policy learning , we used the REINFORCE algorithm to update the parameters of the policy network and find an policy that maximizes the reward .6 https://github.com/code4conference/code4sc( 1 ) As shown in , our Evaluator - SLMbased method yields a large improvement over the baselines , demonstrating that the language - modelbased evaluator is effective as a post-hoc grammar checker for the compressed sentences .( 3 ) As for Google news dataset , LSTMs ( LSTM + pos+dep ) ( & 3 ) is a relatively strong baseline , suggesting that incorporating dependency relations and part - of - speech tags may help model learn the syntactic relations and thus make a better prediction .When further applying Evaluator - SLM , only a tiny improvement is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .For Gigaword dataset with 1.02 million instances , the perplexity of the language model is 20.3 , while for the Google news dataset with 0.2 million instances , the perplexity is 76.5 .The results shows that small improvements are observed on two datasets ( # 4 vs # 5 ; & 4 vs & 5 ) , suggesting that incorporating syntactic knowledge may help evaluator to encourage more unseen but reasonable word collocations .\n",
            "Can Syntax Help ? Improving an LSTM - based Sentence Compression Model for New DomainsTo this end , we extend the deletionbased LSTM model for sentence compression by .Specifically , we propose two major changes to the model by : We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model .( 2 ) Inspired by a previous method , we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences .In addition to the two major changes above , we also use bi-directional LSTM to include contextual information from both directions into the model .In the experiments , our model was trained using the Adam algorithm with a learning rate initialized at 0.001 .The dimension of the hidden layers of bi - LSTM is 100 .Word embeddings are initialized from GloVe 100 dimensional pre-trained embeddings .POS and dependency embeddings are randomly initialized with 40 - dimensional vectors .The embeddings are all updated during training .Dropping probability for dropout layers between stacked LSTM layers is 0.5 .The batch size is set as 30 .We utilize an open source ILP solver 4 in our method .LSTM : This is the basic LSTM - based deletion method proposed by .LSTM + : This is advanced version of the model proposed by , where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word .Traditional ILP :This is the ILP - based method proposed by .Abstractive seq2seq :This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .We can see that indeed this abstractive method performed poorly in cross - domain settings .( 2 ) In the in - domain setting , with the same amount of training data ( 8,000 ) , our BiLSTM method with syntactic features ( BiLSTM + SynFeat and BiL - STM + SynFeat + ILP ) performs similarly to or better than the LSTM + method proposed by , in terms of both F1 and accuracy .This shows that our method is comparable to the LSTM + method in the in - domain setting .( 4 ) In the out - of - domain setting , our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM + methods .( 5 ) The Traditional ILP method also works better than the LSTM and LSTM + methods in the out - of - domain setting .But the Traditional ILP method performs worse in the in - domain setting than both the LSTM and LSTM + methods and our methods .Therefore , our method works reasonably well for both in - domain and out - ofdomain data .We also notice that on Google News , adding the ILP layer decreased the sentence compression performance .We can see that in the in - domain setting , our method does not have any advantage over the LSTM + method .But in the cross - domain setting , our method that uses ILP to impose syntax - based constraints clearly performs better than LSTM + when the amount of training data is relatively small .\n",
            "Universal Sentence EncoderWe present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .We find that transfer learning using sentence embeddings tends to outperform word level transfer .With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .The encoding model is designed to be as general purpose as possible .This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .Deep Averaging Network ( DAN )The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .MR : Movie review snippet sentiment on a five star scale .CR : Sentiment of sentences mined from customer reviews .SUBJ : Subjectivity of sentences from movie reviews and plot summaries .MPQA : Phrase level opinion polarity from news data .TREC : Fine grained question classification sourced from TREC .SST : Binary phrase level sentiment classification .STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .\n",
            "Bag of Tricks for Efficient Text ClassificationThis paper explores a simple and efficient baseline for text classification .shows a simple linear model with rank constraint .The first weight matrix A is a look - up table over the words .The word representations are then averaged into a text representation , which is in turn fed to a linear classifier .We use the softmax function f to compute the probability distribution over the predefined classes .In order to improve our running time , we use a hierarchical softmax ) based on the Huffman coding tree .Instead , we use a bag of n-grams as additional features to capture some partial information about the local word order .This is very efficient in practice while achieving comparable results to methods that explicitly use the order .We maintain a fast and memory efficient mapping of the n-grams by using the hashing trick with the same hashing function as in and 10M bins if we only used bigrams , and 100M otherwise .Sentiment analysisWe use 10 hidden units and run fastText for 5 epochs with a learning rate selected on a validation set from { 0.05 , 0.1 , 0.25 , 0.5 } .On this task , adding bigram information improves the performance by 1 - 4 % .Overall our accuracy is slightly better than char - CNN and char - CRNN and , a bit worse than VDCNN .Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .Tag predictionAt test time , Tagspace needs to compute the scores for all the classes which makes it relatively slow , while our fast inference gives a significant speed - up when the number of classes is large ( more than 300 K here ) .Overall , we are more than an order of magnitude faster to obtain model with a better quality .\n",
            "Translations as Additional Contexts for Sentence ClassificationIn this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .In this paper , we propose a method to mitigate the possible problems when using translated sentences as context based on the following observations .Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .MCFA is a series of modules that uses all the sentence vectors ( e.g. Arabic , English , Korean , etc. ) as context to fix a sentence vector ( e.g. Korean ) .MCFA computes two sentence usability metrics to control the noise when fixing vectors : ( a ) self usability ? i ( a ) weighs the confidence of using sentence a in solving the task .( b ) relative usability ? r ( a , b ) weighs the confidence of using sentence a in fixing sentence b.( 1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .( 3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .Tokenization is done using the polyglot library 7 .We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .We use dropout on all nonlinear connections with a dropout rate of 0.5 .We also use an l 2 constraint of 3 , following for accurate comparisons .We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .During training , we use mini-batch size of 50 .Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .We perform early stopping using a random 10 % of the training set as the development set .We show that CNN + MCFA achieves state of the art performance on three of the four data sets and performs competitively on one data set .When N = 1 , MCFA increases the performance of a normal CNN from 85.0 to 87.6 , beating the current state of the art on the CR data set .When N = 10 , MCFA additionally beats the state of the art on the TREC data set .Finally , our ensemble classifier additionally outperforms all competing models on the MR data set .On all data sets except SUBJ , the accuracy of CNN + B1 decreases from the base CNN accuracy , while the accuracy of our model always improves from the base CNN accuracy .We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .Overall , we conclude that translations are better additional contexts than topics .When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .\n",
            "Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific AbstractsThis hampers the traditional sentence classification approaches to the problem of sequential sentence classification , where structured prediction is needed for better over all classification performance .In this work , we present a hierarchical neural network model for the sequential sentence classification task , which we call a hierarchical sequential labeling network ( HSLN ) .Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings , then uses another bi - LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation , subsequently uses a single - hidden - layer feed - forward network to transform the sentence representation to the probability vector , and finally optimizes the predicted label sequence jointly via a CRF layer .The token embeddings were pre-trained on a large corpus combining Wikipedia , PubMed , and PMC texts ( Moen and Ananiadou , 2013 ) using the word2vec tool 4 ( denoted as \" Word2vec- wiki+P.M. \" ) .They are fixed during the training phase to avoid over-fitting .The model is trained using the Adam optimization method ( Kingma and Ba , 2014 ) .The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch .For regularization , dropout ( Srivastava et al. , 2014 ) is applied to each layer .To reduce this gap , we adopted the dropout with expectation - linear regularization introduced by to explicitly control the inference gap and thus improve the generaliza - tion performance .Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in .The window sizes of the CNN encoder in the sentence encoding layer are 2 , 3 , 4 and 5 .As can be seen from , our HSLN - CNN model uni-formly suffers a little more from the component removal than the HSLN - RNN model , indicating that the HSLN - RNN model is more robust .When the context enriching layer is removed , both models experience the most significant performance drop and can only be on par with the previous stateof - the - art results , strongly demonstrating that this proposed component is the key to the performance improvement of our model .Furthermore , even without the label sequence optimization layer , our model still significantly outperforms the best published methods that are empowered by this layer , indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences .Last but not the least , the dropout regularization and attention - based pooling components we add to our system can help further improve the model in a limited extent . :\n",
            "Structural Scaffolds for Citation Intent Classification in Scientific PublicationsIdentifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .Our code and data are available at : https://github.com/ allenai/scicite .In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .Citation intent of sentence extractions was labeled through the crowdsourcing platform .Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .Each sentence was annotated , on average , 3.74 times .This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .We implement our proposed scaffold framework using the AllenNLP library .For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .We use Beaker 12 for running the experiments .BiLSTM Attention ( with and without ELMo ) .This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .Each scaffold task improves model performance .Adding both scaffolds results in further improvements .And the best results are obtained by using ELMo representation in addition to both scaffolds .Generally we observe that results on categories with more number of instances are higher .For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .Conversely , the results on the FUTUREWORK category are the lowest .This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .\n",
            "Relation Classification via Multi - Level Attention CNNsWe propose a novel CNN architecture that addresses some of the shortcomings of previous approaches .Our CNN architecture relies on a novel multi-level attention mechanism to capture both entity - specific attention ( primary attention at the input level , with respect to the target entities ) and relation - specific pooling attention ( secondary attention with respect to the target relations ) .2 . We introduce a novel pair - wise margin - based objective function that proves superior to standard loss functions .We use the word2 vec skip - gram model to learn initial word representations on Wikipedia .Other matrices are initialized with random values following a Gaussian distribution .We apply a cross-validation procedure on the training data to select suitable hyperparameters .We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .Att - Input - CNN relies only on the primal attention at the input level , performing standard max - pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized .With Att - Input - CNN , we achieve an F1-score of 87.5 % , thus already outperforming not only the original winner of the SemEval task , an SVM - based approach ( 82.2 % ) , but also the wellknown CR - CNN model ( 84.1 % ) with a relative improvement of 4.04 % , and the newly released DRNNs ( 85.8 % ) with a relative improvement of 2.0 % , although the latter approach depends on the Stanford parser to obtain dependency parse information .Our full dual attention model Att - Pooling - CNN achieves an even more favorable F1- score of 88 % .To better quantify the contribution of the different components of our model , we also conduct an ablation study evaluating several simplified models .The first simplification is to use our model without the input attention mechanism but with the pooling attention layer .The second removes both attention mechanisms .The third removes both forms of attention and additionally uses a regular objective function based on the inner product s = r w for a sentence representation r and relation class embedding w.We observe that all three of our components lead to noticeable improvements over these baselines .\n",
            "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural NetworksIn relation extraction , one challenge that is faced when building a machine learning system is the generation of training examples .In this paper , we propose a novel model dubbed Piecewise Convolutional Neural Networks ( PC - NNs ) with multi-instance learning to address the two problems described above .To address the first problem , distant supervised relation extraction is treated as a multi-instance problem similar to previous studies .We design an objective function at the bag level .In the learning process , the uncertainty of instance labels can be taken into account ; this alleviates the wrong label problem .To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .To capture structural and other latent information , we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer .The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence .In this paper , we use the Skip - gram model ( word2 vec ) 5 to train the word embeddings on the NYT corpus .Following , we tune all of the models using three - fold validation on the training set .We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .Because the position dimension has little effect on the result , we heuristically choose d p = 5 .The batch size is fixed to 50 .We use Adadelta in the update procedure ; it relies on two main parameters , ? and ? , which do not significantly affect the performance .In the dropout operation , we randomly set the hidden unit activities to zero with a probability of 0.5 during training .Mintz represents a traditional distantsupervision - based model that was proposed by .MultiR is a multi-instance learning method that was proposed by .MIML is a multi-instance multilabel model that was proposed by .shows the precision - recall curves for each method , where PCNNs + MIL denotes our method , and demonstrates that PCNNs + MIL achieves higher precision over the entire range of recall .PCNNs + MIL enhances the recall to ap - proximately 34 % without any loss of precision .In terms of both precision and recall , PCNNs + MIL outperforms all other evaluated approaches .Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction .Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem .\n",
            "Attention Guided Graph Convolutional Networks for Relation ExtractionIn this paper , we propose the novel Attention Guided Graph Convolutional Networks ( AGGCNs ) , which operate directly on the full tree .Intuitively , we develop a \" soft pruning \" strategy that transforms the original dependency tree into a fully connected edgeweighted graph .These weights can be viewed as the strength of relatedness between nodes , which can be learned in an end - to - end fashion by using self - attention mechanism .we next introduce dense connections ) to the GCN model following .For GCNs , L layers will be needed in order to capture neighborhood information that is L hops away .With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .Our code is available at https://github.com/Cartus / AGGCN_TACREDWe choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.Glo Ve vectors are used as the initialization for word embeddings .For cross - sentence n- ary relation extraction task , we consider three kinds of models as baselines :1 ) a feature - based classifier based on shortest dependency paths between all entity pairs , 2 ) Graph - structured LSTM methods , including Graph LSTM , bidirectional DAG LSTM ( Bidir DAG LSTM ) and Graph State LSTM ( GS GLSTM ) .These methods extend LSTM to encode graphs constructed from input sentences with dependency edges , 3 ) Graph convolutional networks ( GCN ) with pruned trees , 6 https://nlp.stanford.edu/projects/For ternary relation extraction ( first two columns in ) , our AGGCN model achieves accuracies of 87.1 and 87.0 on instances within single sentence ( Single ) and on all instances ( Cross ) , respectively , which outperform all the baselines .More specifically , our AG - GCN model surpasses the state - of - the - art Graphstructured LSTM model ( GS GLSTM ) by 6.8 and 3.8 points for the Single and Cross settings , respectively .Compared to GCN models , our model obtains 1.3 and 1.2 points higher than the best performing model with pruned tree ( K=1 ) .For binary relation extraction ( third and fourth columns in ) , AGGCN consistently outperforms GS GLSTM and GCN as well .AGGCN also performs better than GCNs , although its performance can be boosted via pruned trees .However , our AGGCN model still obtains 8.0 and 5.7 points higher than the GS GLSTM model for ternary and binary relations , respectively .We also notice that our AGGCN achieves a better test accuracy than all GCN models , which further demonstrates its ability to learn better representations from full trees .Our C - AGGCN model achieves an F1 score of 68.2 , which outperforms the state - ofart C - GCN model by 1.8 points .We also notice that AGGCN and C - AGGCN achieve better precision and recall scores than GCN and C - GCN , respectively .The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation .We also evaluate our model on the SemEval dataset under the same settings as .Our C - AGGCN model ( 85.7 ) consistently outperforms the C - GCN model ( 84.8 ) , showing the good generalizability .We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .We also notice that the feed - forward layer is effective in our model .Without the feed - forward layer , the result drops to an F1 score of 67.8 .We can observe that all the C - AGGCN models with varied values of K are able to outperform the state - of - the - art C - GCN model ( reported in ) .In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .In general , C - AGGCN with full trees outperforms C - AGGCN with pruned trees and C - GCN against various sentence lengths .Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .This suggests that C - AGGCN can benefit more from larger graphs ( full tree ) .C - AGGCN consistently outperforms C - GCN under the same amount of training data .When the size of training data increases , we can observe that the performance gap becomes more obvious .Particularly , using 80 % of the training data , the C - AGGCN model is able to achieve a F 1 score of 66.5 , higher than C - GCN trained on the whole dataset .\n",
            "Context - Aware Representations for Knowledge Base Relation ExtractionWe demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .The main goal of relation extraction is to determine a type of relation between two target entities that appear together in a text .In this paper , we consider the sentential relation extraction task : to each occurrence of the target entity pair e 1 , e 2 in some sentence s one has to assign a relation type r from a given set R. A triple e 1 , r , e 2 is called a relation instance and we refer to the relation of the target entity pair as target relation .We present a novel architecture that considers other relations in the sentence as a context for predicting the label of the target relation .Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .The representation of the target relation and representations of the context relations are combined to make the final prediction .All models were trained using the Adam optimizer with categorical crossentropy as the loss function .We use an early stopping criterion on the validation data to determine the number of training epochs .The learning rate is fixed to 0.01 and the rest of the optimization parameters are set as recommended in : ? 1 = 0.9 , ? 2 = 0.999 , ? = 1e ? 08 . The training is performed in batches of 128 instances .We apply Dropout on the penultimate layer as well as on the embeddings layer with a probability of 0.5 .We choose the size of the layers ( RNN layer size o = 256 ) and entity marker embeddings ( d = 3 ) with a random search on the validation set .The models that take the context into account perform similar to the baselines at the smallest recall numbers , but start to positively deviate from them at higher recall rates .In particular , the ContextAtt model performs better than any other system in our study over the entire recall range .Compared to the competitive LSTM - baseline that uses the same relation encoder , the ContextAtt model achieves a 24 % reduction of the average error : from 0.2096 0.002 to 0.1590 0.002 .shows that the ContextAtt model performs best over all relation types .One can also see that the ContextSum does n't universally outperforms the LSTM - baseline .It demonstrates again that using attention is crucial to extract relevant information from the context relations .On the relation - specific results we observe that the context - enabled model demonstrates the most improvement on precision and seems to be especially useful for taxonomy relations ( see SUBCLASS OF , PART OF ) .\n",
            "Extracting Multiple - Relations in One - Pass with Pre-Trained TransformersThe state - of - the - art solutions for extracting multiple entity - relations from an input paragraph always require a multiple - pass encoding on the input .This paper proposes a new solution that can complete the multiple entityrelations extraction task with only one - pass encoding on the input corpus , and achieve a new state - of - the - art accuracy performance , as demonstrated in the ACE 2005 benchmark .Relation extraction ( RE ) aims to find the semantic relation between a pair of entity mentions from an input paragraph .One particular type of the RE task is multiplerelations extraction ( MRE ) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph .Because in real - world applications , whose input paragraphs dominantly contain multiple pairs of entities , an efficient and effective solution for MRE has more important and more practical implications .This work presents a solution that can resolve the inefficient multiple - passes issue of existing solutions for MRE by encoding the input only once , which significantly increases the efficiency and scalability .Specifically , the proposed solution is built on top of the existing transformer - based , pretrained general - purposed language encoders .In this paper we use Bidirectional Encoder Representations from Transformers ( BERT ) as the transformer - based encoder , but this solution is not limited to using BERT alone .The two novel modifications to the original BERT architecture are : ( 1 ) we introduce a structured prediction layer for predicting multiple relations for different entity pairs ; and ( 2 ) we make the selfattention layers aware of the positions of all en-tities in the input paragraph .BERT SP : BERT with structured prediction only , which includes proposed improvement in 3.1 .Entity - Aware BERT SP : our full model , which includes both improvements in 3.1 and 3.2 .BERT SP with position embedding on the final attention layer .This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods .Note that our method was not designed for domain adaptation , it still outperforms those methods with domain adaptation .Among all the BERT - based approaches , finetuning the off - the - shelf BERT does not give a satisfying result , because the sentence embeddings can not distinguish different entity pairs .The simpler version of our approach , BERT SP , can successfully adapt the pre-trained BERT to the MRE task , and achieves comparable performance at the 3 Note the usage of relative position embeddings does notwork for one - pass MRE , since each word corresponds to a varying number of position embedding vectors .It works for the singlerelation per pass setting , but the performance lags behind using only indicators of the two target entities .Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .For BERT SP with entity indicators on inputs , it is expected to perform slightly better in the single - relation setting , because of the mixture of information from multiple pairs .A 2 % gap is observed as expected .For BERT SP with position embeddings on the final attention layer , we train the model in the single - relation setting and test with two different settings , so the results are the same .Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .\n",
            "However , the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly .A basic but highly important challenge in natural language understanding is being able to populate a knowledge base with relational facts contained in a piece of text .Existing work on relation extraction ( e.g. , has been unable to achieve sufficient recall or precision for the results to be usable versus hand - constructed knowledge bases .We propose a new , effective neural network sequence model for relation classification .Its architecture is better customized for the slot filling task : the word representations are augmented by extra distributed representations of word position relative to the subject and object of the putative relation .This means that the neural attention model can effectively exploit the combination of semantic similarity - based attention and positionbased attention .Secondly , we markedly improve the availability of supervised training data by using Mechanical Turk crowd annotation to produce a large supervised training dataset , suitable for the common relations between people , organizations and locations which are used in the TAC KBP evaluations .We name this dataset the TAC Relation Extraction Dataset ( TACRED ) , and will make it available through the Linguistic Data Consortium ( LDC ) in order to respect copyrights on the underlying text .We map words that occur less than 2 times in the training set to a special < UNK > token .We use the pre-trained GloVe vectors to initialize word embeddings .For all the LSTM layers , we find that 2 - layer stacked LSTMs generally work better than one - layer LSTMs .We minimize cross - entropy loss over all 42 relations using AdaGrad .We apply Dropout with p = 0.5 to CNNs and LSTMs .During training we also find a word dropout strategy to be very effective : we randomly set a token to be < UNK > with a probability p.We set p to be 0.06 for the SDP - LSTM model and 0.04 for all other models .We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems , which demonstrates the effectiveness of neural models for relation extraction .Although positional embeddings help increase the F 1 by around 2 % over the plain CNN model , a simple ( 2 - layer ) LSTM model performs surprisingly better than CNN and dependency - based models .Lastly , our proposed position - aware mechanism is very effective and achieves an F 1 score of 65.4 % , with an absolute increase of 3.9 % over the best baseline neural model ( LSTM ) and 7.9 % over the baseline logistic regression system .We also run an ensemble of our position - aware attention model which takes majority votes from 5 runs with random initializations and it further pushes the F 1 score up by 1.6 % .CNN - based models tend to have higher precision ; RNN - based models have better recall .Evaluating relation extraction systems on slot filling is particularly challenging in that : ( 1 ) Endto - end cold start slot filling scores conflate the performance of all modules in the system ( i.e. , entity recognizer , entity linker and relation extractor ) .( 2 ) Errors in hop - 0 predictions can easily propagate to hop - 1 predictions .We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -presents the results of an ablation test of our position - aware attention model on the development set of TACRED .The entire attention mechanism contributes about 1.5 % F 1 , where the position - aware term in Eq.( 3 ) alone contributes about 1 % F 1 score .shows how the slot filling evaluation scores change as we change the amount of negative ( i.e. , no relation ) training data provided to our proposed model .We find that : ( 1 ) At hop - 0 level , precision increases as we provide more negative examples , while recall stays almost unchanged .F 1 score keeps increasing .( 2 ) At hop - all level , F 1 score increases by Performance by sentence length .We find that : ( 1 ) Performance of all models degrades substantially as the sentences get longer .When compared with the CNN - PE model , our position - aware attention model achieves improved F 1 scores on 30 out of the 41 slot types , with the top 5 slot types being org : members , per: country of death , org : shareholders , per:children and per:religion .When compared with SDP - LSTM model , our model achieves improved F 1 scores on 26 out of the 41 slot types , with the top 5 slot types being org : political / religious affiliation , per: country of death , org : alternate names , per:religion and per: alternate names .We observe that slot types with relatively sparse training examples tend to be improved by using the position - aware attention model .We find that the model learns to pay more attention to words that are informative for the relation ( e.g. , \" graduated from \" , \" niece \" and \" chairman \" ) , though it still makes mistakes ( e.g. , \" refused to name the three \" ) .We also observe that the model tends to put a lot of weight onto object entities , as the object NER signatures are very informative to the classification of relations .\n",
            "Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer LearningRelation extraction aims to extract relations between pairs of marked entities in raw texts .In this paper , we propose a novel word - level approach for distant supervised relation extraction by reducing inner-sentence noise and improving robustness against noisy words .To reduce innersentence noise , we utilize a novel Sub - Tree Parse ( STP ) method to remove irrelevant words by intercepting a subtree under the parent of entities ' lowest common ancestor .Furthermore , the entity - wise attention is adopted to alleviate the influence of noisy words in the subtree and emphasize the task - relevant features .To tackle the second challenge , we initialize our model parameters with a priori knowledge learned from the entity type classification task by transfer learning .In the experiment , we utilize word2vec 2 to train word embeddings on NYT corpus .The grid search approach is used to select optimal learning rate lr for Adam optimizer among { 0.1 , 0.001 , 0.0005 , 0.0001 } , GRU size m ? { 100 , 160 , 230 , 400 } , position embedding size l ? { 5 , 10 , 15 , 20}. shows all parameters for our task .GRU size m 230Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity - Task weights ( ?head , ? tail ) 0.5,0.5 Entity - Relation Task weight ?0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ?0.0001From , we can observe that the model with the STP performs best , and the SDP model obtains an even worse result than the pure one .The PR curve areas of BGRU + SDP and BGRU are about 0.332 and 0.337 respectively , while BGRU + STP increases it to 0.366 .The result indicates : ( 1 ) Our STP can get rid of irrelevant words in each instance and obtain more precise sentence representation for relation extraction .( 2 ) The SDP method is not appropriate to handle low - quality sentences where key relation words are not in the SDP .From and , we can obtain : ( 1 ) Regardless of the dataset that we employ , BGRU - WLA ( + STP ) + EWA outperforms BGRU (+ STP ) .To be more specific , the PR curve area has a relative improvement of over 2.3 % , which demonstrates that entity - wise hidden states in the BGRU present more precise relational features than other word states .EWA achieves further improvements and outperforms the baseline by over 4.6 % , because it considers more information than entity or relational words alone .( 1 ) Regardless of the dataset that we use , models with TL achieve better performance , which improve the PR curve area by over 4.7 % .( 2 ) BGRU + STP + TL achieves the best performance and increases the area to 0.383 , while areas of BGRU , BGRU + STP and BGRU + TL are 0.337 , 0.366 and 0.372 respectively .Mintz proposes the humandesigned feature model .MultiR puts forward a graphical model .MIML proposes a multi -instance multi-label model .PCNN puts forward a piecewise CNN for relation extraction .PCNN + ATT proposes the selective attention mechanism with PCNN .BGRU proposes a BGRU with the word - level attention mechanism .\n",
            "Enriching Pre-trained Language Model with Entity Information for Relation ClassificationIn this paper , we apply the pretrained BERT model for relation classification .We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .We then locate the positions of the two target entities in the output embedding from BERT model .We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .We add dropout before each add - on layer .For the pre-trained BERT model , we use the uncased basic model .We compare our method , R - BERT , against results by multiple methods recently published for the SemEval - 2010 Task 8 dataset , including SVM , RNN , MVRNN , CNN + Softmax , FCM , CR - CNN , Attention - CNN , Entity Attention Bi-LSTM .We can see that R - BERT significantly beats all the baseline methods .The MACRO F1 value of R - BERT is 89. 25 , which is much better than the previous best solution on this dataset .We observe that the three methods all perform worse than R - BERT .Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .BERT without special separate tokens can not locate the target entities and lose this key information .On the other hand , incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction .\n",
            "Matching the Blanks : Distributional Similarity for Relation LearningReading text to identify and extract relations between entities has been along standing goal in natural language processing .Typically efforts in relation extraction fall into one of three groups .First , we study the ability of the Transformer neural network architecture to encode relations between entity pairs , and we identify a method of representation that outperforms previous work in supervised relation extraction .Then , we present a method of training this relation representation without any supervision from a knowledge graph or human annotators by matching the blanks .shows that the task agnostic BERT EM and BERT EM + MTB models outperform the previous published state of the art on FewRel task even when they have not seen any FewRel training data .For BERT EM + MTB , the increase over 's supervised approach is very significant - 8.8 % on the 5 - way - 1 - shot task and 12.7 % on the 10 - way - 1 - shot task .BERT EM + MTB also significantly outperforms BERT EM in this unsupervised setting , which is to be expected since there is no relation - specific loss during BERT EM 's training .When given access to all of the training data , BERT EM approaches BERT EM + MTB 's performance .The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .The additional MTB based training further increases F 1 scores for all tasks .For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .This further supports our argument that training by matching the blanks can significantly reduce the amount of human input required to create relation extractors , and populate a knowledge base .\n",
            "Graph Convolution over Pruned Dependency Trees Improves Relation ExtractionIn this work , we propose a novel extension of the graph convolutional network ) that is tailored for relation extraction .Our model encodes the dependency structure over the input sentence with efficient graph convolution operations , then extracts entity - centric representations to make robust relation predictions .We also apply a novel path - centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content , which further improves the performance of several dependencybased models including ours .Dependency - based models .( 1 ) A logistic regression ( LR ) classifier which combines dependencybased features with other lexical features .( 2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .Neural sequence model .Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .Results on the TACRED DatasetWe observe that our GCN model Our Model ( C - GCN ) 84.8 * 76.5 * outperforms all dependency - based models by at least 1.6 F 1 .By using contextualized word representations , the C - GCN model further outperforms the strong PA - LSTM model by 1.3 F 1 , and achieves a new state of the art .In addition , we find our model improves upon other dependencybased models in both precision and recall .Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .As we will show in Section 6.2 , we find that our GCN models have complementary strengths when compared to the PA - LSTM .This simple interpolation between a GCN and a PA - LSTM achieves an F 1 score of 67.1 , outperforming each model alone by at least 2.0 F 1 .An interpolation between a C - GCN and a PA - LSTM further improves the result to 68.2 .Results on the SemEval DatasetWe find that under the conventional with- entity evaluation , our C - GCN model outperforms all existing dependency - based neural models on this sep - arate dataset .Notably , by properly incorporating off - path information , our model outperforms the previous shortest dependency path - based model ( SDP - LSTM ) .Under the mask - entity evaluation , our C - GCN model also outperforms PA - LSTM by a substantial margin , suggesting its generalizability even when entities are not seen .To show the effectiveness of path - centric pruning , we compare the two GCN models and the Tree - LSTM when the pruning distance K is varied .As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .We find that all three models are less effective when the entire dependency tree is present , indicating that including extra information hurts performance .Finally , we note that contextualizing the GCN makes it less sensitive to changes in the tree structures provided , presumably because the model can use word sequence information in the LSTM layer to recover any off - path information that it needs for correct relation extraction .We find that : The entity representations and feedforward layers contribute 1.0 F 1 .( 2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .( 3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .( 4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .\n",
            "RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side InformationDistantly - supervised Relation Extraction ( RE ) methods train an extractor by automatically aligning relation instances in a Knowledge Base ( KB ) with unstructured text .RE models usually ignore such readily available side information .Relation Extraction ( RE ) attempts to fill this gap by extracting semantic relationships between entity pairs from plain text .In this paper , we propose RESIDE , a novel distant supervised relation extraction method which utilizes additional supervision from KB through its neural network based architecture .RESIDE makes principled use of entity type and relation alias information from KBs , to impose soft constraints while predicting the relation .It uses encoded syntactic information obtained from Graph Convolution Networks ( GCN ) , along with embedded side information , to improve neural relation extraction .RESIDE 's source code and datasets used in the paper are available at http://github.com / malllabiisc / RESIDE .Mintz : Multi-class logistic regression model proposed by for distant supervision paradigm .MultiR : Probabilistic graphical model for multi instance learning by MIMLRE :A graphical model which jointly models multiple instances and multiple labels .More details in . PCNN : A CNN based relation extraction model by which uses piecewise max - pooling for sentence representation .PCNN + ATT : A piecewise max - pooling over CNN based model which is used by to get sentence representation followed by attention over sentences .BGWA : Bi - GRU based relation extraction model with word and sentence level attention ) .Overall , we find that RESIDE achieves higher precision over the entire recall range on both the datasets .All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous .RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .The higher performance of BGWA and PCNN + ATT over PCNN shows that attention helps in distant supervised RE .The results validate that GCNs are effective at encoding syntactic information .Further , the improvement from side information shows that it is complementary to the features extracted from text , thus validating the central thesis of this paper , that inducing side information leads to improved relation extraction .We find that the model performs best when aliases are provided by the KB itself .Overall , we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available .We observe that performance improves further with the availability of more alias information .\n"
          ]
        }
      ],
      "source": [
        "summary = ''\n",
        "abstract = ''\n",
        "sum_list = []\n",
        "abs_list = []\n",
        "score = []\n",
        "y = 1\n",
        "x = 0\n",
        "k = 0\n",
        "while k < 100:\n",
        "  summary = ''\n",
        "  abstract = ''\n",
        "  y = 1\n",
        "  \n",
        "  while df1['idx'].iloc[x] == y:\n",
        "    if df1['bi_labels'].iloc[x] == 1:\n",
        "      summary = summary + '' + df1['text'].iloc[x]\n",
        "    if df1['main_heading'].iloc[x]=='abstract':\n",
        "      abstract = abstract + '' + df1['text'].iloc[x]\n",
        "    x += 1\n",
        "    y += 1\n",
        "  print(summary)\n",
        "  score.append(rouge.get_scores(summary, abstract))\n",
        "  sum_list.append(summary)\n",
        "  abs_list.append(abstract)\n",
        "  k += 1\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riCJ8PpKHZPD"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "header = [\"SN\",\"Abstract\", \"Summary\", \"Rogue Score\"]\n",
        "x = 0\n",
        "\n",
        "with open(\"out.csv\", 'w',encoding='UTF8',newline='') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "  while x < len(score):\n",
        "    writer.writerow([x,abs_list[x],sum_list[x],score[x]])\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnaHey6pfFRz",
        "outputId": "1ee025f7-11d7-428c-c986-446ca063adaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_D1Y9ABoTUF",
        "outputId": "f128185d-dc52-41e5-ec62-7fc4192db364"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.5254237288135594,\n",
              "   'p': 0.2897196261682243,\n",
              "   'f': 0.3734939713216722},\n",
              "  'rouge-2': {'r': 0.23976608187134502,\n",
              "   'p': 0.09808612440191387,\n",
              "   'f': 0.13921901115942834},\n",
              "  'rouge-l': {'r': 0.5084745762711864,\n",
              "   'p': 0.2803738317757009,\n",
              "   'f': 0.3614457785505879}}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(summary, abstract)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcJEbY56v8pU"
      },
      "source": [
        "Using BERT for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5bepQJDqdIG",
        "outputId": "5b9505a5-d2eb-47e4-903c-fb0a835c8abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting simpletransformers\n",
            "  Downloading simpletransformers-0.63.7-py3-none-any.whl (249 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting transformers>=4.6.0\n",
            "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 53.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 35.8 MB/s \n",
            "\u001b[?25hCollecting wandb>=0.10.32\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.3.5)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.12.2-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.1 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.0.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 365 kB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.21.6)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (4.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.6.0->simpletransformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 43.9 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (57.4.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers) (1.15.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 74.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 58.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 71.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 59.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 57.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 58.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157 kB 54.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156 kB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.5.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 115 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.8.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (2022.8.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.6.0->simpletransformers) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2022.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0b1-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 37.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.0)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164 kB 44.4 MB/s \n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.0.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.2->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.47.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n",
            "Building wheels for collected packages: pathtools, seqeval, validators\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=7b9d218c81e387218f4fee9e12adeb278aeae1d809bdde1463447dd522c449f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=9628d6129c0a51c9ea37748d8d97956a92a2408803aaf7bfdff3b5ead1b9265d\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=f61f02f6ff8ae9a441a2ca5281f0b70f02275ed8f4c49b43d2990c4baf18de0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built pathtools seqeval validators\n",
            "Installing collected packages: urllib3, smmap, gitdb, commonmark, xxhash, watchdog, validators, tokenizers, shortuuid, setproctitle, sentry-sdk, semver, rich, responses, pympler, pydeck, pathtools, multiprocess, huggingface-hub, GitPython, docker-pycreds, blinker, wandb, transformers, streamlit, seqeval, sentencepiece, datasets, simpletransformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.27 blinker-1.5 commonmark-0.9.1 datasets-2.4.0 docker-pycreds-0.4.0 gitdb-4.0.9 huggingface-hub-0.9.1 multiprocess-0.70.13 pathtools-0.1.2 pydeck-0.8.0b1 pympler-1.0.1 responses-0.18.0 rich-12.5.1 semver-2.13.0 sentencepiece-0.1.97 sentry-sdk-1.9.0 seqeval-1.2.2 setproctitle-1.3.2 shortuuid-1.0.9 simpletransformers-0.63.7 smmap-5.0.0 streamlit-1.12.2 tokenizers-0.12.1 transformers-4.21.3 urllib3-1.25.11 validators-0.20.0 wandb-0.13.2 watchdog-2.1.9 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912,
          "referenced_widgets": [
            "75eb0efdc4e24540bc86976c99b42c75",
            "dff46b0de0fa477d978edcd2f6701de3",
            "12a70a1f18e04d9ab83c039653715405",
            "0f705ce7a6114add9893de7757657ffa",
            "58d4fb2a972f455795e17f6335e1855d",
            "ecce4fe0fdcd4891846322e71c55ef15",
            "07bd67e721b440afb4b056f9ec808a79",
            "288f3dfc97284149a14992accd1d7e30",
            "ae9fb627a6f344978e59984c6f979e9d",
            "585833253eac4144a8dbdd89eca4dbfc",
            "b8461a821eb949b49242d7077942a086",
            "d35d2aad230749ad80f10f13552c1002",
            "667e015c86fd46668fcea75b4602297a",
            "fa08e26640974d61b7317a0f6b3c20e2",
            "aeebd4a3f3514b1583c8f8cbad4abb8b",
            "88986d6b33974193a0b68ce800c21949",
            "952bce3ff021423cbe67a36258dd3719",
            "6849b17a856c443ea01343bf8567536d",
            "3f323a594dd041c29553771affc9a200",
            "2fd3fffc886b492d8fe561e2f3c0b319",
            "9da922cea052441ba005da49a9abce6c",
            "d0d976069a1140d6bd2558c73fe74ff4",
            "8b2e62053a6c461a9b97ac28e0ab7816",
            "71f81469e7f24674972f56cc5d345e69",
            "c14a15989eac4ba794a40e6d85749d6f",
            "3d95dcf5980742c3b25a9974a276c1b5",
            "864ebe6c14124d28a2efe405085e5652",
            "958dac2d817141478ea1b4d142593ea7",
            "9a550c6913bb4f819e4a9e0cc8d5464f",
            "04382b7f1c3b444f8775c44034dc292e",
            "19255b06511b4e8ba8a8790e7b957f45",
            "403ff664844341748c043c484f82b37b",
            "e7cb2746b4d247919b016dd1452e7bf6",
            "2e6ce7f5d7f144a2868050efca9a7594",
            "ba3a9b4527254678addf8a182c9638af",
            "27a452cc658f4efd9f61dba74df1da06",
            "fb4cdd8ed31849868a8c0f92d3b9d45e",
            "318828c1a31243ada8623255cb637574",
            "06fec744e43f4128b5a19d7ed2a73a43",
            "a5cee0eff8314e95a41ce404761da1da",
            "a1321ce01a0040909738ffda6366670b",
            "95f888e3b85349809584f27466c92998",
            "5d9e8c6292084823a4b3d8705b191121",
            "4ab6b8c1fe88428fa5ff31c3be6f92f6",
            "1fa7ea2c894447e4b38ce2147674672a",
            "58d06b5955c94bd0be9062c0401f1bbc",
            "864782c6d3244428b5a553d676431719",
            "3368f998e0cb40b2b0fc5cc1a1337adf",
            "b2ff1eb3ecc946cebf1b50e4b7c23ed6",
            "c9c2b6b5b3ef431493f76063d92b41bd",
            "777803485fd5415f9304d5c01f89b896",
            "2e9c1e27eded4e02a01d70472f310bb4",
            "59f338af6c4c406b9f37dec2d1c5ec36",
            "732380851ca04145a95e5007b740ea39",
            "f3ffbb79481642ba9ababb2108bd3931",
            "6791266921264bf88935b031ec405a16",
            "51e0125f1b5044f08dd5245429a11ac4",
            "72577aed0ac443f1bb9ada40d79ec18a",
            "1e006d52dda54ac7a8458976c6060179",
            "7b30e59942f044eab9fa16d01f01e0e3",
            "d5fa35aed217434897def1c7c7a12ae1",
            "6d62adf8af5d4566a97325b1ffc84f47",
            "4997139df5694380b17163f013d4671a",
            "384f6557298649b790985f0e531789a0",
            "6d61105d303542648862fe266a2b2bfb",
            "e94d21b9bf3f47bc88bddd38335a0ea5",
            "c1037d7536c146faaa9f31f1e7c4c806",
            "151a987387184a9180b626f358d49c56",
            "6f7e74824bdc4e67b44f3bb54abcca28",
            "8530d802c6fb48a79dd4d1789a516efb",
            "fc62281c55ac4ff2b0592dea6d51e739",
            "35fe3e86aaae4bad902d658d8a23c361",
            "5a47929abf0c47ebae695f22a14367fa",
            "da25aa8f39ae4c76820d1174cfbe8edf",
            "3516d8b4210e411d804f27dc43353d6e",
            "e30c3e8808ec4346aca1c2143559b64c",
            "d22aabb6f4e74df5b6d520ab1142c011",
            "f6ae352f8efb47b3aa2f18d49e01dbfe",
            "288660414af84b7ca1083c40abe75d1f",
            "f93dc4f514754ca1b0a415380c09a2d4",
            "3242c3064f8c455fbf294b5e2720dbf1",
            "e6031fe5a55b4314b07d950478f6dbb4",
            "3d1b243fee5c470cbc50147ae818e8d1",
            "4aa89e3543194b849650b6e4d78eaf75",
            "bdf1d034e1f344308914283e791599bf",
            "2e0bec2056094d2d9d284692add90647",
            "a2ec8023c26e43e8a070bf941cb4b6be",
            "644e5cbc99944e57bd0567200e80f6e9",
            "308f54a4120b40b697dc1516ac0986b6",
            "2ffa937d9319487c8efb87f47d1509cd",
            "397f9dfa4c1e468d8cece6453aaba49c",
            "ce479299ae7f4db9aa347950ba5f9dfd",
            "2366deb4b4cd4cda8f348a1db66e944a",
            "5449d5a825d343febc9b7c53b809e7a4",
            "38d37d67caa14ab9b8f85860573b3b9d",
            "7d765f4996ef4d67af274279ecb94a63",
            "b7e0385dc0b94c4ebe669c8f59d80181",
            "b8320f3438e1447abd30cc6f18fb7877",
            "82a5b96eae4c4bda89cc1f4722ae23ac",
            "e4e190cf3acf4bb1853f3b100ec24ee1",
            "d103208689ce4e1ba13f5e40fbeb6ada",
            "0c6f112d86034fb882889f3bbcff4829",
            "db5ddf21d1644317a8b8bd25d2990789",
            "de0efb22b8df4019a46b61cb375ada89",
            "604a86066d854060912ef64af3f8c99a",
            "1a07ecb6b53344c9aec12116e77cc3ca",
            "8bc002aa85874d53b53eae9c8dd104e4",
            "cb75d7f61f434e1bab100bd624d10a09",
            "dfa293868f47423e9a33806479e87f5b",
            "c346f420e9cd4dd3a9631f24516810cf"
          ]
        },
        "id": "_j_FqJJ0pRsO",
        "outputId": "921ccc40-b4a8-4f09-ff7f-9c9e0a561d53"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75eb0efdc4e24540bc86976c99b42c75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d35d2aad230749ad80f10f13552c1002",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/422M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b2e62053a6c461a9b97ac28e0ab7816",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/223k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e6ce7f5d7f144a2868050efca9a7594",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fa7ea2c894447e4b38ce2147674672a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Epoch 0 of 3:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6791266921264bf88935b031ec405a16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/224 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1037d7536c146faaa9f31f1e7c4c806",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Epoch 1 of 3:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6ae352f8efb47b3aa2f18d49e01dbfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/224 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308f54a4120b40b697dc1516ac0986b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Epoch 2 of 3:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4e190cf3acf4bb1853f3b100ec24ee1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/224 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(15,\n",
              " defaultdict(list,\n",
              "             {'global_step': [5, 10, 15],\n",
              "              'train_loss': [0.6684433817863464,\n",
              "               0.6455604434013367,\n",
              "               0.26190826296806335],\n",
              "              'mcc': [0.17600664594905818,\n",
              "               0.35262491742120977,\n",
              "               0.30488250292212377],\n",
              "              'tp': [36, 33, 20],\n",
              "              'tn': [39, 109, 152],\n",
              "              'fp': [148, 78, 35],\n",
              "              'fn': [1, 4, 17],\n",
              "              'auroc': [0.6461916461916463,\n",
              "               0.7823384882208412,\n",
              "               0.8025726261020378],\n",
              "              'auprc': [0.26833763755825285,\n",
              "               0.39514813676764005,\n",
              "               0.4736273347754718],\n",
              "              'F1_score': [0.3257918552036199,\n",
              "               0.44594594594594594,\n",
              "               0.4347826086956522],\n",
              "              'eval_loss': [0.7000990786722728,\n",
              "               0.582463946725641,\n",
              "               0.45661281182297636]}))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import random\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "df = pd.read_csv('/content/all_sent.csv')\n",
        "df = df.drop(columns=['BIO', 'BIO_1', 'BIO_2', 'labels']).rename(\n",
        "    columns={'bi_labels': 'labels'})\n",
        "df['title'] = df['main_heading'] + ': ' + df['heading']\n",
        "df.loc[((df['main_heading'] == df['heading']) | (\n",
        "    pd.isnull(df['heading']))), 'title'] = df['main_heading']\n",
        "df['title'] = df['title'].fillna('')\n",
        "df['paper'] = df['topic'] + df['paper_idx'].astype(str)\n",
        "ids = df[\"paper\"].unique()\n",
        "random.seed(1)\n",
        "random.shuffle(ids)\n",
        "bound = int(0.9*len(ids))\n",
        "\n",
        "train_df = df.set_index(\"paper\").loc[ids[:bound]].reset_index()\n",
        "eval_df = df.set_index(\"paper\").loc[ids[bound:]].reset_index()\n",
        "train_df = train_df.sample(frac=1, random_state=1)\n",
        "\n",
        "# Some sentences are in the 'related work' or 'conclusion' section, and should be masked out.\n",
        "train_df = train_df[train_df['mask'] == 1]\n",
        "eval_df = eval_df[eval_df['mask'] == 1]\n",
        "\n",
        "train_pos = train_df[train_df['labels'] == 1]\n",
        "train_neg = train_df[train_df['labels'] == 0]\n",
        "imbalance_ratio = len(train_neg) / len(train_pos)\n",
        "\n",
        "# Create a ClassificationModel\n",
        "model_args = ClassificationArgs()\n",
        "model_args.use_early_stopping = True\n",
        "model_args.early_stopping_metric = \"F1_score\"\n",
        "model_args.early_stopping_metric_minimize = False\n",
        "model_args.early_stopping_patience = 2\n",
        "model_args.early_stopping_consider_epochs = True\n",
        "model_args.evaluate_during_training = True\n",
        "model_args.evaluate_during_training_verbose = True\n",
        "\n",
        "model_args.downsample = 1.0\n",
        "model_args.normalize_ofs = True\n",
        "model_args.out_learning_rate = 0.001\n",
        "\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.output_dir = 'binary/'\n",
        "model_args.best_model_dir = 'binary/best_model'\n",
        "model_args.save_model_every_epoch = True\n",
        "model_args.save_steps = -1\n",
        "model_args.manual_seed = 1\n",
        "model_args.fp16 = False\n",
        "model_args.num_train_epochs = 3\n",
        "model_args.train_batch_size = 64\n",
        "model_args.use_multiprocessing = False  # set to True if cpu memory is enough\n",
        "model_args.gradient_accumulation_steps = 4\n",
        "model_args.learning_rate = 0.001\n",
        "model_args.scheduler = \"polynomial_decay_schedule_with_warmup\"\n",
        "model_args.polynomial_decay_schedule_power = 0.5\n",
        "model_args.warmup_steps = 200\n",
        "model_args.do_lower_case = True\n",
        "\n",
        "\n",
        "# Create a TransformerModel\n",
        "model = ClassificationModel(\n",
        "    \"bert\",\n",
        "    \"allenai/scibert_scivocab_uncased\",\n",
        "    weight=[1, imbalance_ratio/model_args.downsample],\n",
        "    use_cuda=False,\n",
        "    args=model_args,\n",
        ")\n",
        "\n",
        "model.train_model(train_df, eval_df=eval_df,\n",
        "                    F1_score=sklearn.metrics.f1_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvkfsbJXHgWO"
      },
      "source": [
        "###Preprocessing of Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cbyBTSoaGc8M"
      },
      "outputs": [],
      "source": [
        "def find_source(data, ls = []):\n",
        "    if isinstance(data,dict):\n",
        "        for key in data.keys():\n",
        "            if key==\"from sentence\":\n",
        "                sentences=data[key].split('\\n') # The value can be one or more sentences\n",
        "                for s in sentences:\n",
        "                    ls.append(s.strip())\n",
        "            elif isinstance(data[key], dict):\n",
        "                find_source(data[key],ls)\n",
        "            elif isinstance(data[key], list):\n",
        "                for i in data[key]:\n",
        "                    find_source(i,ls)\n",
        "    elif isinstance(data,list):\n",
        "        for i in data:\n",
        "            find_source(i,ls)\n",
        "    return ls    # might have repeated sentences\n",
        "\n",
        "# determine if a triple is contained in the trace of traversing the json object in a depth-firsr manner\n",
        "def is_contained(trace, triple):\n",
        "    if len(trace) >= len(triple):\n",
        "        for i in range(len(trace)-2):\n",
        "            if trace[i:i+3] == triple:\n",
        "                return True\n",
        "        return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Get the previous two words in a trace,\n",
        "# to be prefixed to the coordinated items in a list or a dictionary\n",
        "def get_prefix(data, trace):\n",
        "    if isinstance(data, dict) or isinstance(data, list):\n",
        "        return trace[-2:]\n",
        "\n",
        "# traverse the json object recursively,\n",
        "# to find the source sentence of the triple\n",
        "def find_tri_sent(data, triple, trace=[], ls=[], prefix=[]):\n",
        "    # Parse the json file recursively and return a list of source sentences\n",
        "    if isinstance(data, dict):\n",
        "        for i, key in enumerate(data.keys()):\n",
        "            if key != \"from sentence\":\n",
        "                if prefix and i != 0:\n",
        "                    trace += prefix\n",
        "                trace.append(key)\n",
        "                find_tri_sent(data[key], triple, trace, ls,\n",
        "                              get_prefix(data[key], trace))\n",
        "            else:\n",
        "                if is_contained(trace, triple):\n",
        "                    ls.append(data[key].strip())\n",
        "    elif isinstance(data, list):\n",
        "        for i, item in enumerate(data):\n",
        "            if prefix and i != 0:\n",
        "                trace += prefix\n",
        "            find_tri_sent(item, triple, trace, ls, prefix)\n",
        "    elif isinstance(data, str):\n",
        "        trace.append(data)\n",
        "    return ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "arjwMM_pHmc1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Data preprocessing and cleaning\n",
        "get a dataframe of all sentences, together with relevant information to the tasks\n",
        "'''\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/Scispace/Test_Data'\n",
        "sep = os.path.sep\n",
        "\n",
        "def get_dir(topic_ls=None, paper_ls=None):\n",
        "    # Get the list of paper directories\n",
        "    dir_ls = []\n",
        "    if topic_ls is None:\n",
        "        topic_ls = os.listdir(base_dir)\n",
        "        #topic_ls.remove('train-README.md')\n",
        "        #topic_ls.remove('trial-README.md')\n",
        "    if paper_ls is None:\n",
        "        for topic in topic_ls:\n",
        "            paper_ls = os.listdir(os.path.join(base_dir, topic))\n",
        "            for i in paper_ls:\n",
        "                dir_ls.append(os.path.join(base_dir, topic, i))\n",
        "    else:\n",
        "        for topic in topic_ls:\n",
        "            for i in paper_ls:\n",
        "                dir_ls.append(os.path.join(base_dir, topic, str(i)))\n",
        "    return dir_ls\n",
        "\n",
        "def get_file_path(dirs):\n",
        "    # Get the relevant files from each directory of paper.\n",
        "    rx = '(.*Stanza-out.txt$)|(^sentences.txt$)'\n",
        "    file_path = []\n",
        "    for dir in dirs:\n",
        "        new = ['', '']  # stores the paths of the sentence file and the label file\n",
        "        for file in os.listdir(dir):\n",
        "            res = re.match(rx, file)\n",
        "            if res:\n",
        "                if res.group(1):\n",
        "                    new[0] = os.path.join(dir, file)\n",
        "                if res.group(2):\n",
        "                    new[1] = os.path.join(dir, file)\n",
        "        file_path.append(new)\n",
        "    return file_path\n",
        "\n",
        "def is_heading(line):\n",
        "    # Determine if a line is a heading\n",
        "    ls = line.split(' ')\n",
        "    # Titles rarely end with these words\n",
        "    False_end = ['by', 'as', 'in', 'and', 'that']\n",
        "    if len(ls) < 10 and ls[-1] not in False_end:\n",
        "        rx = '^[A-Z][^?]*[^?:]$|^title$|^abstract$'  # regex heuristic rules\n",
        "        res = re.match(rx, line)\n",
        "        return True if res else False\n",
        "    return False\n",
        "\n",
        "def is_main_heading(line, judge_mask=False):\n",
        "    '''\n",
        "    Assume that the line is a heading, determine if it is a main heading\n",
        "    A main heading is either a typical main section heading, or it contains lexical cues that are considered important for judgement.\n",
        "    '''\n",
        "    if len(line.split(' ')) <= 4:\n",
        "        if judge_mask:    # if the aim is to judge whether the sentence should be skipped\n",
        "            lex_cue = 'background|related|conclusion'  # |related work\n",
        "        else:\n",
        "            lex_cue = 'title|abstract|introduction|background|related|conclusion|model|models|method|methods|approach|architecture|system|application|experiment|experiments|experimental setup|implementation|hyperparameters|training|result|results|ablation|baseline|evaluation'  # |related work\n",
        "        exp = re.compile(lex_cue)\n",
        "        # Decide if it is a main heading\n",
        "        return True if exp.search(line.lower()) else False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Determin if a sentence conforms to a specific case method.\n",
        "# There are three case methods in all, eg: Attention Is All You Need; ATTENTION IS ALL YOU NEED; Attention is all you need\n",
        "\n",
        "def check_case(line, flag):\n",
        "    if flag == 1:\n",
        "        match = re.search(r'[a-z]', line)\n",
        "        if match:\n",
        "            return False\n",
        "        return True\n",
        "    else:\n",
        "        wd_num = 0\n",
        "        words = line.split(' ')\n",
        "        if flag == 0:\n",
        "            stp_wd = ['a', 'an', 'and', 'the', 'or', 'if', 'by', 'as', 'to', \n",
        "            'of', 'for', 'in', 'on', 'but', 'via', 'nor', 'with']\n",
        "            if not words[0].istitle():\n",
        "                wd_num += 1\n",
        "            if len(words) > 1:\n",
        "                if not words[-1].istitle():\n",
        "                    wd_num += 1\n",
        "                for word in words[1:-1]:\n",
        "                    if not word.istitle() and word not in stp_wd:\n",
        "                        wd_num += 1\n",
        "            return wd_num <= math.ceil(len(words)/5)\n",
        "        if flag == 2:\n",
        "            if not words[0].istitle():\n",
        "                wd_num += 1\n",
        "            for word in words[1:]:\n",
        "                if re.match(r'[A-Z]', word):\n",
        "                    wd_num += 1\n",
        "            return wd_num <= math.ceil(len(words)/3)\n",
        "\n",
        "# read the relevant files from the folder of one paper, and produce a data table for that paper.\n",
        "def load_paper_sentence(sent_path, label_path):\n",
        "    sent = []\n",
        "    count = [0, 0, 0]\n",
        "    task, index = sent_path.split(sep)[-3:-1]\n",
        "    # Decide the case type of the titles in this paper, by counting over the main headings and find the maximum\n",
        "    with open(sent_path, 'r') as f:\n",
        "        while(True):\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                if is_heading(line) and is_main_heading(line):\n",
        "                    for m in range(3):\n",
        "                        if check_case(line, m):\n",
        "                            count[m] += 1\n",
        "            else:\n",
        "                break\n",
        "    ocr_path = sent_path[:-14]+'Grobid-out.txt'\n",
        "    with open(ocr_path, 'r') as f:\n",
        "        fl=f.readlines()\n",
        "    title_ls = []\n",
        "    for i in range(len(fl)):\n",
        "        if fl[i]=='\\n':\n",
        "            if i<(len(fl)-1):\n",
        "                title_ls.append(fl[i+1].rstrip())\n",
        "        if fl[i].rstrip().lower() in ['title','abstract','introduction']:\n",
        "            title_ls.append(fl[i].rstrip())\n",
        "\n",
        "    with open(sent_path, 'r') as f:\n",
        "        i = 0\n",
        "        flg = count.index(max(count))\n",
        "        # two string buffers, storing the heading and the main heading respectively\n",
        "        heading, main_h = '', ''\n",
        "        ofs1 = ofs3 = 0\n",
        "        while(True):\n",
        "            i += 1\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                if line in title_ls:\n",
        "                    ofs3 = 0\n",
        "                else:\n",
        "                    ofs3 += 1\n",
        "                if is_heading(line) and check_case(line, flg):\n",
        "                    heading = line    # update the heading buffer\n",
        "                    if is_main_heading(line):\n",
        "                        ofs1 = 0\n",
        "                        main_h = line    # update the main heading buffer too\n",
        "                        # The line itself is a main heading, no heading needs to be stored.\n",
        "                        sent.append(\n",
        "                            [i, line, '', '', task, index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 1, 0, None])\n",
        "                    else:\n",
        "                        ofs1 += 1\n",
        "                        # for plain headings, store the main heading it belongs to.\n",
        "                        # judge if it should be masked\n",
        "                        if is_main_heading(main_h, judge_mask=True):\n",
        "                            sent.append([i, line, main_h, '', task,\n",
        "                                         index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 0, 0, None])\n",
        "                        else:\n",
        "                            sent.append([i, line, main_h, '', task,\n",
        "                                         index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 1, 0, None])\n",
        "                else:\n",
        "                    # For plain text line, store both the heading and the main heading.\n",
        "                    ofs1 += 1\n",
        "                    if is_main_heading(main_h, judge_mask=True):\n",
        "                        sent.append([i, line, main_h, heading, \n",
        "                                     task, index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 0, 0, None])\n",
        "                    else:\n",
        "                        sent.append([i, line, main_h, heading, \n",
        "                                     task, index, None, None, None, ofs1, 0, i-1, 0, ofs3, 0, 1, 0, None])\n",
        "            else:\n",
        "                break\n",
        "    for i in range(1,len(sent)):\n",
        "        if sent[i][9]==0:\n",
        "            sof = sent[i-1][9]\n",
        "            if sof>1:\n",
        "                for j in range(i-sof,i):\n",
        "                    sent[j][10] = sent[j][9]/sof\n",
        "        if sent[i][13] == 0:\n",
        "            sof = sent[i-1][13]\n",
        "            if sof>1:\n",
        "                for j in range(i-sof, i):\n",
        "                    sent[j][14] = sent[j][13]/sof\n",
        "        if i == len(sent)-1:\n",
        "            sof = sent[i][9]\n",
        "            if sof > 1:\n",
        "                for j in range(i-sof+1, i+1):\n",
        "                    sent[j][10] = sent[j][9]/sof\n",
        "            sof = sent[i][13]\n",
        "            if sof > 1:\n",
        "                for j in range(i-sof+1, i+1):\n",
        "                    sent[j][14] = sent[j][13]/sof\n",
        "        sent[i][12] = sent[i][11]/len(sent)\n",
        "\n",
        "    # slice the sentence with the span of characters, returns the span of words\n",
        "    def get_word_idx(sent, start, end):\n",
        "        ls = sent.split(' ')\n",
        "        if isinstance(start, str):\n",
        "            start = int(start)\n",
        "        if isinstance(end, str):\n",
        "            end = int(end)\n",
        "        # if the span of characters doesn't conform to word boundaries, 'st' and 'en' will remain 0.\n",
        "        st, en = 0, 0\n",
        "        length = [len(word) for word in ls]\n",
        "        count = 0\n",
        "        for i in range(len(ls)):\n",
        "            if start == count:\n",
        "                st = i\n",
        "                break\n",
        "            count += (length[i]+1)\n",
        "        for j in range(st, len(ls)):\n",
        "            count += (length[j]+1)\n",
        "            if end == (count-1):\n",
        "                en = j + 1\n",
        "                break\n",
        "        return st, en\n",
        "\n",
        "    # Mark the label of contribution-ralated sentences, and initialize their BIO tag sequences.\n",
        "    with open(label_path, 'r') as f:\n",
        "        while(True):\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                sent[int(line)-1][-2] = 1\n",
        "                sent[int(line)-1][6] = ['O'] * \\\n",
        "                    len(sent[int(line)-1][1].split(' '))\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # go over the entities and change the corresponding part of BIO sequences\n",
        "    ent_path = sep.join(label_path.split(sep)[:-1]+['entities.txt'])\n",
        "    with open(ent_path, 'r') as f:\n",
        "        while(True):\n",
        "            line = f.readline().rstrip(\"\\n\")\n",
        "            if line:\n",
        "                info = line.split('\\t')\n",
        "                sentence = sent[int(info[0])-1][1]\n",
        "                # if sentence.split(' ')[0].lower()[1:] == sentence.split(' ')[0][1:]:\n",
        "                # sentence = sentence[0].lower() + sentence[1:]\n",
        "                st, en = get_word_idx(sentence, info[1], info[2])\n",
        "                phrase = info[3].strip()\n",
        "                # If the span of characters does not match the given phrase, use the given phrase instead\n",
        "                if ' '.join(sentence.split(' ')[st: en]).strip() != phrase:\n",
        "                    st_char = (' ' + sentence).find(' ' + phrase + ' ')\n",
        "                    st, en = get_word_idx(\n",
        "                        sentence, st_char, st_char + len(phrase))\n",
        "                    if st == 0 and en == 0:\n",
        "                        print(\n",
        "                            f'Could not find the phrase \\'{info[3]}\\' in the {int(info[0])}th sentence of \\'{task}\\' paper {index}')\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(\n",
        "                            f'In the {int(info[0])}th sentence of \\'{task}\\' paper {index}, the entity \\'{info[3]}\\' is not in the span ({info[1]}, {info[2]})')\n",
        "                for j in range(st, en):\n",
        "                    if sent[int(info[0])-1][6] is None:\n",
        "                        print(\n",
        "                            f'A phrase exists in the {int(info[0])}th sentence of \\'{task}\\' paper {index}, which is not labeled as a contribution sentence.')\n",
        "                        sent[int(info[0])-1][6] = ['O'] * \\\n",
        "                            len(sent[int(info[0])-1][1].split(' '))\n",
        "                    if j == st:\n",
        "                        sent[int(info[0])-1][6][j] = 'B'\n",
        "                    else:\n",
        "                        sent[int(info[0])-1][6][j] = 'I'\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # decide which information unit each positive sentence belongs to.\n",
        "    j_dir = sep.join(sent_path.split(sep)[:-1]) + sep + 'info-units'\n",
        "    for unit in os.listdir(j_dir):  # For each json file representing an information unit\n",
        "        js_file = os.path.join(j_dir, unit)\n",
        "        try:\n",
        "            with open(js_file, 'r') as f:\n",
        "                data = json.load(f, strict=False)\n",
        "            lst = find_source(data, [])\n",
        "            if \"TITLE\" in lst:  # When the title is a source sentence, sometimes it is abbreviated as 'TITLE'\n",
        "                sent[1][-1] = unit[:-5]\n",
        "            for j in range(len(sent)):\n",
        "                if sent[j][1] in lst:\n",
        "                    sent[j][-1] = unit[:-5]\n",
        "        except json.JSONDecodeError as e:\n",
        "            js_position = sep.join(js_file.split(sep)[-4:])\n",
        "            print(f'JSONDecodeError in {js_position}\\n', e)\n",
        "            continue\n",
        "\n",
        "    # given a sequence of BIO tags, get the list of tuples representing spans of entities\n",
        "    def get_entity_spans(ls):\n",
        "        spans = []\n",
        "        for i in range(len(ls)):\n",
        "            st, ed = 0, 0\n",
        "            if ls[i] == 'B':\n",
        "                st, ed = i, i + 1\n",
        "                for j in range(i+1, len(ls)):\n",
        "                    if ls[j] == 'I':\n",
        "                        ed += 1\n",
        "                    else:\n",
        "                        break\n",
        "                spans.append((st, ed))\n",
        "        return spans\n",
        "\n",
        "    for i in range(len(sent)):\n",
        "        if sent[i][6] is not None:\n",
        "            sent[i][8] = sent[i][7] = sent[i][6]\n",
        "\n",
        "    # try to find the SPO(Subject, Predicate, Object) type of each phrase\n",
        "    aux = []\n",
        "    for i in range(len(sent)):\n",
        "        if sent[i][6] is not None:\n",
        "            tup_ls = get_entity_spans(sent[i][6])\n",
        "            # use three booleans to indicate if the phrase has ever been a subject, predicate or object\n",
        "            tuple_ls = [[0, 0, 0, tup] for tup in tup_ls]\n",
        "            word_ls = sent[i][1].split(' ')\n",
        "            phrase_ls = [' '.join(word_ls[st:en]) for st, en in tup_ls]\n",
        "            # store the sentence idx, tuple_ls, and phrase_ls.\n",
        "            aux.append([i, tuple_ls, phrase_ls, []])        \n",
        "    t_dir = sep.join(sent_path.split(sep)[:-1]) + sep + 'triples'\n",
        "    paper_triple_stat = [0] * 5\n",
        "    for unit in os.listdir(t_dir):\n",
        "        t_file = os.path.join(t_dir, unit)\n",
        "        js_file = os.path.join(j_dir, unit.replace('.txt', '.json'))\n",
        "        try:\n",
        "            with open(js_file,'r') as g:\n",
        "                js = json.load(g, strict=False)\n",
        "                js = {'Contribution': js}\n",
        "        except json.JSONDecodeError as e:\n",
        "            js_position = sep.join(js_file.split(sep)[-4:])\n",
        "            print(f'JSONDecodeError in {js_position}\\n', e)\n",
        "            continue\n",
        "        except FileNotFoundError as fe:\n",
        "            print(fe)\n",
        "            continue\n",
        "        with open(t_file, 'r') as f:\n",
        "            while(True):\n",
        "                line = f.readline().rstrip(\"\\n\")\n",
        "                if line:\n",
        "                    # empty the temporary buffer\n",
        "                    for a in range(len(aux)):\n",
        "                        aux[a][3] = []\n",
        "                    if line[0] == '(':\n",
        "                        line = line[1:]\n",
        "                    if line[-1] == ')':\n",
        "                        line = line[:-1]\n",
        "                    triple = line.split('||')\n",
        "                    evidence = find_tri_sent(\n",
        "                        js, triple, [], [], [])  # unit[:-4]\n",
        "                    if not evidence:\n",
        "                        js_position = sep.join(js_file.split(sep)[-4:]) #\n",
        "                        paper_triple_stat[0] += 1\n",
        "                        print(f'the triple \\'{triple}\\' not found in {js_position}')\n",
        "                    else:                       \n",
        "                        cands = evidence[0].split('\\n')\n",
        "                        for i in range(len(cands)):\n",
        "                            for j in range(len(aux)):\n",
        "                                if cands[i].strip() == sent[aux[j][0]][1]:\n",
        "                                    for w in range(3):\n",
        "                                        for k in range(len(aux[j][2])):\n",
        "                                            if aux[j][2][k] == triple[w]:\n",
        "                                                aux[j][3].append((w, k))\n",
        "                                                break\n",
        "                                    break\n",
        "                        lens = [len(aux[j][3]) for j in range(len(aux))]\n",
        "                        try:\n",
        "                            paper_triple_stat[max(lens)] += 1\n",
        "                        except IndexError:\n",
        "                            print(f'List index out of range. The actual number of max is {max(lens)} for triple \\'{triple}\\' in\\n', t_file)\n",
        "                        if max(lens)!=0:\n",
        "                            idx = lens.index(max(lens))\n",
        "                            found = [0, 0, 0]\n",
        "                            for t in range(len(aux[idx][3])):\n",
        "                                w, k = aux[idx][3][t]\n",
        "                                aux[idx][1][k][w] = 1\n",
        "                                found[w] = 1\n",
        "                            for i in range(3):\n",
        "                                if found[i] == 0:\n",
        "                                    for j in range(len(aux)):\n",
        "                                        for w, k in aux[j][3]:\n",
        "                                            if w == i and triple[w] == aux[j][2][k]:\n",
        "                                                aux[j][1][k][w] = 1\n",
        "                                                break\n",
        "                                        else:\n",
        "                                            continue\n",
        "                                        break\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "    # An S-P-O type corresponds to a combination of boolean indicators\n",
        "    # The 4 keys stand for 'predicate', 'subject', 'object', 'both subject and object' respectively.\n",
        "    good_state = {'p': [0, 1, 0], 's': [1, 0, 0],\n",
        "                    'ob': [0, 0, 1], 'b': [1, 0, 1]}\n",
        "    for i in range(len(aux)):\n",
        "        for item in aux[i][1]:\n",
        "            if item[:3] not in good_state.values():\n",
        "                # if the label of any phrase in the sentence cannot be decided,\n",
        "                # delete the tag sequence to filter out this sentence\n",
        "                sent[aux[i][0]][7] = sent[aux[i][0]][8] = None\n",
        "                break\n",
        "\n",
        "    for i in range(len(aux)):\n",
        "        '''\n",
        "        interprete the boolean states to phrase types according to the BIO_type setting,\n",
        "        and change the corresponding parts in BIO sequences\n",
        "        BIO_type=1: decide whether it is a predicate\n",
        "        BIO_type=2: decide which of the four keys in 'good_state' it belongs to\n",
        "        '''\n",
        "        if sent[aux[i][0]][7] is not None:\n",
        "            sent[aux[i][0]][7] = ['O']*len(sent[aux[i][0]][7])\n",
        "            sent[aux[i][0]][8] = ['O']*len(sent[aux[i][0]][8])\n",
        "            for item in aux[i][1]:\n",
        "                st, en = item[3]\n",
        "                if item[:3] == good_state['p']:\n",
        "                    sent[aux[i][0]][7][st] = 'B-p'\n",
        "                    for j in range(st+1, en):\n",
        "                        sent[aux[i][0]][7][j] = 'I-p'\n",
        "                else:\n",
        "                    sent[aux[i][0]][7][st] = 'B-n'\n",
        "                    for j in range(st+1, en):\n",
        "                        sent[aux[i][0]][7][j] = 'I-n'\n",
        "                for key, value in good_state.items():\n",
        "                    if item[:3] == value:\n",
        "                        sent[aux[i][0]][8][st] = 'B-'+key\n",
        "                        for j in range(st+1, en):\n",
        "                            sent[aux[i][0]][8][j] = 'I-'+key\n",
        "    # print(f'paper triple stat: {paper_triple_stat}')\n",
        "    return sent, paper_triple_stat\n",
        "\n",
        "def load_data_sentence(file_path):\n",
        "    # Get the data table of all the papers in file_path\n",
        "    triple_stat = [0] * 5\n",
        "    data = []\n",
        "    for tuple in file_path:\n",
        "        sentence_path, label_path = tuple\n",
        "        paper_data, paper_triple_stat = load_paper_sentence(\n",
        "        sentence_path, label_path)\n",
        "        for i in range(5):\n",
        "            triple_stat[i] += paper_triple_stat[i]\n",
        "        data += paper_data\n",
        "    return data\n",
        "\n",
        "dirs = get_dir()\n",
        "file_path = get_file_path(dirs)\n",
        "data = load_data_sentence(file_path)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.columns = ['idx', 'text', 'main_heading', 'heading',\n",
        "              'topic', 'paper_idx', 'BIO', 'BIO_1', 'BIO_2', 'offset1', 'pro1', 'offset2', 'pro2', 'offset3', 'pro3', 'mask', 'bi_labels', 'labels']\n",
        "\n",
        "df.to_csv('test_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNxI50AAwFo9"
      },
      "source": [
        "Predicting Contribution statement based on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9a957d8da89c41f7bcfb96ff3fd51816",
            "724e59563ef745d5a537da0273bc38ec",
            "bc3c47dc51f840dc97cf5221d52213c4",
            "bfb0d35e68724d128554f88271f3f621",
            "e09c72cb3ba14000a5273640a69cd43b",
            "12386461390048318002014283377d48",
            "1a82fbc9af214752be187e5004c14545",
            "4358ad162ea64892b53bc5dfb2afe63d",
            "8bfa846e754e497d9d6aa7ede3d2e84a",
            "d537bdf9ddc44f589c3f472ba148bead",
            "87fa669488d140db844c269ae984cb9e",
            "49748effb83444a7b09dd16986e9e49b",
            "9ab0ed92d637448fbcb40c8be210c678",
            "8d9e54b4deff4132abe778d7e4f8109a",
            "f19dd4ab30ab4ecca2bbc5b136c2e744",
            "693141d504c644ebb95e68b2df1b82b9",
            "5e32f594b411471699e6d791b8b435a7",
            "5ae93ed947514994953586b090d2de2a",
            "4cde691cde71407f816a90b6902d5073",
            "55300087f0c041768847662015d0b4b3",
            "533a932dbdb74ff7baadf22e23852777",
            "30bef9a33c7040b5a9f76c255f970607"
          ]
        },
        "id": "fBaX89xJsaBN",
        "outputId": "ec4ad850-7286-493c-8405-68aea59dbc5d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a957d8da89c41f7bcfb96ff3fd51816",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/267 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49748effb83444a7b09dd16986e9e49b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running Evaluation:   0%|          | 0/34 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "df = pd.read_csv('./test_data.csv')\n",
        "df = df.drop(columns=['BIO_2', 'labels']).rename(\n",
        "    columns={'bi_labels': 'labels'})\n",
        "df['title'] = df['main_heading'] + ': ' + df['heading']\n",
        "df.loc[((df['main_heading'] == df['heading']) | (\n",
        "    pd.isnull(df['heading']))), 'title'] = df['main_heading']\n",
        "df['title'] = df['title'].fillna('')\n",
        "\n",
        "model_args = ClassificationArgs()\n",
        "\n",
        "model_args.normalize_ofs = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.use_multiprocessing = False\n",
        "model_args.manual_seed = 1\n",
        "model_args.fp16 = False\n",
        "model_args.do_lower_case = True\n",
        "\n",
        "# Create a TransformerModel\n",
        "model = ClassificationModel(\n",
        "    \"bert\",\n",
        "    \"./binary/best_model\",\n",
        "    use_cuda = False,\n",
        "    args=model_args,\n",
        ")\n",
        "\n",
        "result, model_outputs, wrong_predictions = model.eval_model(\n",
        "    df, F1_score=sklearn.metrics.f1_score)\n",
        "\n",
        "predictions = model_outputs.argmax(axis=1)\n",
        "# select the sentences that are predicted positive, to be the input for subtask 2\n",
        "mask = df['mask'].values\n",
        "# sentences in the 'related work' or 'conclusion' sections are forced to be negative\n",
        "predictions = predictions * mask\n",
        "pos = df[predictions == 1]\n",
        "pos.to_csv('pos_predict_out.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H5HAj7G-0UoY",
        "outputId": "fb3229b3-9567-40e8-baa0-927ec94c5347"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-67737533-2983-467d-9f00-d98560e76cbd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>main_heading</th>\n",
              "      <th>heading</th>\n",
              "      <th>topic</th>\n",
              "      <th>paper_idx</th>\n",
              "      <th>BIO</th>\n",
              "      <th>BIO_1</th>\n",
              "      <th>offset1</th>\n",
              "      <th>pro1</th>\n",
              "      <th>offset2</th>\n",
              "      <th>pro2</th>\n",
              "      <th>offset3</th>\n",
              "      <th>pro3</th>\n",
              "      <th>mask</th>\n",
              "      <th>labels</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>title</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Universal Sentence Encoder</td>\n",
              "      <td>title</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>['B', 'I', 'I']</td>\n",
              "      <td>['B-n', 'I-n', 'I-n']</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.006757</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>abstract</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>0.013514</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We present models for encoding sentences into ...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', ...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>3</td>\n",
              "      <td>0.020270</td>\n",
              "      <td>1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>abstract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>The models are efficient and result in accurat...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>4</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>2</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abstract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>144</td>\n",
              "      <td>The sentence level embeddings surpass the perf...</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>143</td>\n",
              "      <td>0.966216</td>\n",
              "      <td>2</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conclusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>145</td>\n",
              "      <td>Models that make use of sentence and word leve...</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>144</td>\n",
              "      <td>0.972973</td>\n",
              "      <td>3</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conclusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>146</td>\n",
              "      <td>We observe that transfer learning is most help...</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>145</td>\n",
              "      <td>0.979730</td>\n",
              "      <td>4</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conclusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>147</td>\n",
              "      <td>The encoding models make different trade - off...</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>146</td>\n",
              "      <td>0.986486</td>\n",
              "      <td>5</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conclusion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>148</td>\n",
              "      <td>The pre-trained encoding models will be made p...</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>Conclusion</td>\n",
              "      <td>text-classification</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>147</td>\n",
              "      <td>0.993243</td>\n",
              "      <td>6</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Conclusion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>148 rows Ã— 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67737533-2983-467d-9f00-d98560e76cbd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67737533-2983-467d-9f00-d98560e76cbd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67737533-2983-467d-9f00-d98560e76cbd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     idx                                               text main_heading  \\\n",
              "0      1                                              title          NaN   \n",
              "1      2                         Universal Sentence Encoder        title   \n",
              "2      3                                           abstract          NaN   \n",
              "3      4  We present models for encoding sentences into ...     abstract   \n",
              "4      5  The models are efficient and result in accurat...     abstract   \n",
              "..   ...                                                ...          ...   \n",
              "143  144  The sentence level embeddings surpass the perf...   Conclusion   \n",
              "144  145  Models that make use of sentence and word leve...   Conclusion   \n",
              "145  146  We observe that transfer learning is most help...   Conclusion   \n",
              "146  147  The encoding models make different trade - off...   Conclusion   \n",
              "147  148  The pre-trained encoding models will be made p...   Conclusion   \n",
              "\n",
              "        heading                topic  paper_idx  \\\n",
              "0           NaN  text-classification          6   \n",
              "1           NaN  text-classification          6   \n",
              "2           NaN  text-classification          6   \n",
              "3      abstract  text-classification          6   \n",
              "4      abstract  text-classification          6   \n",
              "..          ...                  ...        ...   \n",
              "143  Conclusion  text-classification          6   \n",
              "144  Conclusion  text-classification          6   \n",
              "145  Conclusion  text-classification          6   \n",
              "146  Conclusion  text-classification          6   \n",
              "147  Conclusion  text-classification          6   \n",
              "\n",
              "                                                   BIO  \\\n",
              "0                                                  NaN   \n",
              "1                                      ['B', 'I', 'I']   \n",
              "2                                                  NaN   \n",
              "3    ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', ...   \n",
              "4                                                  NaN   \n",
              "..                                                 ...   \n",
              "143                                                NaN   \n",
              "144                                                NaN   \n",
              "145                                                NaN   \n",
              "146                                                NaN   \n",
              "147                                                NaN   \n",
              "\n",
              "                                                 BIO_1  offset1      pro1  \\\n",
              "0                                                  NaN        0  0.000000   \n",
              "1                                ['B-n', 'I-n', 'I-n']        1  0.000000   \n",
              "2                                                  NaN        0  0.000000   \n",
              "3    ['O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n...        1  0.111111   \n",
              "4                                                  NaN        2  0.222222   \n",
              "..                                                 ...      ...       ...   \n",
              "143                                                NaN        2  0.333333   \n",
              "144                                                NaN        3  0.500000   \n",
              "145                                                NaN        4  0.666667   \n",
              "146                                                NaN        5  0.833333   \n",
              "147                                                NaN        6  1.000000   \n",
              "\n",
              "     offset2      pro2  offset3      pro3  mask  labels       title  \n",
              "0          0  0.000000        0  0.000000     1       0              \n",
              "1          1  0.006757        1  0.000000     1       1       title  \n",
              "2          2  0.013514        0  0.000000     1       0              \n",
              "3          3  0.020270        1  0.111111     1       1    abstract  \n",
              "4          4  0.027027        2  0.222222     1       0    abstract  \n",
              "..       ...       ...      ...       ...   ...     ...         ...  \n",
              "143      143  0.966216        2  0.333333     0       0  Conclusion  \n",
              "144      144  0.972973        3  0.500000     0       0  Conclusion  \n",
              "145      145  0.979730        4  0.666667     0       0  Conclusion  \n",
              "146      146  0.986486        5  0.833333     0       0  Conclusion  \n",
              "147      147  0.993243        6  1.000000     0       0  Conclusion  \n",
              "\n",
              "[148 rows x 17 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "yCPdsyrR1MQ2"
      },
      "outputs": [],
      "source": [
        "df1=pd.read_csv('/content/pos_predict_out.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKq__m6P3CP0",
        "outputId": "af7d7640-b348-4075-9213-3a827028fd04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      1\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "904    0\n",
              "905    0\n",
              "906    0\n",
              "907    0\n",
              "908    0\n",
              "Name: labels, Length: 909, dtype: int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ntsut0LwRmu"
      },
      "source": [
        "Extratcting all the contribution statement from prediction and generating abstractive summary using google pegasus.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "s6NV6g6X2SwC"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPVO7MlN-TVE",
        "outputId": "253e9aa5-1b23-4e78-ef4c-999be58e882b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "934"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnhBpLku4cVb",
        "outputId": "1b6e7060-e1b3-4b82-e269-d973dc82d469"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "909"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df1['idx'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2q_JDen6_BT",
        "outputId": "668f6fae-8ae5-4b6a-e3b1-feb6b3a85785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cm23-BHD8qs6"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y8T3Bskb8whB"
      },
      "outputs": [],
      "source": [
        "src_text =[\"Universal Sentence EncoderWe present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .We find that transfer learning using sentence embeddings tends to outperform word level transfer .With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .Deep Averaging Network ( DAN )The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .MR : Movie review snippet sentiment on a five star scale .CR : Sentiment of sentences mined from customer reviews .SUBJ : Subjectivity of sentences from movie reviews and plot summaries .MPQA : Phrase level opinion polarity from news data .TREC : Fine grained question classification sourced from TREC .SST : Binary phrase level sentiment classification .STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2tqJyZKpHy8",
        "outputId": "5c932cd4-fea9-4e14-a94d-d62fd9d13386"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "934"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gph-wdGE9ICr",
        "outputId": "7dfa5275-ec10-4c1b-ffbb-850b0b254283"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Universal Sentence EncoderWe present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .We find that transfer learning using sentence embeddings tends to outperform word level transfer .With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .Deep Averaging Network ( DAN )The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .MR : Movie review snippet sentiment on a five star scale .CR : Sentiment of sentences mined from customer reviews .SUBJ : Subjectivity of sentences from movie reviews and plot summaries .MPQA : Phrase level opinion polarity from news data .TREC : Fine grained question classification sourced from TREC .SST : Binary phrase level sentiment classification .STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .']"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oV7MrEIx9YJC"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tFmmHL4e9M4r"
      },
      "outputs": [],
      "source": [
        "model_name = 'google/pegasus-xsum'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "0375M1VCaen9"
      },
      "outputs": [],
      "source": [
        "batch = tokenizer(src_text,truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
        "translated = model.generate(**batch,min_length = 500,max_length= 1024)\n",
        "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu1LPN1vc80S",
        "outputId": "2e4d2cdd-e5ef-4c8c-ef3d-01b6f4365771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We present two models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks and find that transfer learning using sentence embeddings tends to outperform word level transfer with minimal amounts of supervised training data for transfer learning via STS , with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with minimal amounts of supervised training data for transfer learning via STS, with surprisingly good performance with surprisingly good performance with surprisingly good performance via STS, with surprisingly good performance via STS, with surprisingly good performance via STS, with transfer learning via STS, with transfer learning via STS, with transfer learning via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS, with STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS via STS.\n"
          ]
        }
      ],
      "source": [
        "print(tgt_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "2fCRF2w6bCNP",
        "outputId": "8f77032a-4e09-408b-a65a-be6b8f6cd9d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .For both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance .Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning .We find that transfer learning using sentence embeddings tends to outperform word level transfer .With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .We obtain encouraging results on Word Embedding Association Tests ( WEAT ) targeted at detecting model bias .Our pre-trained sentence encoding models are made freely available for download and on TF Hub .'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C3R2z4yaqU8",
        "outputId": "e2e13303-fe51-454f-aef9-7d3126853a4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'rouge-1': {'r': 0.43333333333333335,\n",
              "   'p': 0.9285714285714286,\n",
              "   'f': 0.590909086570248},\n",
              "  'rouge-2': {'r': 0.2992125984251969,\n",
              "   'p': 0.7037037037037037,\n",
              "   'f': 0.4198894985757457},\n",
              "  'rouge-l': {'r': 0.43333333333333335,\n",
              "   'p': 0.9285714285714286,\n",
              "   'f': 0.590909086570248}}]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rouge.get_scores(tgt_text, abstract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "Ey-_VoZPg2bc",
        "outputId": "157a0f00-e86e-46dc-a68e-6e007fb1fae4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f1705aef-b962-47ab-b61b-f79924e67107\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>main_heading</th>\n",
              "      <th>heading</th>\n",
              "      <th>topic</th>\n",
              "      <th>paper_idx</th>\n",
              "      <th>BIO</th>\n",
              "      <th>BIO_1</th>\n",
              "      <th>offset1</th>\n",
              "      <th>pro1</th>\n",
              "      <th>offset2</th>\n",
              "      <th>pro2</th>\n",
              "      <th>offset3</th>\n",
              "      <th>pro3</th>\n",
              "      <th>mask</th>\n",
              "      <th>labels</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Robust Multilingual Part - of - Speech Tagging...</td>\n",
              "      <td>title</td>\n",
              "      <td>title</td>\n",
              "      <td>part-of-speech_tagging</td>\n",
              "      <td>0</td>\n",
              "      <td>['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', ...</td>\n",
              "      <td>['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.004065</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Adversarial training ( AT ) 1 is a powerful re...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>part-of-speech_tagging</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>3</td>\n",
              "      <td>0.012195</td>\n",
              "      <td>1</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abstract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>In our experiments on the Penn Treebank WSJ co...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>part-of-speech_tagging</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>6</td>\n",
              "      <td>0.024390</td>\n",
              "      <td>4</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abstract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>We also demonstrate that 3 ) the improved tagg...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>part-of-speech_tagging</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>7</td>\n",
              "      <td>0.028455</td>\n",
              "      <td>5</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abstract</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>These positive results motivate further use of...</td>\n",
              "      <td>abstract</td>\n",
              "      <td>abstract</td>\n",
              "      <td>part-of-speech_tagging</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9</td>\n",
              "      <td>0.036585</td>\n",
              "      <td>7</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abstract</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1705aef-b962-47ab-b61b-f79924e67107')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1705aef-b962-47ab-b61b-f79924e67107 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1705aef-b962-47ab-b61b-f79924e67107');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   idx                                               text main_heading  \\\n",
              "0    2  Robust Multilingual Part - of - Speech Tagging...        title   \n",
              "1    4  Adversarial training ( AT ) 1 is a powerful re...     abstract   \n",
              "2    7  In our experiments on the Penn Treebank WSJ co...     abstract   \n",
              "3    8  We also demonstrate that 3 ) the improved tagg...     abstract   \n",
              "4   10  These positive results motivate further use of...     abstract   \n",
              "\n",
              "    heading                   topic  paper_idx  \\\n",
              "0     title  part-of-speech_tagging          0   \n",
              "1  abstract  part-of-speech_tagging          0   \n",
              "2  abstract  part-of-speech_tagging          0   \n",
              "3  abstract  part-of-speech_tagging          0   \n",
              "4  abstract  part-of-speech_tagging          0   \n",
              "\n",
              "                                                 BIO  \\\n",
              "0  ['O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', ...   \n",
              "1                                                NaN   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "\n",
              "                                               BIO_1  offset1      pro1  \\\n",
              "0  ['O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', ...        1  0.000000   \n",
              "1                                                NaN        1  0.142857   \n",
              "2                                                NaN        4  0.571429   \n",
              "3                                                NaN        5  0.714286   \n",
              "4                                                NaN        7  1.000000   \n",
              "\n",
              "   offset2      pro2  offset3      pro3  mask  labels     title  \n",
              "0        1  0.004065        1  0.000000     1       1     title  \n",
              "1        3  0.012195        1  0.142857     1       0  abstract  \n",
              "2        6  0.024390        4  0.571429     1       0  abstract  \n",
              "3        7  0.028455        5  0.714286     1       0  abstract  \n",
              "4        9  0.036585        7  1.000000     1       0  abstract  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FyZR3fQUB60",
        "outputId": "c7647722-b1cc-4d3b-82cb-f2cc0a2ba711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: Structural Scaffolds for Citation Intent Classification in Scientific PublicationsOur code and data are available at : https://github.com/ allenai/scicite .In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .\n",
            "Abstractive Summary: In this work, we propose a neural scaffold framework for citation intent classification to incorporate knowledge into citations from the structure of scientific papers and introduce a new dataset of citation intents, SciCite , which is at least five times as large as existing datasets and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains and covers a variety of scientific domains\n",
            "Abstract: We propose structural scaffolds , a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents .Our model achieves a new state - of the - art on an existing ACL anthology dataset ( ACL - ARC ) with a 13.3 % absolute increase in F1 score , without relying on external linguistic resources or hand - engineered features as done in existing methods .In addition , we introduce a new dataset of citation intents ( Sci - Cite ) which is more than five times larger and covers multiple scientific domains compared with existing datasets .Our code and data are available at : https://github.com/ allenai/scicite .\n",
            "[{'rouge-1': {'r': 0.3488372093023256, 'p': 0.6818181818181818, 'f': 0.4615384570603551}, 'rouge-2': {'r': 0.12612612612612611, 'p': 0.27450980392156865, 'f': 0.17283950185871066}, 'rouge-l': {'r': 0.27906976744186046, 'p': 0.5454545454545454, 'f': 0.3692307647526627}}]\n"
          ]
        }
      ],
      "source": [
        "summary = ''\n",
        "abstract = ''\n",
        "sum_list = []\n",
        "abs_list = []\n",
        "score = []\n",
        "y = 1\n",
        "x = 0\n",
        "k = 0\n",
        "while df1['idx'].iloc[x]< len(df1['idx']):\n",
        "  if df1['labels'].iloc[x] == 1:\n",
        "    summary = summary + '' + df1['text'].iloc[x]\n",
        "  if df1['main_heading'].iloc[x]=='abstract':\n",
        "    abstract = abstract + '' + df1['text'].iloc[x]\n",
        "  x += 1\n",
        "sum_list.append(summary)\n",
        "abs_list.append(abstract)\n",
        "batch = tokenizer(summary,truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
        "translated = model.generate(**batch,min_length = len(abstract.split())-50,max_length= len(abstract.split())+50)\n",
        "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "score.append(rouge.get_scores(summary, abstract))\n",
        "print(\"Summary:\",summary)\n",
        "print(\"Abstractive Summary:\",tgt_text)\n",
        "print(\"Abstract:\",abstract)\n",
        "print(rouge.get_scores(tgt_text, abstract))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOfTTCMlqxP8",
        "outputId": "7fc7da7e-0855-49e9-f382-2f109218c971"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(abstract.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu_Vyt7RnPU1",
        "outputId": "202b3429-21d6-40ae-b9a7-6bfe1c20c669"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(summary.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "QG48WV5-UDyO"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "header = [\"SN\",\"Abstract\", \"Summary\", \"Rogue Score\"]\n",
        "x = 0\n",
        "\n",
        "with open(\"out.csv\", 'w',encoding='UTF8',newline='') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "  while x < len(score):\n",
        "    writer.writerow([x,abs_list[x],sum_list[x],score[x]])\n",
        "    x += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04382b7f1c3b444f8775c44034dc292e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06fec744e43f4128b5a19d7ed2a73a43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07bd67e721b440afb4b056f9ec808a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c6f112d86034fb882889f3bbcff4829": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bc002aa85874d53b53eae9c8dd104e4",
            "max": 224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb75d7f61f434e1bab100bd624d10a09",
            "value": 1
          }
        },
        "0f705ce7a6114add9893de7757657ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585833253eac4144a8dbdd89eca4dbfc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8461a821eb949b49242d7077942a086",
            "value": " 385/385 [00:00&lt;00:00, 8.95kB/s]"
          }
        },
        "12386461390048318002014283377d48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12a70a1f18e04d9ab83c039653715405": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_288f3dfc97284149a14992accd1d7e30",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae9fb627a6f344978e59984c6f979e9d",
            "value": 385
          }
        },
        "151a987387184a9180b626f358d49c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35fe3e86aaae4bad902d658d8a23c361",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5a47929abf0c47ebae695f22a14367fa",
            "value": "Epochs 1/3. Running Loss:    0.6456: 100%"
          }
        },
        "19255b06511b4e8ba8a8790e7b957f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a07ecb6b53344c9aec12116e77cc3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a82fbc9af214752be187e5004c14545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e006d52dda54ac7a8458976c6060179": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d61105d303542648862fe266a2b2bfb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e94d21b9bf3f47bc88bddd38335a0ea5",
            "value": " 1/224 [00:00&lt;01:04,  3.45it/s]"
          }
        },
        "1fa7ea2c894447e4b38ce2147674672a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58d06b5955c94bd0be9062c0401f1bbc",
              "IPY_MODEL_864782c6d3244428b5a553d676431719",
              "IPY_MODEL_3368f998e0cb40b2b0fc5cc1a1337adf"
            ],
            "layout": "IPY_MODEL_b2ff1eb3ecc946cebf1b50e4b7c23ed6"
          }
        },
        "2366deb4b4cd4cda8f348a1db66e944a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27a452cc658f4efd9f61dba74df1da06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1321ce01a0040909738ffda6366670b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95f888e3b85349809584f27466c92998",
            "value": 3
          }
        },
        "288660414af84b7ca1083c40abe75d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d1b243fee5c470cbc50147ae818e8d1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4aa89e3543194b849650b6e4d78eaf75",
            "value": "  0%"
          }
        },
        "288f3dfc97284149a14992accd1d7e30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e0bec2056094d2d9d284692add90647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e6ce7f5d7f144a2868050efca9a7594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba3a9b4527254678addf8a182c9638af",
              "IPY_MODEL_27a452cc658f4efd9f61dba74df1da06",
              "IPY_MODEL_fb4cdd8ed31849868a8c0f92d3b9d45e"
            ],
            "layout": "IPY_MODEL_318828c1a31243ada8623255cb637574"
          }
        },
        "2e9c1e27eded4e02a01d70472f310bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd3fffc886b492d8fe561e2f3c0b319": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ffa937d9319487c8efb87f47d1509cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5449d5a825d343febc9b7c53b809e7a4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_38d37d67caa14ab9b8f85860573b3b9d",
            "value": "Epochs 2/3. Running Loss:    0.2619: 100%"
          }
        },
        "308f54a4120b40b697dc1516ac0986b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ffa937d9319487c8efb87f47d1509cd",
              "IPY_MODEL_397f9dfa4c1e468d8cece6453aaba49c",
              "IPY_MODEL_ce479299ae7f4db9aa347950ba5f9dfd"
            ],
            "layout": "IPY_MODEL_2366deb4b4cd4cda8f348a1db66e944a"
          }
        },
        "30bef9a33c7040b5a9f76c255f970607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "318828c1a31243ada8623255cb637574": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3242c3064f8c455fbf294b5e2720dbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2ec8023c26e43e8a070bf941cb4b6be",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_644e5cbc99944e57bd0567200e80f6e9",
            "value": " 1/224 [00:00&lt;00:47,  4.67it/s]"
          }
        },
        "3368f998e0cb40b2b0fc5cc1a1337adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_732380851ca04145a95e5007b740ea39",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f3ffbb79481642ba9ababb2108bd3931",
            "value": " 20/20 [25:03&lt;00:00, 60.97s/it]"
          }
        },
        "3516d8b4210e411d804f27dc43353d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35fe3e86aaae4bad902d658d8a23c361": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384f6557298649b790985f0e531789a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38d37d67caa14ab9b8f85860573b3b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "397f9dfa4c1e468d8cece6453aaba49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d765f4996ef4d67af274279ecb94a63",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7e0385dc0b94c4ebe669c8f59d80181",
            "value": 20
          }
        },
        "3d1b243fee5c470cbc50147ae818e8d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d95dcf5980742c3b25a9974a276c1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_403ff664844341748c043c484f82b37b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e7cb2746b4d247919b016dd1452e7bf6",
            "value": " 223k/223k [00:00&lt;00:00, 1.97MB/s]"
          }
        },
        "3f323a594dd041c29553771affc9a200": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "403ff664844341748c043c484f82b37b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4358ad162ea64892b53bc5dfb2afe63d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49748effb83444a7b09dd16986e9e49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ab0ed92d637448fbcb40c8be210c678",
              "IPY_MODEL_8d9e54b4deff4132abe778d7e4f8109a",
              "IPY_MODEL_f19dd4ab30ab4ecca2bbc5b136c2e744"
            ],
            "layout": "IPY_MODEL_693141d504c644ebb95e68b2df1b82b9"
          }
        },
        "4997139df5694380b17163f013d4671a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa89e3543194b849650b6e4d78eaf75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ab6b8c1fe88428fa5ff31c3be6f92f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cde691cde71407f816a90b6902d5073": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e0125f1b5044f08dd5245429a11ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fa35aed217434897def1c7c7a12ae1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6d62adf8af5d4566a97325b1ffc84f47",
            "value": "  0%"
          }
        },
        "533a932dbdb74ff7baadf22e23852777": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5449d5a825d343febc9b7c53b809e7a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55300087f0c041768847662015d0b4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "585833253eac4144a8dbdd89eca4dbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d06b5955c94bd0be9062c0401f1bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c2b6b5b3ef431493f76063d92b41bd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_777803485fd5415f9304d5c01f89b896",
            "value": "Epochs 0/3. Running Loss:    0.6684: 100%"
          }
        },
        "58d4fb2a972f455795e17f6335e1855d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f338af6c4c406b9f37dec2d1c5ec36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a47929abf0c47ebae695f22a14367fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ae93ed947514994953586b090d2de2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d9e8c6292084823a4b3d8705b191121": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e32f594b411471699e6d791b8b435a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "604a86066d854060912ef64af3f8c99a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "644e5cbc99944e57bd0567200e80f6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "667e015c86fd46668fcea75b4602297a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_952bce3ff021423cbe67a36258dd3719",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6849b17a856c443ea01343bf8567536d",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "6791266921264bf88935b031ec405a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51e0125f1b5044f08dd5245429a11ac4",
              "IPY_MODEL_72577aed0ac443f1bb9ada40d79ec18a",
              "IPY_MODEL_1e006d52dda54ac7a8458976c6060179"
            ],
            "layout": "IPY_MODEL_7b30e59942f044eab9fa16d01f01e0e3"
          }
        },
        "6849b17a856c443ea01343bf8567536d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "693141d504c644ebb95e68b2df1b82b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d61105d303542648862fe266a2b2bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d62adf8af5d4566a97325b1ffc84f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f7e74824bdc4e67b44f3bb54abcca28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da25aa8f39ae4c76820d1174cfbe8edf",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3516d8b4210e411d804f27dc43353d6e",
            "value": 20
          }
        },
        "71f81469e7f24674972f56cc5d345e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_958dac2d817141478ea1b4d142593ea7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a550c6913bb4f819e4a9e0cc8d5464f",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "724e59563ef745d5a537da0273bc38ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12386461390048318002014283377d48",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1a82fbc9af214752be187e5004c14545",
            "value": "  0%"
          }
        },
        "72577aed0ac443f1bb9ada40d79ec18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4997139df5694380b17163f013d4671a",
            "max": 224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_384f6557298649b790985f0e531789a0",
            "value": 1
          }
        },
        "732380851ca04145a95e5007b740ea39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75eb0efdc4e24540bc86976c99b42c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dff46b0de0fa477d978edcd2f6701de3",
              "IPY_MODEL_12a70a1f18e04d9ab83c039653715405",
              "IPY_MODEL_0f705ce7a6114add9893de7757657ffa"
            ],
            "layout": "IPY_MODEL_58d4fb2a972f455795e17f6335e1855d"
          }
        },
        "777803485fd5415f9304d5c01f89b896": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b30e59942f044eab9fa16d01f01e0e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d765f4996ef4d67af274279ecb94a63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a5b96eae4c4bda89cc1f4722ae23ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8530d802c6fb48a79dd4d1789a516efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30c3e8808ec4346aca1c2143559b64c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d22aabb6f4e74df5b6d520ab1142c011",
            "value": " 20/20 [24:35&lt;00:00, 62.18s/it]"
          }
        },
        "864782c6d3244428b5a553d676431719": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e9c1e27eded4e02a01d70472f310bb4",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59f338af6c4c406b9f37dec2d1c5ec36",
            "value": 20
          }
        },
        "864ebe6c14124d28a2efe405085e5652": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87fa669488d140db844c269ae984cb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88986d6b33974193a0b68ce800c21949": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2e62053a6c461a9b97ac28e0ab7816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71f81469e7f24674972f56cc5d345e69",
              "IPY_MODEL_c14a15989eac4ba794a40e6d85749d6f",
              "IPY_MODEL_3d95dcf5980742c3b25a9974a276c1b5"
            ],
            "layout": "IPY_MODEL_864ebe6c14124d28a2efe405085e5652"
          }
        },
        "8bc002aa85874d53b53eae9c8dd104e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bfa846e754e497d9d6aa7ede3d2e84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d9e54b4deff4132abe778d7e4f8109a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cde691cde71407f816a90b6902d5073",
            "max": 34,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55300087f0c041768847662015d0b4b3",
            "value": 34
          }
        },
        "952bce3ff021423cbe67a36258dd3719": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958dac2d817141478ea1b4d142593ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f888e3b85349809584f27466c92998": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a550c6913bb4f819e4a9e0cc8d5464f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a957d8da89c41f7bcfb96ff3fd51816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_724e59563ef745d5a537da0273bc38ec",
              "IPY_MODEL_bc3c47dc51f840dc97cf5221d52213c4",
              "IPY_MODEL_bfb0d35e68724d128554f88271f3f621"
            ],
            "layout": "IPY_MODEL_e09c72cb3ba14000a5273640a69cd43b"
          }
        },
        "9ab0ed92d637448fbcb40c8be210c678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e32f594b411471699e6d791b8b435a7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5ae93ed947514994953586b090d2de2a",
            "value": "Running Evaluation: 100%"
          }
        },
        "9da922cea052441ba005da49a9abce6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1321ce01a0040909738ffda6366670b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ec8023c26e43e8a070bf941cb4b6be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5cee0eff8314e95a41ce404761da1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae9fb627a6f344978e59984c6f979e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aeebd4a3f3514b1583c8f8cbad4abb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da922cea052441ba005da49a9abce6c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d0d976069a1140d6bd2558c73fe74ff4",
            "value": " 422M/422M [00:09&lt;00:00, 50.6MB/s]"
          }
        },
        "b2ff1eb3ecc946cebf1b50e4b7c23ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e0385dc0b94c4ebe669c8f59d80181": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8320f3438e1447abd30cc6f18fb7877": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8461a821eb949b49242d7077942a086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba3a9b4527254678addf8a182c9638af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06fec744e43f4128b5a19d7ed2a73a43",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a5cee0eff8314e95a41ce404761da1da",
            "value": "Epoch 3 of 3: 100%"
          }
        },
        "bc3c47dc51f840dc97cf5221d52213c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4358ad162ea64892b53bc5dfb2afe63d",
            "max": 267,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bfa846e754e497d9d6aa7ede3d2e84a",
            "value": 1
          }
        },
        "bdf1d034e1f344308914283e791599bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfb0d35e68724d128554f88271f3f621": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d537bdf9ddc44f589c3f472ba148bead",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_87fa669488d140db844c269ae984cb9e",
            "value": " 1/267 [00:00&lt;01:30,  2.93it/s]"
          }
        },
        "c1037d7536c146faaa9f31f1e7c4c806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_151a987387184a9180b626f358d49c56",
              "IPY_MODEL_6f7e74824bdc4e67b44f3bb54abcca28",
              "IPY_MODEL_8530d802c6fb48a79dd4d1789a516efb"
            ],
            "layout": "IPY_MODEL_fc62281c55ac4ff2b0592dea6d51e739"
          }
        },
        "c14a15989eac4ba794a40e6d85749d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04382b7f1c3b444f8775c44034dc292e",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19255b06511b4e8ba8a8790e7b957f45",
            "value": 227845
          }
        },
        "c346f420e9cd4dd3a9631f24516810cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c2b6b5b3ef431493f76063d92b41bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb75d7f61f434e1bab100bd624d10a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce479299ae7f4db9aa347950ba5f9dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8320f3438e1447abd30cc6f18fb7877",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_82a5b96eae4c4bda89cc1f4722ae23ac",
            "value": " 20/20 [25:27&lt;00:00, 61.19s/it]"
          }
        },
        "d0d976069a1140d6bd2558c73fe74ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d103208689ce4e1ba13f5e40fbeb6ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_604a86066d854060912ef64af3f8c99a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1a07ecb6b53344c9aec12116e77cc3ca",
            "value": "  0%"
          }
        },
        "d22aabb6f4e74df5b6d520ab1142c011": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d35d2aad230749ad80f10f13552c1002": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_667e015c86fd46668fcea75b4602297a",
              "IPY_MODEL_fa08e26640974d61b7317a0f6b3c20e2",
              "IPY_MODEL_aeebd4a3f3514b1583c8f8cbad4abb8b"
            ],
            "layout": "IPY_MODEL_88986d6b33974193a0b68ce800c21949"
          }
        },
        "d537bdf9ddc44f589c3f472ba148bead": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fa35aed217434897def1c7c7a12ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da25aa8f39ae4c76820d1174cfbe8edf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5ddf21d1644317a8b8bd25d2990789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa293868f47423e9a33806479e87f5b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c346f420e9cd4dd3a9631f24516810cf",
            "value": " 1/224 [00:00&lt;01:43,  2.16it/s]"
          }
        },
        "de0efb22b8df4019a46b61cb375ada89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa293868f47423e9a33806479e87f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff46b0de0fa477d978edcd2f6701de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecce4fe0fdcd4891846322e71c55ef15",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_07bd67e721b440afb4b056f9ec808a79",
            "value": "Downloading config.json: 100%"
          }
        },
        "e09c72cb3ba14000a5273640a69cd43b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e30c3e8808ec4346aca1c2143559b64c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e190cf3acf4bb1853f3b100ec24ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d103208689ce4e1ba13f5e40fbeb6ada",
              "IPY_MODEL_0c6f112d86034fb882889f3bbcff4829",
              "IPY_MODEL_db5ddf21d1644317a8b8bd25d2990789"
            ],
            "layout": "IPY_MODEL_de0efb22b8df4019a46b61cb375ada89"
          }
        },
        "e6031fe5a55b4314b07d950478f6dbb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7cb2746b4d247919b016dd1452e7bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e94d21b9bf3f47bc88bddd38335a0ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecce4fe0fdcd4891846322e71c55ef15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19dd4ab30ab4ecca2bbc5b136c2e744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_533a932dbdb74ff7baadf22e23852777",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30bef9a33c7040b5a9f76c255f970607",
            "value": " 34/34 [01:49&lt;00:00,  2.63s/it]"
          }
        },
        "f3ffbb79481642ba9ababb2108bd3931": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6ae352f8efb47b3aa2f18d49e01dbfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_288660414af84b7ca1083c40abe75d1f",
              "IPY_MODEL_f93dc4f514754ca1b0a415380c09a2d4",
              "IPY_MODEL_3242c3064f8c455fbf294b5e2720dbf1"
            ],
            "layout": "IPY_MODEL_e6031fe5a55b4314b07d950478f6dbb4"
          }
        },
        "f93dc4f514754ca1b0a415380c09a2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf1d034e1f344308914283e791599bf",
            "max": 224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e0bec2056094d2d9d284692add90647",
            "value": 1
          }
        },
        "fa08e26640974d61b7317a0f6b3c20e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f323a594dd041c29553771affc9a200",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fd3fffc886b492d8fe561e2f3c0b319",
            "value": 442221694
          }
        },
        "fb4cdd8ed31849868a8c0f92d3b9d45e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d9e8c6292084823a4b3d8705b191121",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ab6b8c1fe88428fa5ff31c3be6f92f6",
            "value": " 3/3 [1:20:09&lt;00:00, 1606.09s/it]"
          }
        },
        "fc62281c55ac4ff2b0592dea6d51e739": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
