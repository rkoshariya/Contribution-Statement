idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,offset1,pro1,offset2,pro2,offset3,pro3,mask,labels,title
2,Structural Scaffolds for Citation Intent Classification in Scientific Publications,title,title,Sentence_classification,0,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']",1,0.0,1,0.0037453183520599,1,0.0,1,1,title
5,"We propose structural scaffolds , a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents .",abstract,abstract,Sentence_classification,0,,,2,0.4,4,0.0149812734082397,2,0.4,1,0,abstract
6,"Our model achieves a new state - of the - art on an existing ACL anthology dataset ( ACL - ARC ) with a 13.3 % absolute increase in F1 score , without relying on external linguistic resources or hand - engineered features as done in existing methods .",abstract,abstract,Sentence_classification,0,,,3,0.6,5,0.0187265917602996,3,0.6,1,0,abstract
7,"In addition , we introduce a new dataset of citation intents ( Sci - Cite ) which is more than five times larger and covers multiple scientific domains compared with existing datasets .",abstract,abstract,Sentence_classification,0,,,4,0.8,6,0.0224719101123595,4,0.8,1,0,abstract
8,Our code and data are available at : https://github.com/ allenai/scicite .,abstract,abstract,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']",5,1.0,7,0.0262172284644194,5,1.0,1,1,abstract
14,"Therefore , identifying the intent of citations ( is critical in improving automated analysis of academic literature and scientific impact measurement .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,5,0.2777777777777778,13,0.048689138576779,5,0.2777777777777778,1,0,Introduction: The nature of citations can be different .
15,"Other applications of citation intent classification are enhanced research experience , information retrieval , summarization ( Co - .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,6,0.3333333333333333,14,0.0524344569288389,6,0.3333333333333333,1,0,Introduction: The nature of citations can be different .
18,"Title : Gait asymmetry in patients with Parkinson 's disease and elderly fallers ... han and , and studying evolution of scientific fields .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,9,0.5,17,0.0636704119850187,9,0.5,1,0,Introduction: The nature of citations can be different .
19,"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5555555555555556,18,0.0674157303370786,10,0.5555555555555556,1,1,Introduction: The nature of citations can be different .
20,A citation context includes text spans in a citing paper describing a referenced work and has been shown to be the primary signal in intent classification .,Introduction,The nature of citations can be different .,Sentence_classification,0,,,11,0.6111111111111112,19,0.0711610486891385,11,0.6111111111111112,1,0,Introduction: The nature of citations can be different .
23,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",14,0.7777777777777778,22,0.0823970037453183,14,0.7777777777777778,1,1,Introduction: The nature of citations can be different .
24,"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8333333333333334,23,0.0861423220973782,15,0.8333333333333334,1,1,Introduction: The nature of citations can be different .
25,"Unlike the primary task of citation intent prediction , it is easy to collect large amounts of training data for scaffold tasks since the labels naturally occur in the process of writing a paper and thus , there is no need for manual annotation .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,16,0.8888888888888888,24,0.0898876404494382,16,0.8888888888888888,1,0,Introduction: The nature of citations can be different .
26,"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['B', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']",17,0.9444444444444444,25,0.0936329588014981,17,0.9444444444444444,1,1,Introduction: The nature of citations can be different .
27,"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,1.0,26,0.097378277153558,18,1.0,1,1,Introduction: The nature of citations can be different .
29,We propose a neural multitask learning framework for classification of citation intents .,Model,Model,Sentence_classification,0,,,1,0.0344827586206896,28,0.1048689138576779,1,0.0555555555555555,1,0,Model
32,Our model uses a large auxiliary dataset to incorporate this structural information available in scientific documents into the citation intents .,Model,Model,Sentence_classification,0,,,4,0.1379310344827586,31,0.1161048689138576,4,0.2222222222222222,1,0,Model
43,where w is a parameter served as the query vector for dot - product attention .,Model,LSTM ) .,Sentence_classification,0,,,15,0.5172413793103449,42,0.1573033707865168,15,0.8333333333333334,1,0,Model: LSTM ) .
48,In scientific writing there is a connection between the structure of scientific papers and the intent of citations .,Model,Structural scaffolds,Sentence_classification,0,,,20,0.6896551724137931,47,0.1760299625468165,1,0.0333333333333333,1,0,Model: Structural scaffolds
49,"To leverage this connection for more effective classification of citation intents , we propose a multitask framework with two structural scaffolds ( auxiliary tasks ) related to the structure of scientific documents .",Model,Structural scaffolds,Sentence_classification,0,,,21,0.7241379310344828,48,0.1797752808988764,2,0.0666666666666666,1,0,Model: Structural scaffolds
50,A key point for our proposed scaffolds is that they do not need any additional manual annotation as labels for these tasks occur naturally in scientific writing .,Model,Structural scaffolds,Sentence_classification,0,,,22,0.7586206896551724,49,0.1835205992509363,3,0.1,1,0,Model: Structural scaffolds
53,"The first scaffold task that we consider is "" citation worthiness "" of a sentence , indicating whether a sentence needs a citation .",Model,Citation worthiness .,Sentence_classification,0,,,25,0.8620689655172413,52,0.1947565543071161,6,0.2,1,0,Model: Citation worthiness .
54,"The language expressed in citation sentences is likely distinctive from regular sentences in scientific writing , and such information could also be useful for better language modeling of the citation contexts .",Model,Citation worthiness .,Sentence_classification,0,,,26,0.896551724137931,53,0.198501872659176,7,0.2333333333333333,1,0,Model: Citation worthiness .
55,"To this end , using citation markers such as "" [ 12 ] "" or "" Lee et al ( 2010 ) "" , we identify sentences in a paper that include citations and the negative samples are sentences without citation markers .",Model,Citation worthiness .,Sentence_classification,0,,,27,0.9310344827586208,54,0.2022471910112359,8,0.2666666666666666,1,0,Model: Citation worthiness .
56,The goal of the model for this task is to predict whether a particular sentence needs a citation .,Model,Citation worthiness .,Sentence_classification,0,,,28,0.9655172413793104,55,0.2059925093632958,9,0.3,1,0,Model: Citation worthiness .
59,The second scaffold task relates to predicting the section title in which a citation appears .,Section title .,Section title .,Sentence_classification,0,,,1,0.0526315789473684,58,0.2172284644194756,12,0.4,1,0,Section title .
63,"Therefore , we use the section title prediction as a scaffold for predicting citation intents .",Section title .,Section title .,Sentence_classification,0,,,5,0.2631578947368421,62,0.2322097378277153,16,0.5333333333333333,1,0,Section title .
65,We are using the section titles from a larger set of data than training data for the main task as a proxy to learn linguistic patterns thatare helpful for citation intents .,Section title .,Section title .,Sentence_classification,0,,,7,0.3684210526315789,64,0.2397003745318352,18,0.6,1,0,Section title .
66,"In particular , we leverage a large number of scientific papers for which the section information is known for each citation to automatically generate large amounts of training data for this scaffold task .",Section title .,Section title .,Sentence_classification,0,,,8,0.4210526315789473,65,0.2434456928838951,19,0.6333333333333333,1,0,Section title .
69,Multitask learning as defined by is an approach to inductive transfer learning that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias .,Section title .,Multitask formulation .,Sentence_classification,0,,,11,0.5789473684210527,68,0.2546816479400749,22,0.7333333333333333,1,0,Section title .: Multitask formulation .
73,We use a Multi Layer Perceptron ( MLP ) for each task and then a softmax layer to obtain prediction probabilites .,Section title .,Multitask formulation .,Sentence_classification,0,,,15,0.7894736842105263,72,0.2696629213483146,26,0.8666666666666667,1,0,Section title .: Multitask formulation .
87,"In each training epoch , we construct mini-batches with the same number of instances from each of then tasks .",Training,Training,Sentence_classification,0,,,9,0.5625,86,0.3220973782771535,9,0.75,1,0,Training
89,"We compute the gradient of the loss for each mini-batch and tune model parameters using the AdaDelta optimizer ( Zeiler , 2012 ) with gradient clipping threshold of 5.0 .",Training,Training,Sentence_classification,0,,,11,0.6875,88,0.3295880149812734,11,0.9166666666666666,1,0,Training
90,We stop training the model when the development macro F1 score does not improve for five consecutive epochs .,Training,Training,Sentence_classification,0,,,12,0.75,89,0.3333333333333333,12,1.0,1,0,Training
93,"While there has been along history of studying citation intents , there are only a few existing publicly available datasets on Intent cateogry Definition",Training,Data,Sentence_classification,0,,,15,0.9375,92,0.3445692883895131,2,1.0,1,0,Training: Data
105,Weighted measurements were superior to T2 - weighted contrast imaging which was in accordance with former studies Similar results to our study were reported in the study of Lee et al ( 2010 ) .,Method,Method,Sentence_classification,0,,,5,0.0961538461538461,104,0.3895131086142322,5,0.3333333333333333,1,0,Method
106,the task of citation intent classification .,Method,Method,Sentence_classification,0,,,6,0.1153846153846153,105,0.3932584269662921,6,0.4,1,0,Method
107,We use the most recent and comprehensive ( ACL - ARC citations dataset ) by as a benchmark dataset to compare the performance of our model to previous work .,Method,Method,Sentence_classification,0,,,7,0.1346153846153846,106,0.3970037453183521,7,0.4666666666666667,1,0,Method
108,"In addition , to address the limited scope and size of this dataset , we introduce SciCite , a new dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL - ARC .",Method,Method,Sentence_classification,0,,,8,0.1538461538461538,107,0.4007490636704119,8,0.5333333333333333,1,0,Method
110,ACL - ARC citations dataset,Method,,Sentence_classification,0,,,10,0.1923076923076923,109,0.4082397003745318,10,0.6666666666666666,1,0,Method
111,ACL - ARC is a dataset of citation intents released by .,Method,ACL - ARC citations dataset,Sentence_classification,0,,,11,0.2115384615384615,110,0.4119850187265917,11,0.7333333333333333,1,0,Method: ACL - ARC citations dataset
112,"The dataset is based on a sample of papers from the ACL Anthology Reference Corpus and includes 1,941 citation instances from 186 papers and is annotated by domain experts in the NLP field .",Method,ACL - ARC citations dataset,Sentence_classification,0,,,12,0.2307692307692307,111,0.4157303370786517,12,0.8,1,0,Method: ACL - ARC citations dataset
113,"The data was split into three standard stratified sets of train , validation , and test with 85 % of data used for training and remaining 15 % divided equally for validation and test .",Method,ACL - ARC citations dataset,Sentence_classification,0,,,13,0.25,112,0.4194756554307116,13,0.8666666666666667,1,0,Method: ACL - ARC citations dataset
116,SciCite dataset,Method,,Sentence_classification,0,,,16,0.3076923076923077,115,0.4307116104868914,0,0.0,1,0,Method
120,"Furthermore , these datasets are usually domain - specific and are relatively small ( less than 2,000 annotated citations ) .",Method,SciCite dataset,Sentence_classification,0,,,20,0.3846153846153846,119,0.445692883895131,4,0.4,1,0,Method: SciCite dataset
121,"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .",Method,SciCite dataset,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']",21,0.4038461538461538,120,0.449438202247191,5,0.5,1,1,Method: SciCite dataset
124,"Therefore , our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers .",Method,SciCite dataset,Sentence_classification,0,,,24,0.4615384615384615,123,0.4606741573033708,8,0.8,1,0,Method: SciCite dataset
128,Citation intent of sentence extractions was labeled through the crowdsourcing platform .,Method,Data collection and annotation,Sentence_classification,0,"['B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",28,0.5384615384615384,127,0.4756554307116105,1,0.0555555555555555,1,1,Method: Data collection and annotation
129,"We selected a sample of papers from the Semantic Scholar corpus , 7 consisting of papers in general computer science and medicine domains .",Method,Data collection and annotation,Sentence_classification,0,,,29,0.5576923076923077,128,0.4794007490636704,2,0.1111111111111111,1,0,Method: Data collection and annotation
130,Citation contexts were extracted using science - parse .,Method,,Sentence_classification,0,,,30,0.5769230769230769,129,0.4831460674157303,3,0.1666666666666666,1,0,Method
134,"We later removed instances annotated with the OTHER option from our dataset ( less than 1 % of the annotated data ) , many of which were due to citation contexts which are incomplete or too short for the annotator to infer the citation intent .",Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,34,0.6538461538461539,133,0.49812734082397,7,0.3888888888888889,1,0,Method: Citation contexts were extracted using science - parse .
135,We used 50 test questions annotated by a domain expert to ensure crowdsource workers were following directions and disqualify annotators with accuracy less than 75 % .,Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,35,0.6730769230769231,134,0.50187265917603,8,0.4444444444444444,1,0,Method: Citation contexts were extracted using science - parse .
136,"Furthermore , crowdsource workers were required to remain on the annotation page ( five annotations ) for at least ten seconds before proceeding to the next page .",Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,36,0.6923076923076923,135,0.5056179775280899,9,0.5,1,0,Method: Citation contexts were extracted using science - parse .
138,The annotations were aggregated along with a confidence score describing the level of agreement between multiple crowdsource workers .,Method,Annotations were dynamically collected .,Sentence_classification,0,,,38,0.7307692307692307,137,0.5131086142322098,11,0.6111111111111112,1,0,Method: Annotations were dynamically collected .
139,The confidence score is the agreement on a single instance weighted by a trust score ( accuracy of the annotator on the initial 50 test questions ) .,Method,Annotations were dynamically collected .,Sentence_classification,0,,,39,0.75,138,0.5168539325842697,12,0.6666666666666666,1,0,Method: Annotations were dynamically collected .
140,"To only collect high quality annotations , instances with confidence score of ? 0.7 were discarded .",Method,Annotations were dynamically collected .,Sentence_classification,0,,,40,0.7692307692307693,139,0.5205992509363296,13,0.7222222222222222,1,0,Method: Annotations were dynamically collected .
141,"In addition , a subset of the dataset with 100 samples was re-annotated by a trained , expert annotator to check for quality , and the agreement rate with crowdsource workers was 86 % .",Method,Annotations were dynamically collected .,Sentence_classification,0,,,41,0.7884615384615384,140,0.5243445692883895,14,0.7777777777777778,1,0,Method: Annotations were dynamically collected .
142,"Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .",Method,Annotations were dynamically collected .,Sentence_classification,0,"['B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",42,0.8076923076923077,141,0.5280898876404494,15,0.8333333333333334,1,1,Method: Annotations were dynamically collected .
143,"Each sentence was annotated , on average , 3.74 times .",Method,Annotations were dynamically collected .,Sentence_classification,0,"['O', 'B', 'B', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",43,0.8269230769230769,142,0.5318352059925093,16,0.8888888888888888,1,1,Method: Annotations were dynamically collected .
144,"This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .",Method,Annotations were dynamically collected .,Sentence_classification,0,"['O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']",44,0.8461538461538461,143,0.5355805243445693,17,0.9444444444444444,1,1,Method: Annotations were dynamically collected .
145,"In addition to the crowdsourced data , a separate test set of size 1,861 was annotated by a trained , expert annotator to ensure high quality of the dataset .",Method,Annotations were dynamically collected .,Sentence_classification,0,,,45,0.8653846153846154,144,0.5393258426966292,18,1.0,1,0,Method: Annotations were dynamically collected .
147,"For the first scaffold ( citation worthiness ) , we sample sentences from papers and consider the sentences with citations as positive labels .",Method,Data for scaffold tasks,Sentence_classification,0,,,47,0.903846153846154,146,0.5468164794007491,1,0.1666666666666666,1,0,Method: Data for scaffold tasks
149,"For the second scaffold ( citation section title ) , respective to each test dataset , we sample citations from the ACL - ARC corpus and Semantic Scholar corpus 9 and extract the citation context as well as their corresponding sections .",Method,Data for scaffold tasks,Sentence_classification,0,,,49,0.9423076923076924,148,0.5543071161048689,3,0.5,1,0,Method: Data for scaffold tasks
150,"We manually define regular expression patterns mappings to normalized section titles : "" introduction "" , "" related work "" , "" method "" , "" experiments "" , "" conclusion "" .",Method,Data for scaffold tasks,Sentence_classification,0,,,50,0.9615384615384616,149,0.5580524344569289,4,0.6666666666666666,1,0,Method: Data for scaffold tasks
152,"Overall , the size of the data for scaffold tasks on the ACL - ARC dataset is about 47K ( section title scaffold ) and 50 K ( citation worthiness ) while on SciCite is about 91 K and 73 K for section title and citation worthiness scaffolds , respectively .",Method,Data for scaffold tasks,Sentence_classification,0,,,52,1.0,151,0.5655430711610487,6,1.0,1,0,Method: Data for scaffold tasks
155,We implement our proposed scaffold framework using the AllenNLP library .,Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']",1,0.0625,154,0.5767790262172284,1,0.0625,1,1,Implementation
156,"For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .",Implementation,Implementation,Sentence_classification,0,"['B', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']",2,0.125,155,0.5805243445692884,2,0.125,1,1,Implementation
157,"For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .",Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'O']",,3,0.1875,156,0.5842696629213483,3,0.1875,1,1,Implementation
158,We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .,Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']",4,0.25,157,0.5880149812734082,4,0.25,1,1,Implementation
159,"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .",Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",5,0.3125,158,0.5917602996254682,5,0.3125,1,1,Implementation
161,i are tuned for best performance on the validation set of the respective datasets using a 0.0 to 0.3 grid search .,Implementation,Implementation,Sentence_classification,0,,,7,0.4375,160,0.599250936329588,7,0.4375,1,0,Implementation
163,"Citation worthiness saffold : ? 2 = 0.08 , ? 3 = 0 , section title scaffold : ? 3 = 0.09 , ? 2 = 0 ; both scaffolds : ? 2 = 0.1 , ? 3 = 0.05 .",Implementation,Implementation,Sentence_classification,0,,,9,0.5625,162,0.6067415730337079,9,0.5625,1,0,Implementation
164,Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .,Implementation,Implementation,Sentence_classification,0,"['B', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,10,0.625,163,0.6104868913857678,10,0.625,1,1,Implementation
166,"On the smaller dataset , our best model takes approximately 30 minutes per epoch to train ( training time without ELMo is significantly faster ) .",Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,12,0.75,165,0.6179775280898876,12,0.75,1,0,Implementation: We use Beaker 12 for running the experiments .
167,It is known that multiple runs of probabilistic deep learning models can have variance in over all scores .,Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,13,0.8125,166,0.6217228464419475,13,0.8125,1,0,Implementation: We use Beaker 12 for running the experiments .
172,We compare our results to several baselines including the model with state - of - the - art performance on the ACL - ARC dataset .,Baselines,Baselines,Sentence_classification,0,,,1,0.1428571428571428,171,0.6404494382022472,1,0.1428571428571428,1,0,Baselines
173,BiLSTM Attention ( with and without ELMo ) .,Baselines,,Sentence_classification,0,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']",2,0.2857142857142857,172,0.6441947565543071,2,0.2857142857142857,1,1,Baselines
174,"This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .",Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.4285714285714285,173,0.6479400749063671,3,0.4285714285714285,1,1,Baselines: BiLSTM Attention ( with and without ELMo ) .
175,We experiment with two variants of this model : with and without using the contextualized word vector representations ( ELMo ) of .,Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,4,0.5714285714285714,174,0.651685393258427,4,0.5714285714285714,1,0,Baselines: BiLSTM Attention ( with and without ELMo ) .
177,leave - one - out cross validation in our experiments since it is impractical to re-train each variant of our deep learning models thousands of times .,Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,6,0.8571428571428571,176,0.6591760299625468,6,0.8571428571428571,1,0,Baselines: BiLSTM Attention ( with and without ELMo ) .
178,"Therefore , we opted for a standard setup of stratified train / validation / test data splits with 85 % data used for training and the rest equally split between validation and test .",Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,7,1.0,177,0.6629213483146067,7,1.0,1,0,Baselines: BiLSTM Attention ( with and without ELMo ) .
181,We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,Results,Results,Sentence_classification,0,"['O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']",2,0.0333333333333333,180,0.6741573033707865,2,0.0952380952380952,1,1,Results
182,"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .",Results,Results,Sentence_classification,0,"['B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",3,0.05,181,0.6779026217228464,3,0.1428571428571428,1,1,Results
183,Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .,Results,Results,Sentence_classification,0,"['B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']",4,0.0666666666666666,182,0.6816479400749064,4,0.1904761904761904,1,1,Results
184,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",Results,Results,Sentence_classification,0,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.0833333333333333,183,0.6853932584269663,5,0.238095238095238,1,1,Results
185,"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .",Results,Results,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'B', 'B', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']",,6,0.1,184,0.6891385767790262,6,0.2857142857142857,1,1,Results
186,"We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .",Results,Results,Sentence_classification,0,"['O', 'B', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.1166666666666666,185,0.6928838951310862,7,0.3333333333333333,1,1,Results
188,"We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .",Results,Results,Sentence_classification,0,"['O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",9,0.15,187,0.700374531835206,9,0.4285714285714285,1,1,Results
190,"shows the main results on SciCite dataset , where we see similar patterns .",Results,Results,Sentence_classification,0,,,11,0.1833333333333333,189,0.7078651685393258,11,0.5238095238095238,1,0,Results
195,"On this dataset , the best baseline is the neural baseline with addition of ELMo contextual vectors achieving an F 1 score of 82.6 followed by , which is expected because neural models generally achieve higher gains when more training data is available and because was not designed with the SciCite dataset in mind .",Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,,,16,0.2666666666666666,194,0.7265917602996255,16,0.7619047619047619,1,0,Results: Adding both scaffolds results in further improvements .
196,The breakdown of results by intent on ACL - ARC and SciCite datasets is respectively shown in Tables 5 and 6 .,Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,,,17,0.2833333333333333,195,0.7303370786516854,17,0.8095238095238095,1,0,Results: Adding both scaffolds results in further improvements .
197,Generally we observe that results on categories with more number of instances are higher .,Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']",18,0.3,196,0.7340823970037453,18,0.8571428571428571,1,1,Results: Adding both scaffolds results in further improvements .
202,"To gain more insight into why the scaffolds are helping the model in improved citation intent classification , we examine the attention weights assigned to inputs for our best proposed model ( ' BiLSTM - Attn w / ELMo + both scaffolds ' ) compared with the best neural baseline ( ' BiLSTM - Attn w / ELMO ' ) .",Results,Analysis,Sentence_classification,0,,,23,0.3833333333333333,201,0.7528089887640449,1,0.0588235294117647,1,0,Results: Analysis
204,shows an example input citation along with the horizontal line and the heatmap of attention weights for this input resulting from our model versus the baseline .,Results,Analysis,Sentence_classification,0,,,25,0.4166666666666667,203,0.7602996254681648,3,0.1764705882352941,1,0,Results: Analysis
206,"We observe that our model puts more weight on words surrounding the word "" future "" which is plausible given the true label .",Results,Analysis,Sentence_classification,0,,,27,0.45,205,0.7677902621722846,5,0.2941176470588235,1,0,Results: Analysis
207,"On the other hand , the baseline model attends most to the words "" compare "" and consequently incorrectly predicts a COMPARE label .",Results,Analysis,Sentence_classification,0,,,28,0.4666666666666667,206,0.7715355805243446,6,0.3529411764705882,1,0,Results: Analysis
209,"The baseline incorrectly classifies it as a BACK - GROUND , likely due to attending to another part of the sentence ( "" analyzed seprately "" ) .",Results,Analysis,Sentence_classification,0,,,30,0.5,208,0.7790262172284644,8,0.4705882352941176,1,0,Results: Analysis
210,Our model correctly classifies this instance by putting more attention weights on words that relate to comparison of the results .,Results,Analysis,Sentence_classification,0,,,31,0.5166666666666667,209,0.7827715355805244,9,0.5294117647058824,1,0,Results: Analysis
211,This suggests that the our model is more successful in learning optimal parameters for representing the citation text and classifying its respective intent compared with the baseline .,Results,Analysis,Sentence_classification,0,,,32,0.5333333333333333,210,0.7865168539325843,10,0.5882352941176471,1,0,Results: Analysis
216,One general error pattern is that the model has more tendency to make false positive errors in the BACKGROUND category likely due to this category dominating both datasets .,Results,Error analysis .,Sentence_classification,0,,,37,0.6166666666666667,215,0.8052434456928839,15,0.8823529411764706,1,0,Results: Error analysis .
217,It 's interesting that for the ACL - ARC dataset some prediction Category ( # instances ),Results,Error analysis .,Sentence_classification,0,,,38,0.6333333333333333,216,0.8089887640449438,16,0.9411764705882352,1,0,Results: Error analysis .
227,A sample of model 's classification errors on ACL - ARC dataset errors are due to the model failing to properly differentiate the USE category with BACKGROUND .,Results,Error analysis .,Sentence_classification,0,,,48,0.8,226,0.846441947565543,2,0.1538461538461538,1,0,Results: Error analysis .
