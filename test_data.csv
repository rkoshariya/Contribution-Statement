idx,text,main_heading,heading,topic,paper_idx,BIO,BIO_1,BIO_2,offset1,pro1,offset2,pro2,offset3,pro3,mask,bi_labels,labels
1,title,,,Sentence_classification,0,,,,0,0.0,0,0.0,0,0.0,1,0,
2,Structural Scaffolds for Citation Intent Classification in Scientific Publications,title,title,Sentence_classification,0,"['O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I']","['O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n']","['O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob']",1,0.0,1,0.003745318352059925,1,0.0,1,1,research-problem
3,abstract,,,Sentence_classification,0,,,,0,0.0,2,0.00749063670411985,0,0.0,1,0,
4,"Identifying the intent of a citation in scientific papers ( e.g. , background information , use of methods , comparing results ) is critical for machine reading of individual publications and automated analysis of the scientific literature .",abstract,abstract,Sentence_classification,0,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",1,0.2,3,0.011235955056179775,1,0.2,1,1,research-problem
5,"We propose structural scaffolds , a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents .",abstract,abstract,Sentence_classification,0,,,,2,0.4,4,0.0149812734082397,2,0.4,1,0,
6,"Our model achieves a new state - of the - art on an existing ACL anthology dataset ( ACL - ARC ) with a 13.3 % absolute increase in F1 score , without relying on external linguistic resources or hand - engineered features as done in existing methods .",abstract,abstract,Sentence_classification,0,,,,3,0.6,5,0.018726591760299626,3,0.6,1,0,
7,"In addition , we introduce a new dataset of citation intents ( Sci - Cite ) which is more than five times larger and covers multiple scientific domains compared with existing datasets .",abstract,abstract,Sentence_classification,0,,,,4,0.8,6,0.02247191011235955,4,0.8,1,0,
8,Our code and data are available at : https://github.com/ allenai/scicite .,abstract,abstract,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'O']",5,1.0,7,0.026217228464419477,5,1.0,1,1,code
9,Introduction,,,Sentence_classification,0,,,,0,0.0,8,0.0299625468164794,0,0.0,1,0,
10,Citations play a unique role in scientific discourse and are crucial for understanding and analyzing scientific work .,Introduction,Introduction,Sentence_classification,0,,,,1,0.05555555555555555,9,0.033707865168539325,1,0.05555555555555555,1,0,
11,"They are also typically used as the main measure for assessing impact of scientific publications , venues , and researchers .",Introduction,Introduction,Sentence_classification,0,,,,2,0.1111111111111111,10,0.03745318352059925,2,0.1111111111111111,1,0,
12,The nature of citations can be different .,Introduction,,Sentence_classification,0,,,,3,0.16666666666666666,11,0.04119850187265917,3,0.16666666666666666,1,0,
13,Some citations indicate direct use of a method while some others merely serve as acknowledging a prior work .,Introduction,The nature of citations can be different .,Sentence_classification,0,,,,4,0.2222222222222222,12,0.0449438202247191,4,0.2222222222222222,1,0,
14,"Therefore , identifying the intent of citations ( is critical in improving automated analysis of academic literature and scientific impact measurement .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,5,0.2777777777777778,13,0.04868913857677903,5,0.2777777777777778,1,0,
15,"Other applications of citation intent classification are enhanced research experience , information retrieval , summarization ( Co - .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,6,0.3333333333333333,14,0.052434456928838954,6,0.3333333333333333,1,0,
16,"A previously described computerized force sensitive system was used to quantify gait cycle timing , specifically the swing time and the stride - to - stride variability of swing time .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,7,0.3888888888888889,15,0.056179775280898875,7,0.3888888888888889,1,0,
17,.,Introduction,The nature of citations can be different .,Sentence_classification,0,,,,8,0.4444444444444444,16,0.0599250936329588,8,0.4444444444444444,1,0,
18,"Title : Gait asymmetry in patients with Parkinson 's disease and elderly fallers ... han and , and studying evolution of scientific fields .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,9,0.5,17,0.06367041198501873,9,0.5,1,0,
19,"In this work , we approach the problem of citation intent classification by modeling the language expressed in the citation context .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",10,0.5555555555555556,18,0.06741573033707865,10,0.5555555555555556,1,1,research-problem
20,A citation context includes text spans in a citing paper describing a referenced work and has been shown to be the primary signal in intent classification .,Introduction,The nature of citations can be different .,Sentence_classification,0,,,,11,0.6111111111111112,19,0.07116104868913857,11,0.6111111111111112,1,0,
21,"Existing models for this problem are feature - based , modeling the citation context with respect to a set of predefined handengineered features ( such as linguistic patterns or cue phrases ) and ignoring other signals that could improve prediction .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,12,0.6666666666666666,20,0.0749063670411985,12,0.6666666666666666,1,0,
22,"In this paper we argue that better representations can be obtained directly from data , sidestepping problems associated with external features .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,13,0.7222222222222222,21,0.07865168539325842,13,0.7222222222222222,1,0,
23,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'I', 'B', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-b', 'B-p', 'B-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",14,0.7777777777777778,22,0.08239700374531835,14,0.7777777777777778,1,1,model
24,"In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",15,0.8333333333333334,23,0.08614232209737828,15,0.8333333333333334,1,1,model
25,"Unlike the primary task of citation intent prediction , it is easy to collect large amounts of training data for scaffold tasks since the labels naturally occur in the process of writing a paper and thus , there is no need for manual annotation .",Introduction,The nature of citations can be different .,Sentence_classification,0,,,,16,0.8888888888888888,24,0.0898876404494382,16,0.8888888888888888,1,0,
26,"On two datasets , we show that the proposed neural scaffold model outperforms existing methods by large margins .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['B', 'B', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'B', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'O']",17,0.9444444444444444,25,0.09363295880149813,17,0.9444444444444444,1,1,model
27,"Our contributions are : ( i ) we propose a neural scaffold framework for citation intent classification to incorporate into citations knowledge from structure of scientific papers ; ( ii ) we achieve a new state - of - the - art of 67.9 % F1 on the ACL - ARC citations benchmark , an absolute 13.3 % increase over the previous state - of - the - art ; and ( iii ) we introduce SciCite , a new dataset of citation intents which is at least five times as large as existing datasets and covers a variety of scientific domains .",Introduction,The nature of citations can be different .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-b', 'B-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",18,1.0,26,0.09737827715355805,18,1.0,1,1,model
28,Model,,,Sentence_classification,0,,,,0,0.0,27,0.10112359550561797,0,0.0,1,0,
29,We propose a neural multitask learning framework for classification of citation intents .,Model,Model,Sentence_classification,0,,,,1,0.034482758620689655,28,0.10486891385767791,1,0.05555555555555555,1,0,
30,"In particular , we introduce and use two structural scaffolds , auxiliary tasks related to the structure of scientific papers .",Model,Model,Sentence_classification,0,,,,2,0.06896551724137931,29,0.10861423220973783,2,0.1111111111111111,1,0,
31,The auxiliary tasks may not be of interest by themselves but are used to inform the main task .,Model,Model,Sentence_classification,0,,,,3,0.10344827586206896,30,0.11235955056179775,3,0.16666666666666666,1,0,
32,Our model uses a large auxiliary dataset to incorporate this structural information available in scientific documents into the citation intents .,Model,Model,Sentence_classification,0,,,,4,0.13793103448275862,31,0.11610486891385768,4,0.2222222222222222,1,0,
33,The overview of our model is illustrated in .,Model,,Sentence_classification,0,,,,5,0.1724137931034483,32,0.1198501872659176,5,0.2777777777777778,1,0,
34,Let C denote the citation and x denote the ci-tation context relevant to C .,Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,6,0.20689655172413793,33,0.12359550561797752,6,0.3333333333333333,1,0,
35,"We encode the tokens in the citation context of size n as x = {x 1 , ... , x n } , where xi ?",Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,7,0.2413793103448276,34,0.12734082397003746,7,0.3888888888888889,1,0,
36,"Rd 1 is a word vector of size d 1 which concatenates non-contextualized word representations ( Glo Ve , and contextualized embeddings , i.e. :",Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,8,0.27586206896551724,35,0.13108614232209737,8,0.4444444444444444,1,0,
37,We then use a bidirectional long short - term memory ) ( BiL - STM ) network with hidden size of d 2 to obtain a contextual representation of each token vector with respect to the entire sequence :,Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,9,0.3103448275862069,36,0.1348314606741573,9,0.5,1,0,
38,2,Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,10,0.3448275862068966,37,0.13857677902621723,10,0.5555555555555556,1,0,
39,"where h ? R ( n , 2d 2 ) and ? ??? ?",Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,11,0.3793103448275862,38,0.14232209737827714,11,0.6111111111111112,1,0,
40,"LSTM ( x , i ) processes x from left to write and returns the LSTM hidden state at position i ( and vice versa for the backward direction ? ??? ?",Model,The overview of our model is illustrated in .,Sentence_classification,0,,,,12,0.41379310344827586,39,0.14606741573033707,12,0.6666666666666666,1,0,
41,LSTM ) .,Model,,Sentence_classification,0,,,,13,0.4482758620689655,40,0.149812734082397,13,0.7222222222222222,1,0,
42,We then use an attention mechanism to get a single vector representing the whole input sequence :,Model,LSTM ) .,Sentence_classification,0,,,,14,0.4827586206896552,41,0.15355805243445692,14,0.7777777777777778,1,0,
43,where w is a parameter served as the query vector for dot - product attention .,Model,LSTM ) .,Sentence_classification,0,,,,15,0.5172413793103449,42,0.15730337078651685,15,0.8333333333333334,1,0,
44,3,Model,LSTM ) .,Sentence_classification,0,,,,16,0.5517241379310345,43,0.16104868913857678,16,0.8888888888888888,1,0,
45,So far we have obtained the citation representation as a vector z .,Model,LSTM ) .,Sentence_classification,0,,,,17,0.5862068965517241,44,0.1647940074906367,17,0.9444444444444444,1,0,
46,"Next , we describe our two proposed structural scaffolds for citation intent prediction .",Model,LSTM ) .,Sentence_classification,0,,,,18,0.6206896551724138,45,0.16853932584269662,18,1.0,1,0,
47,Structural scaffolds,Model,,Sentence_classification,0,,,,19,0.6551724137931034,46,0.17228464419475656,0,0.0,1,0,
48,In scientific writing there is a connection between the structure of scientific papers and the intent of citations .,Model,Structural scaffolds,Sentence_classification,0,,,,20,0.6896551724137931,47,0.1760299625468165,1,0.03333333333333333,1,0,
49,"To leverage this connection for more effective classification of citation intents , we propose a multitask framework with two structural scaffolds ( auxiliary tasks ) related to the structure of scientific documents .",Model,Structural scaffolds,Sentence_classification,0,,,,21,0.7241379310344828,48,0.1797752808988764,2,0.06666666666666667,1,0,
50,A key point for our proposed scaffolds is that they do not need any additional manual annotation as labels for these tasks occur naturally in scientific writing .,Model,Structural scaffolds,Sentence_classification,0,,,,22,0.7586206896551724,49,0.18352059925093633,3,0.1,1,0,
51,The structural scaffolds in our model are the following :,Model,Structural scaffolds,Sentence_classification,0,,,,23,0.7931034482758621,50,0.18726591760299627,4,0.13333333333333333,1,0,
52,Citation worthiness .,Model,,Sentence_classification,0,,,,24,0.8275862068965517,51,0.19101123595505617,5,0.16666666666666666,1,0,
53,"The first scaffold task that we consider is "" citation worthiness "" of a sentence , indicating whether a sentence needs a citation .",Model,Citation worthiness .,Sentence_classification,0,,,,25,0.8620689655172413,52,0.1947565543071161,6,0.2,1,0,
54,"The language expressed in citation sentences is likely distinctive from regular sentences in scientific writing , and such information could also be useful for better language modeling of the citation contexts .",Model,Citation worthiness .,Sentence_classification,0,,,,26,0.896551724137931,53,0.19850187265917604,7,0.23333333333333334,1,0,
55,"To this end , using citation markers such as "" [ 12 ] "" or "" Lee et al ( 2010 ) "" , we identify sentences in a paper that include citations and the negative samples are sentences without citation markers .",Model,Citation worthiness .,Sentence_classification,0,,,,27,0.9310344827586207,54,0.20224719101123595,8,0.26666666666666666,1,0,
56,The goal of the model for this task is to predict whether a particular sentence needs a citation .,Model,Citation worthiness .,Sentence_classification,0,,,,28,0.9655172413793104,55,0.20599250936329588,9,0.3,1,0,
57,4,Model,Citation worthiness .,Sentence_classification,0,,,,29,1.0,56,0.20973782771535582,10,0.3333333333333333,1,0,
58,Section title .,,,Sentence_classification,0,,,,0,0.0,57,0.21348314606741572,11,0.36666666666666664,1,0,
59,The second scaffold task relates to predicting the section title in which a citation appears .,Section title .,Section title .,Sentence_classification,0,,,,1,0.05263157894736842,58,0.21722846441947566,12,0.4,1,0,
60,"Scientific documents follow a standard structure where the authors typically first introduce the problem , describe methodology , share results , discuss findings and conclude the paper .",Section title .,Section title .,Sentence_classification,0,,,,2,0.10526315789473684,59,0.2209737827715356,13,0.43333333333333335,1,0,
61,The intent of a citation could be relevant to the section of the paper in which the citation appears .,Section title .,Section title .,Sentence_classification,0,,,,3,0.15789473684210525,60,0.2247191011235955,14,0.4666666666666667,1,0,
62,"For example , method - related citations are more likely to appear in the methods section .",Section title .,Section title .,Sentence_classification,0,,,,4,0.21052631578947367,61,0.22846441947565543,15,0.5,1,0,
63,"Therefore , we use the section title prediction as a scaffold for predicting citation intents .",Section title .,Section title .,Sentence_classification,0,,,,5,0.2631578947368421,62,0.23220973782771537,16,0.5333333333333333,1,0,
64,Note that this scaffold task is different than simply adding section title as an additional feature in the input .,Section title .,Section title .,Sentence_classification,0,,,,6,0.3157894736842105,63,0.23595505617977527,17,0.5666666666666667,1,0,
65,We are using the section titles from a larger set of data than training data for the main task as a proxy to learn linguistic patterns thatare helpful for citation intents .,Section title .,Section title .,Sentence_classification,0,,,,7,0.3684210526315789,64,0.2397003745318352,18,0.6,1,0,
66,"In particular , we leverage a large number of scientific papers for which the section information is known for each citation to automatically generate large amounts of training data for this scaffold task .",Section title .,Section title .,Sentence_classification,0,,,,8,0.42105263157894735,65,0.24344569288389514,19,0.6333333333333333,1,0,
67,5,Section title .,Section title .,Sentence_classification,0,,,,9,0.47368421052631576,66,0.24719101123595505,20,0.6666666666666666,1,0,
68,Multitask formulation .,Section title .,,Sentence_classification,0,,,,10,0.5263157894736842,67,0.250936329588015,21,0.7,1,0,
69,Multitask learning as defined by is an approach to inductive transfer learning that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias .,Section title .,Multitask formulation .,Sentence_classification,0,,,,11,0.5789473684210527,68,0.2546816479400749,22,0.7333333333333333,1,0,
70,It requires the model to have at least some sharable parameters between the tasks .,Section title .,Multitask formulation .,Sentence_classification,0,,,,12,0.631578947368421,69,0.25842696629213485,23,0.7666666666666667,1,0,
71,"In a general setting in our model , we have a main task T ask ( 1 ) and n ? 1 auxiliary tasks T ask .",Section title .,Multitask formulation .,Sentence_classification,0,,,,13,0.6842105263157895,70,0.26217228464419473,24,0.8,1,0,
72,"As shown in , each scaffold task will have its task - specific parameters for effective classifica-tion and the parameters for the lower layers of the network are shared across tasks .",Section title .,Multitask formulation .,Sentence_classification,0,,,,14,0.7368421052631579,71,0.26591760299625467,25,0.8333333333333334,1,0,
73,We use a Multi Layer Perceptron ( MLP ) for each task and then a softmax layer to obtain prediction probabilites .,Section title .,Multitask formulation .,Sentence_classification,0,,,,15,0.7894736842105263,72,0.2696629213483146,26,0.8666666666666667,1,0,
74,"In particular , given the vector z we pass it ton MLPs and obtain n output vectors y ( i ) :",Section title .,Multitask formulation .,Sentence_classification,0,,,,16,0.8421052631578947,73,0.27340823970037453,27,0.9,1,0,
75,"We are only interested in the output y ( 1 ) and the rest of outputs ( y ( 2 ) , ... , y ( n ) ) are regarding the scaffold tasks and only used in training to inform the model of knowledge in the structure of the scientific documents .",Section title .,Multitask formulation .,Sentence_classification,0,,,,17,0.8947368421052632,74,0.27715355805243447,28,0.9333333333333333,1,0,
76,"For each task , we output the class with the highest probability in y .",Section title .,Multitask formulation .,Sentence_classification,0,,,,18,0.9473684210526315,75,0.2808988764044944,29,0.9666666666666667,1,0,
77,An alternative inference method is to sample from the output distribution .,Section title .,Multitask formulation .,Sentence_classification,0,,,,19,1.0,76,0.2846441947565543,30,1.0,1,0,
78,Training,,,Sentence_classification,0,,,,0,0.0,77,0.2883895131086142,0,0.0,1,0,
79,"Let D 1 be the labeled dataset for the main task T ask ( 1 ) , and Di denote the labeled datasets corresponding to the scaffold task T ask ( i ) where i ?",Training,Training,Sentence_classification,0,,,,1,0.0625,78,0.29213483146067415,1,0.08333333333333333,1,0,
80,"{ 2 , ... , n}. Similarly , let L 1 and Li be the main loss and the loss of the auxiliary task i , respectively .",Training,Training,Sentence_classification,0,,,,2,0.125,79,0.2958801498127341,2,0.16666666666666666,1,0,
81,The final loss of the model is :,Training,Training,Sentence_classification,0,,,,3,0.1875,80,0.299625468164794,3,0.25,1,0,
82,where ?,Training,Training,Sentence_classification,0,,,,4,0.25,81,0.30337078651685395,4,0.3333333333333333,1,0,
83,i is a hyper - parameter specifying the sensitivity of the parameters of the model to each specific task .,Training,Training,Sentence_classification,0,,,,5,0.3125,82,0.30711610486891383,5,0.4166666666666667,1,0,
84,Here we have two scaffold tasks and hence n=3 . ?,Training,Training,Sentence_classification,0,,,,6,0.375,83,0.31086142322097376,6,0.5,1,0,
85,i could be tuned based on performance on validation set ( see 4 for details ) .,Training,Training,Sentence_classification,0,,,,7,0.4375,84,0.3146067415730337,7,0.5833333333333334,1,0,
86,We train this model jointly across tasks and in an end - to - end fashion .,Training,Training,Sentence_classification,0,,,,8,0.5,85,0.31835205992509363,8,0.6666666666666666,1,0,
87,"In each training epoch , we construct mini-batches with the same number of instances from each of then tasks .",Training,Training,Sentence_classification,0,,,,9,0.5625,86,0.32209737827715357,9,0.75,1,0,
88,"We compute the total loss for each mini-batch as described in Equation 1 , where Li = 0 for all instances of other tasks j =i .",Training,Training,Sentence_classification,0,,,,10,0.625,87,0.3258426966292135,10,0.8333333333333334,1,0,
89,"We compute the gradient of the loss for each mini-batch and tune model parameters using the AdaDelta optimizer ( Zeiler , 2012 ) with gradient clipping threshold of 5.0 .",Training,Training,Sentence_classification,0,,,,11,0.6875,88,0.3295880149812734,11,0.9166666666666666,1,0,
90,We stop training the model when the development macro F1 score does not improve for five consecutive epochs .,Training,Training,Sentence_classification,0,,,,12,0.75,89,0.3333333333333333,12,1.0,1,0,
91,Data,Training,,Sentence_classification,0,,,,13,0.8125,90,0.33707865168539325,0,0.0,1,0,
92,We compare our results on two datasets from different scientific domains .,Training,Data,Sentence_classification,0,,,,14,0.875,91,0.3408239700374532,1,0.5,1,0,
93,"While there has been along history of studying citation intents , there are only a few existing publicly available datasets on Intent cateogry Definition",Training,Data,Sentence_classification,0,,,,15,0.9375,92,0.3445692883895131,2,1.0,1,0,
94,Example,Training,,Sentence_classification,0,,,,16,1.0,93,0.34831460674157305,0,0.0,1,0,
95,Background information,,,Sentence_classification,0,,,,0,0.0,94,0.352059925093633,0,0.0,1,0,
96,"The citation states , mentions , or points to the background information giving more context about a problem , concept , approach , topic , or importance of the problem in the field .",Background information,Background information,Sentence_classification,0,,,,1,0.25,95,0.35580524344569286,1,0.25,0,0,
97,Recent evidence suggests that co-occurring alexithymia may explain deficits .,Background information,Background information,Sentence_classification,0,,,,2,0.5,96,0.3595505617977528,2,0.5,0,0,
98,Locally high - temperature melting regions can act as permanent termination sites .,Background information,Background information,Sentence_classification,0,,,,3,0.75,97,0.36329588014981273,3,0.75,0,0,
99,One line of work is focused on changing the objective function .,Background information,Background information,Sentence_classification,0,,,,4,1.0,98,0.36704119850187267,4,1.0,0,0,
100,Method,,,Sentence_classification,0,,,,0,0.0,99,0.3707865168539326,0,0.0,1,0,
101,"Making use of a method , tool , approach or dataset",Method,Method,Sentence_classification,0,,,,1,0.019230769230769232,100,0.37453183520599254,1,0.06666666666666667,1,0,
102,Fold differences were calculated by a mathematical model described in .,Method,Method,Sentence_classification,0,,,,2,0.038461538461538464,101,0.3782771535580524,2,0.13333333333333333,1,0,
103,We use Orthogonal Initialization Result comparison,Method,Method,Sentence_classification,0,,,,3,0.057692307692307696,102,0.38202247191011235,3,0.2,1,0,
104,Comparison of the paper 's results / findings with the results / findings of other work,Method,Method,Sentence_classification,0,,,,4,0.07692307692307693,103,0.3857677902621723,4,0.26666666666666666,1,0,
105,Weighted measurements were superior to T2 - weighted contrast imaging which was in accordance with former studies Similar results to our study were reported in the study of Lee et al ( 2010 ) .,Method,Method,Sentence_classification,0,,,,5,0.09615384615384616,104,0.3895131086142322,5,0.3333333333333333,1,0,
106,the task of citation intent classification .,Method,Method,Sentence_classification,0,,,,6,0.11538461538461539,105,0.39325842696629215,6,0.4,1,0,
107,We use the most recent and comprehensive ( ACL - ARC citations dataset ) by as a benchmark dataset to compare the performance of our model to previous work .,Method,Method,Sentence_classification,0,,,,7,0.1346153846153846,106,0.3970037453183521,7,0.4666666666666667,1,0,
108,"In addition , to address the limited scope and size of this dataset , we introduce SciCite , a new dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL - ARC .",Method,Method,Sentence_classification,0,,,,8,0.15384615384615385,107,0.40074906367041196,8,0.5333333333333333,1,0,
109,Below is a description of both datasets .,Method,,Sentence_classification,0,,,,9,0.17307692307692307,108,0.4044943820224719,9,0.6,1,0,
110,ACL - ARC citations dataset,Method,,Sentence_classification,0,,,,10,0.19230769230769232,109,0.40823970037453183,10,0.6666666666666666,1,0,
111,ACL - ARC is a dataset of citation intents released by .,Method,ACL - ARC citations dataset,Sentence_classification,0,,,,11,0.21153846153846154,110,0.41198501872659177,11,0.7333333333333333,1,0,
112,"The dataset is based on a sample of papers from the ACL Anthology Reference Corpus and includes 1,941 citation instances from 186 papers and is annotated by domain experts in the NLP field .",Method,ACL - ARC citations dataset,Sentence_classification,0,,,,12,0.23076923076923078,111,0.4157303370786517,12,0.8,1,0,
113,"The data was split into three standard stratified sets of train , validation , and test with 85 % of data used for training and remaining 15 % divided equally for validation and test .",Method,ACL - ARC citations dataset,Sentence_classification,0,,,,13,0.25,112,0.41947565543071164,13,0.8666666666666667,1,0,
114,"Each citation unit includes information about the immediate citation context , surrounding context , as well as information about the citing and cited paper .",Method,ACL - ARC citations dataset,Sentence_classification,0,,,,14,0.2692307692307692,113,0.4232209737827715,14,0.9333333333333333,1,0,
115,The data includes six intent categories outlined in .,Method,,Sentence_classification,0,,,,15,0.28846153846153844,114,0.42696629213483145,15,1.0,1,0,
116,SciCite dataset,Method,,Sentence_classification,0,,,,16,0.3076923076923077,115,0.4307116104868914,0,0.0,1,0,
117,Most existing datasets contain citation categories thatare too fine - grained .,Method,SciCite dataset,Sentence_classification,0,,,,17,0.3269230769230769,116,0.4344569288389513,1,0.1,1,0,
118,Some of these intent categories are very rare or not useful in meta analysis of scientific publications .,Method,SciCite dataset,Sentence_classification,0,,,,18,0.34615384615384615,117,0.43820224719101125,2,0.2,1,0,
119,"Since some of these fine - grained categories only cover a minimal percentage of all citations , it is difficult to use them to gain insights or draw conclusions on impacts of papers .",Method,SciCite dataset,Sentence_classification,0,,,,19,0.36538461538461536,118,0.4419475655430712,3,0.3,1,0,
120,"Furthermore , these datasets are usually domain - specific and are relatively small ( less than 2,000 annotated citations ) .",Method,SciCite dataset,Sentence_classification,0,,,,20,0.38461538461538464,119,0.44569288389513106,4,0.4,1,0,
121,"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .",Method,SciCite dataset,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",21,0.40384615384615385,120,0.449438202247191,5,0.5,1,1,dataset
122,"Through examination of citation intents , we found out many of the categories defined in previous work such as motivation , extension or future work , can be considered as background information providing more context for the current research topic .",Method,SciCite dataset,Sentence_classification,0,,,,22,0.4230769230769231,121,0.45318352059925093,6,0.6,1,0,
123,More interesting intent categories are a direct use of a method or comparison of results .,Method,SciCite dataset,Sentence_classification,0,,,,23,0.4423076923076923,122,0.45692883895131087,7,0.7,1,0,
124,"Therefore , our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers .",Method,SciCite dataset,Sentence_classification,0,,,,24,0.46153846153846156,123,0.4606741573033708,8,0.8,1,0,
125,"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .",Method,SciCite dataset,Sentence_classification,0,"['O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'O']","['O', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'O', 'B-n', 'O']","['O', 'B-p', 'B-b', 'I-b', 'I-b', 'O', 'O', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'O', 'B-ob', 'O']",25,0.4807692307692308,124,0.46441947565543074,9,0.9,1,1,dataset
126,Below we describe data collection and annotation details .,Method,,Sentence_classification,0,,,,26,0.5,125,0.4681647940074906,10,1.0,1,0,
127,Data collection and annotation,Method,,Sentence_classification,0,,,,27,0.5192307692307693,126,0.47191011235955055,0,0.0,1,0,
128,Citation intent of sentence extractions was labeled through the crowdsourcing platform .,Method,Data collection and annotation,Sentence_classification,0,"['B', 'I', 'B', 'B', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",28,0.5384615384615384,127,0.4756554307116105,1,0.05555555555555555,1,1,dataset
129,"We selected a sample of papers from the Semantic Scholar corpus , 7 consisting of papers in general computer science and medicine domains .",Method,Data collection and annotation,Sentence_classification,0,,,,29,0.5576923076923077,128,0.4794007490636704,2,0.1111111111111111,1,0,
130,Citation contexts were extracted using science - parse .,Method,,Sentence_classification,0,,,,30,0.5769230769230769,129,0.48314606741573035,3,0.16666666666666666,1,0,
131,"The annotators were asked to identify the intent of a citation , and were directed to select among three citation intent options :",Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,,31,0.5961538461538461,130,0.4868913857677903,4,0.2222222222222222,1,0,
132,"METHOD , RESULTCOMPARISON and BACKGROUND .",Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,,32,0.6153846153846154,131,0.49063670411985016,5,0.2777777777777778,1,0,
133,The annotation interface also included a dummy option OTHER which helps improve the quality of annotations of other categories .,Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,,33,0.6346153846153846,132,0.4943820224719101,6,0.3333333333333333,1,0,
134,"We later removed instances annotated with the OTHER option from our dataset ( less than 1 % of the annotated data ) , many of which were due to citation contexts which are incomplete or too short for the annotator to infer the citation intent .",Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,,34,0.6538461538461539,133,0.49812734082397003,7,0.3888888888888889,1,0,
135,We used 50 test questions annotated by a domain expert to ensure crowdsource workers were following directions and disqualify annotators with accuracy less than 75 % .,Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,,35,0.6730769230769231,134,0.50187265917603,8,0.4444444444444444,1,0,
136,"Furthermore , crowdsource workers were required to remain on the annotation page ( five annotations ) for at least ten seconds before proceeding to the next page .",Method,Citation contexts were extracted using science - parse .,Sentence_classification,0,,,,36,0.6923076923076923,135,0.5056179775280899,9,0.5,1,0,
137,Annotations were dynamically collected .,Method,,Sentence_classification,0,,,,37,0.7115384615384616,136,0.5093632958801498,10,0.5555555555555556,1,0,
138,The annotations were aggregated along with a confidence score describing the level of agreement between multiple crowdsource workers .,Method,Annotations were dynamically collected .,Sentence_classification,0,,,,38,0.7307692307692307,137,0.5131086142322098,11,0.6111111111111112,1,0,
139,The confidence score is the agreement on a single instance weighted by a trust score ( accuracy of the annotator on the initial 50 test questions ) .,Method,Annotations were dynamically collected .,Sentence_classification,0,,,,39,0.75,138,0.5168539325842697,12,0.6666666666666666,1,0,
140,"To only collect high quality annotations , instances with confidence score of ? 0.7 were discarded .",Method,Annotations were dynamically collected .,Sentence_classification,0,,,,40,0.7692307692307693,139,0.5205992509363296,13,0.7222222222222222,1,0,
141,"In addition , a subset of the dataset with 100 samples was re-annotated by a trained , expert annotator to check for quality , and the agreement rate with crowdsource workers was 86 % .",Method,Annotations were dynamically collected .,Sentence_classification,0,,,,41,0.7884615384615384,140,0.5243445692883895,14,0.7777777777777778,1,0,
142,"Citation contexts were annotated by 850 crowdsource workers who made a total of 29,926 annotations and individually made between 4 and 240 annotations .",Method,Annotations were dynamically collected .,Sentence_classification,0,"['B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'O', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",42,0.8076923076923077,141,0.5280898876404494,15,0.8333333333333334,1,1,dataset
143,"Each sentence was annotated , on average , 3.74 times .",Method,Annotations were dynamically collected .,Sentence_classification,0,"['O', 'B', 'B', 'B', 'O', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-n', 'B-p', 'B-n', 'O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-b', 'B-p', 'B-b', 'O', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",43,0.8269230769230769,142,0.5318352059925093,16,0.8888888888888888,1,1,dataset
144,"This resulted in a total 9,159 crowdsourced instances which were divided to training and validation sets with 90 % of the data used for the training set .",Method,Annotations were dynamically collected .,Sentence_classification,0,"['O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'O']","['O', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'I-p', 'O', 'B-b', 'I-b', 'B-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'O']",44,0.8461538461538461,143,0.5355805243445693,17,0.9444444444444444,1,1,dataset
145,"In addition to the crowdsourced data , a separate test set of size 1,861 was annotated by a trained , expert annotator to ensure high quality of the dataset .",Method,Annotations were dynamically collected .,Sentence_classification,0,,,,45,0.8653846153846154,144,0.5393258426966292,18,1.0,1,0,
146,Data for scaffold tasks,Method,,Sentence_classification,0,,,,46,0.8846153846153846,145,0.5430711610486891,0,0.0,1,0,
147,"For the first scaffold ( citation worthiness ) , we sample sentences from papers and consider the sentences with citations as positive labels .",Method,Data for scaffold tasks,Sentence_classification,0,,,,47,0.9038461538461539,146,0.5468164794007491,1,0.16666666666666666,1,0,
148,"We also remove the citation markers from those sentences such as numbered citations ( e.g. , [ 1 ] ) or name - year combinations ( e.g , Lee et al ) to not make the second task artificially easy by only detecting citation markers .",Method,Data for scaffold tasks,Sentence_classification,0,,,,48,0.9230769230769231,147,0.550561797752809,2,0.3333333333333333,1,0,
149,"For the second scaffold ( citation section title ) , respective to each test dataset , we sample citations from the ACL - ARC corpus and Semantic Scholar corpus 9 and extract the citation context as well as their corresponding sections .",Method,Data for scaffold tasks,Sentence_classification,0,,,,49,0.9423076923076923,148,0.5543071161048689,3,0.5,1,0,
150,"We manually define regular expression patterns mappings to normalized section titles : "" introduction "" , "" related work "" , "" method "" , "" experiments "" , "" conclusion "" .",Method,Data for scaffold tasks,Sentence_classification,0,,,,50,0.9615384615384616,149,0.5580524344569289,4,0.6666666666666666,1,0,
151,Section titles which did not map to any of the aforementioned titles were excluded from the dataset .,Method,Data for scaffold tasks,Sentence_classification,0,,,,51,0.9807692307692307,150,0.5617977528089888,5,0.8333333333333334,1,0,
152,"Overall , the size of the data for scaffold tasks on the ACL - ARC dataset is about 47K ( section title scaffold ) and 50 K ( citation worthiness ) while on SciCite is about 91 K and 73 K for section title and citation worthiness scaffolds , respectively .",Method,Data for scaffold tasks,Sentence_classification,0,,,,52,1.0,151,0.5655430711610487,6,1.0,1,0,
153,Experiments,,,Sentence_classification,0,,,,0,0.0,152,0.5692883895131086,0,0.0,1,0,
154,Implementation,,,Sentence_classification,0,,,,0,0.0,153,0.5730337078651685,0,0.0,1,0,
155,We implement our proposed scaffold framework using the AllenNLP library .,Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'O', 'B', 'I', 'I', 'B', 'O', 'B', 'I', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'O']",1,0.0625,154,0.5767790262172284,1,0.0625,1,1,hyperparameters
156,"For word representations , we use 100 - dimensional GloVe vectors trained on a corpus of 6B tokens from Wikipedia and Gigaword .",Implementation,Implementation,Sentence_classification,0,"['B', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'B', 'B', 'I', 'I', 'O']","['B-p', 'B-n', 'I-n', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'B-p', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['B-p', 'B-b', 'I-b', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'I-p', 'O', 'B-b', 'B-p', 'B-ob', 'I-ob', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",2,0.125,155,0.5805243445692884,2,0.125,1,1,hyperparameters
157,"For contextual representations , we use ELMo vectors released by with output dimension size of 1,024 which have been trained on a dataset of 5.5 B tokens .",Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'B', 'I', 'I', 'O']",,,3,0.1875,156,0.5842696629213483,3,0.1875,1,1,hyperparameters
158,We use a single - layer BiLSTM with a hidden dimension size of 50 for each direction 11 .,Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'O', 'O', 'O', 'O']",4,0.25,157,0.5880149812734082,4,0.25,1,1,hyperparameters
159,"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .",Implementation,Implementation,Sentence_classification,0,"['O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'B', 'B', 'O', 'B', 'I', 'I', 'I', 'O']","['O', 'B-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'O', 'B-n', 'I-n', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O', 'B-ob', 'I-ob', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'O']",5,0.3125,158,0.5917602996254682,5,0.3125,1,1,hyperparameters
160,The hyperparameters ?,Implementation,Implementation,Sentence_classification,0,,,,6,0.375,159,0.5955056179775281,6,0.375,1,0,
161,i are tuned for best performance on the validation set of the respective datasets using a 0.0 to 0.3 grid search .,Implementation,Implementation,Sentence_classification,0,,,,7,0.4375,160,0.599250936329588,7,0.4375,1,0,
162,"For example , the following hyperparameters are used for the ACL - ARC .",Implementation,Implementation,Sentence_classification,0,,,,8,0.5,161,0.602996254681648,8,0.5,1,0,
163,"Citation worthiness saffold : ? 2 = 0.08 , ? 3 = 0 , section title scaffold : ? 3 = 0.09 , ? 2 = 0 ; both scaffolds : ? 2 = 0.1 , ? 3 = 0.05 .",Implementation,Implementation,Sentence_classification,0,,,,9,0.5625,162,0.6067415730337079,9,0.5625,1,0,
164,Batch size is 8 for ACL - ARC dataset and 32 for SciCite dataset ( recall that SciCite is larger than ACL - ARC ) .,Implementation,Implementation,Sentence_classification,0,"['B', 'I', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",,,10,0.625,163,0.6104868913857678,10,0.625,1,1,hyperparameters
165,We use Beaker 12 for running the experiments .,Implementation,,Sentence_classification,0,"['O', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'O']","['O', 'O', 'B-n', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'O']","['O', 'O', 'B-b', 'O', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'O']",11,0.6875,164,0.6142322097378277,11,0.6875,1,1,hyperparameters
166,"On the smaller dataset , our best model takes approximately 30 minutes per epoch to train ( training time without ELMo is significantly faster ) .",Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,,12,0.75,165,0.6179775280898876,12,0.75,1,0,
167,It is known that multiple runs of probabilistic deep learning models can have variance in over all scores .,Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,,13,0.8125,166,0.6217228464419475,13,0.8125,1,0,
168,We control this by setting random - number generator seeds ; the reported over all results are average of multiple runs with different random seeds .,Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,,14,0.875,167,0.6254681647940075,14,0.875,1,0,
169,"To facilitate reproducibility , we release our code , data , and trained models .",Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,,15,0.9375,168,0.6292134831460674,15,0.9375,1,0,
170,14,Implementation,We use Beaker 12 for running the experiments .,Sentence_classification,0,,,,16,1.0,169,0.6329588014981273,16,1.0,1,0,
171,Baselines,,,Sentence_classification,0,,,,0,0.0,170,0.6367041198501873,0,0.0,1,0,
172,We compare our results to several baselines including the model with state - of - the - art performance on the ACL - ARC dataset .,Baselines,Baselines,Sentence_classification,0,,,,1,0.14285714285714285,171,0.6404494382022472,1,0.14285714285714285,1,0,
173,BiLSTM Attention ( with and without ELMo ) .,Baselines,,Sentence_classification,0,"['B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']","['B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O']",2,0.2857142857142857,172,0.6441947565543071,2,0.2857142857142857,1,1,baselines
174,"This baseline uses a similar architecture to our proposed neural multitask learning framework , except that it only optimizes the network for the main loss regarding the citation intent classification ( L 1 ) and does not include the structural scaffolds .",Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,"['O', 'O', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'B-p', 'O', 'B-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",3,0.42857142857142855,173,0.6479400749063671,3,0.42857142857142855,1,1,baselines
175,We experiment with two variants of this model : with and without using the contextualized word vector representations ( ELMo ) of .,Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,,4,0.5714285714285714,174,0.651685393258427,4,0.5714285714285714,1,0,
176,This baseline is useful for evaluating the effect of adding scaffolds in controlled experiments .,Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,,5,0.7142857142857143,175,0.6554307116104869,5,0.7142857142857143,1,0,
177,leave - one - out cross validation in our experiments since it is impractical to re-train each variant of our deep learning models thousands of times .,Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,,6,0.8571428571428571,176,0.6591760299625468,6,0.8571428571428571,1,0,
178,"Therefore , we opted for a standard setup of stratified train / validation / test data splits with 85 % data used for training and the rest equally split between validation and test .",Baselines,BiLSTM Attention ( with and without ELMo ) .,Sentence_classification,0,,,,7,1.0,177,0.6629213483146067,7,1.0,1,0,
179,Results,,,Sentence_classification,0,,,,0,0.0,178,0.6666666666666666,0,0.0,1,0,
180,Our main results for the ACL - ARC dataset is shown in .,Results,Results,Sentence_classification,0,,,,1,0.016666666666666666,179,0.6704119850187266,1,0.047619047619047616,1,0,
181,We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,Results,Results,Sentence_classification,0,"['O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O']",2,0.03333333333333333,180,0.6741573033707865,2,0.09523809523809523,1,1,results
182,"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .",Results,Results,Sentence_classification,0,"['B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'B', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'O', 'B', 'I', 'B', 'B', 'I', 'I', 'I', 'O']","['B-p', 'I-p', 'O', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'I-p', 'O', 'O', 'B-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",3,0.05,181,0.6779026217228464,3,0.14285714285714285,1,1,results
183,Adding the second scaffold in ' BiLSTM - Attn + citation worthiness scaffold ' also results in similar improvements : 56.3 (?= 4.5 ) .,Results,Results,Sentence_classification,0,"['B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'O']","['B-p', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'O']","['B-p', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'I-b', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'O']",4,0.06666666666666667,182,0.6816479400749064,4,0.19047619047619047,1,1,results
184,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .",Results,Results,Sentence_classification,0,"['O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'B-n', 'I-n', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'B-b', 'I-b', 'O', 'B-p', 'I-p', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",5,0.08333333333333333,183,0.6853932584269663,5,0.23809523809523808,1,1,results
185,"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .",Results,Results,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'B', 'B', 'B', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'B', 'B', 'I', 'I', 'I', 'I', 'O']",,,6,0.1,184,0.6891385767790262,6,0.2857142857142857,1,1,results
186,"We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .",Results,Results,Sentence_classification,0,"['O', 'B', 'O', 'O', 'B', 'I', 'B', 'B', 'I', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-n', 'I-n', 'B-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']","['O', 'B-p', 'O', 'O', 'B-b', 'I-b', 'B-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",7,0.11666666666666667,185,0.6928838951310862,7,0.3333333333333333,1,1,results
187,We note that these results were obtained without using hand - curated features or additional linguistic resources as used in .,Results,Results,Sentence_classification,0,,,,8,0.13333333333333333,186,0.6966292134831461,8,0.38095238095238093,1,0,
188,"We also experimented with adding features used in to our best model and not only we did not see any improvements , but we observed at least 1.7 % decline in performance .",Results,Results,Sentence_classification,0,"['O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'B', 'B', 'O']","['O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'O', 'B-n', 'I-n', 'I-n', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'O', 'B-ob', 'I-ob', 'I-ob', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",9,0.15,187,0.700374531835206,9,0.42857142857142855,1,1,results
189,This suggests that these additional manual features do not provide the model with any additional useful signals beyond what the model already learns from the data .,Results,Results,Sentence_classification,0,,,,10,0.16666666666666666,188,0.704119850187266,10,0.47619047619047616,1,0,
190,"shows the main results on SciCite dataset , where we see similar patterns .",Results,Results,Sentence_classification,0,,,,11,0.18333333333333332,189,0.7078651685393258,11,0.5238095238095238,1,0,
191,Each scaffold task improves model performance .,Results,,Sentence_classification,0,"['B', 'I', 'I', 'B', 'B', 'I', 'O']","['B-n', 'I-n', 'I-n', 'B-p', 'B-n', 'I-n', 'O']","['B-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'I-ob', 'O']",12,0.2,190,0.7116104868913857,12,0.5714285714285714,1,1,results
192,Adding both scaffolds results in further improvements .,Results,,Sentence_classification,0,"['O', 'B', 'I', 'B', 'I', 'B', 'I', 'O']","['O', 'B-n', 'I-n', 'B-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'B-b', 'I-b', 'B-p', 'I-p', 'B-ob', 'I-ob', 'O']",13,0.21666666666666667,191,0.7153558052434457,13,0.6190476190476191,1,1,results
193,And the best results are obtained by using ELMo representation in addition to both scaffolds .,Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,"['O', 'O', 'B', 'I', 'O', 'O', 'B', 'I', 'B', 'I', 'B', 'I', 'I', 'B', 'I', 'O']","['O', 'O', 'B-n', 'I-n', 'O', 'O', 'B-p', 'I-p', 'B-n', 'I-n', 'B-p', 'I-p', 'I-p', 'B-n', 'I-n', 'O']","['O', 'O', 'B-s', 'I-s', 'O', 'O', 'B-p', 'I-p', 'B-b', 'I-b', 'B-p', 'I-p', 'I-p', 'B-ob', 'I-ob', 'O']",14,0.23333333333333334,192,0.7191011235955056,14,0.6666666666666666,1,1,results
194,"Note that this dataset is more than five times larger in size than the ACL - ARC , therefore the performance numbers are generally higher and the F 1 gains are generally smaller since it is easier for the models to learn optimal parameters utilizing the larger annotated data .",Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,,,,15,0.25,193,0.7228464419475655,15,0.7142857142857143,1,0,
195,"On this dataset , the best baseline is the neural baseline with addition of ELMo contextual vectors achieving an F 1 score of 82.6 followed by , which is expected because neural models generally achieve higher gains when more training data is available and because was not designed with the SciCite dataset in mind .",Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,,,,16,0.26666666666666666,194,0.7265917602996255,16,0.7619047619047619,1,0,
196,The breakdown of results by intent on ACL - ARC and SciCite datasets is respectively shown in Tables 5 and 6 .,Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,,,,17,0.2833333333333333,195,0.7303370786516854,17,0.8095238095238095,1,0,
197,Generally we observe that results on categories with more number of instances are higher .,Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'I', 'I', 'I', 'B', 'B', 'O']","['O', 'O', 'O', 'O', 'B-n', 'B-p', 'B-n', 'B-p', 'B-n', 'I-n', 'I-n', 'I-n', 'B-p', 'B-n', 'O']","['O', 'O', 'O', 'O', 'B-b', 'B-p', 'B-b', 'B-p', 'B-b', 'I-b', 'I-b', 'I-b', 'B-p', 'B-ob', 'O']",18,0.3,196,0.7340823970037453,18,0.8571428571428571,1,1,results
198,"For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .",Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,"['O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'B', 'O', 'B', 'B', 'O', 'B', 'B', 'O', 'B', 'I', 'O']",,,19,0.31666666666666665,197,0.7378277153558053,19,0.9047619047619048,1,1,results
199,"Conversely , the results on the FUTUREWORK category are the lowest .",Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'O', 'B', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-n', 'I-n', 'B-p', 'O', 'B-n', 'O']","['O', 'O', 'O', 'O', 'O', 'O', 'B-b', 'I-b', 'B-p', 'O', 'B-ob', 'O']",20,0.3333333333333333,198,0.7415730337078652,20,0.9523809523809523,1,1,results
200,This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .,Results,Adding both scaffolds results in further improvements .,Sentence_classification,0,"['O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'I', 'B', 'B', 'I', 'O', 'O', 'O', 'O']",,,21,0.35,199,0.7453183520599251,21,1.0,1,1,results
201,Analysis,Results,,Sentence_classification,0,,,,22,0.36666666666666664,200,0.7490636704119851,0,0.0,1,0,
202,"To gain more insight into why the scaffolds are helping the model in improved citation intent classification , we examine the attention weights assigned to inputs for our best proposed model ( ' BiLSTM - Attn w / ELMo + both scaffolds ' ) compared with the best neural baseline ( ' BiLSTM - Attn w / ELMO ' ) .",Results,Analysis,Sentence_classification,0,,,,23,0.38333333333333336,201,0.7528089887640449,1,0.058823529411764705,1,0,
203,We conduct this analysis for examples from both datasets .,Results,Analysis,Sentence_classification,0,,,,24,0.4,202,0.7565543071161048,2,0.11764705882352941,1,0,
204,shows an example input citation along with the horizontal line and the heatmap of attention weights for this input resulting from our model versus the baseline .,Results,Analysis,Sentence_classification,0,,,,25,0.4166666666666667,203,0.7602996254681648,3,0.17647058823529413,1,0,
205,For first example ( 3 a ) the true label is FU - TUREWORK .,Results,Analysis,Sentence_classification,0,,,,26,0.43333333333333335,204,0.7640449438202247,4,0.23529411764705882,1,0,
206,"We observe that our model puts more weight on words surrounding the word "" future "" which is plausible given the true label .",Results,Analysis,Sentence_classification,0,,,,27,0.45,205,0.7677902621722846,5,0.29411764705882354,1,0,
207,"On the other hand , the baseline model attends most to the words "" compare "" and consequently incorrectly predicts a COMPARE label .",Results,Analysis,Sentence_classification,0,,,,28,0.4666666666666667,206,0.7715355805243446,6,0.35294117647058826,1,0,
208,In second example ( 3 b ) the true label is RESULTCOMPARISON .,Results,Analysis,Sentence_classification,0,,,,29,0.48333333333333334,207,0.7752808988764045,7,0.4117647058823529,1,0,
209,"The baseline incorrectly classifies it as a BACK - GROUND , likely due to attending to another part of the sentence ( "" analyzed seprately "" ) .",Results,Analysis,Sentence_classification,0,,,,30,0.5,208,0.7790262172284644,8,0.47058823529411764,1,0,
210,Our model correctly classifies this instance by putting more attention weights on words that relate to comparison of the results .,Results,Analysis,Sentence_classification,0,,,,31,0.5166666666666667,209,0.7827715355805244,9,0.5294117647058824,1,0,
211,This suggests that the our model is more successful in learning optimal parameters for representing the citation text and classifying its respective intent compared with the baseline .,Results,Analysis,Sentence_classification,0,,,,32,0.5333333333333333,210,0.7865168539325843,10,0.5882352941176471,1,0,
212,Note that the only difference between our model and the neural baseline is inclusion of the structural scaffolds .,Results,Analysis,Sentence_classification,0,,,,33,0.55,211,0.7902621722846442,11,0.6470588235294118,1,0,
213,"Therefore , suggesting the effectiveness the scaffolds in informing the main task of relevant signals for citation intent classification .",Results,Analysis,Sentence_classification,0,,,,34,0.5666666666666667,212,0.7940074906367042,12,0.7058823529411765,1,0,
214,Error analysis .,Results,,Sentence_classification,0,,,,35,0.5833333333333334,213,0.797752808988764,13,0.7647058823529411,1,0,
215,We next investigate errors made by our best model plots classification errors ) .,Results,Error analysis .,Sentence_classification,0,,,,36,0.6,214,0.8014981273408239,14,0.8235294117647058,1,0,
216,One general error pattern is that the model has more tendency to make false positive errors in the BACKGROUND category likely due to this category dominating both datasets .,Results,Error analysis .,Sentence_classification,0,,,,37,0.6166666666666667,215,0.8052434456928839,15,0.8823529411764706,1,0,
217,It 's interesting that for the ACL - ARC dataset some prediction Category ( # instances ),Results,Error analysis .,Sentence_classification,0,,,,38,0.6333333333333333,216,0.8089887640449438,16,0.9411764705882353,1,0,
218,Background Compare Extension Future Motivation Use Average ( Macro ),Results,Error analysis .,Sentence_classification,0,,,,39,0.65,217,0.8127340823970037,17,1.0,1,0,
219,MOTIVATION USE,Results,Error analysis .,Sentence_classification,0,,,,40,0.6666666666666666,218,0.8164794007490637,0,0.0,1,0,
220,ASARES is presented in detail in ( CITATION ) .,Results,Error analysis .,Sentence_classification,0,,,,41,0.6833333333333333,219,0.8202247191011236,1,0.0,1,0,
221,USE BACKGROUND,Results,Error analysis .,Sentence_classification,0,,,,42,0.7,220,0.8239700374531835,0,0.0,1,0,
222,The advantage of tuning similarity to the application of interest has been shown previously by ( CITATION ) .,Results,Error analysis .,Sentence_classification,0,,,,43,0.7166666666666667,221,0.8277153558052435,1,0.0,1,0,
223,COMPARE BACKGROUND,Results,Error analysis .,Sentence_classification,0,,,,44,0.7333333333333333,222,0.8314606741573034,0,0.0,1,0,
224,"One possible direction is to consider linguistically motivated approaches , such as the extraction of syntactic phrase tables as proposed by ( CITATION ) .",Results,Error analysis .,Sentence_classification,0,,,,45,0.75,223,0.8352059925093633,1,0.0,1,0,
225,FUTUREWORK BACKGROUND,Results,Error analysis .,Sentence_classification,0,,,,46,0.7666666666666667,224,0.8389513108614233,0,0.0,1,0,
226,"After the extraction , pruning techniques ( CITATION ) can be applied to increase the precision of the extraction . :",Results,Error analysis .,Sentence_classification,0,,,,47,0.7833333333333333,225,0.8426966292134831,1,0.07692307692307693,1,0,
227,A sample of model 's classification errors on ACL - ARC dataset errors are due to the model failing to properly differentiate the USE category with BACKGROUND .,Results,Error analysis .,Sentence_classification,0,,,,48,0.8,226,0.846441947565543,2,0.15384615384615385,1,0,
228,We found out that some of these errors would have been possibly prevented by using additional context .,Results,Error analysis .,Sentence_classification,0,,,,49,0.8166666666666667,227,0.850187265917603,3,0.23076923076923078,1,0,
229,shows a sample of such classification errors .,Results,Error analysis .,Sentence_classification,0,,,,50,0.8333333333333334,228,0.8539325842696629,4,0.3076923076923077,1,0,
230,"For the citation in the first row of the table , the model is likely distracted by "" model in ( citation ) "" and "" ILP formulation from ( citation ) "" deeming the sentence is referring to the use of another method from a cited paper and it misses the first part of the sentence describing the motivation .",Results,Error analysis .,Sentence_classification,0,,,,51,0.85,229,0.8576779026217228,5,0.38461538461538464,1,0,
231,"This is likely due to the small number of training instances in the MOTIVATION category , preventing the model to learn such nuances .",Results,Error analysis .,Sentence_classification,0,,,,52,0.8666666666666667,230,0.8614232209737828,6,0.46153846153846156,1,0,
232,"For the examples in the second and third row , it is not clear if it is possible to make the correct prediction without additional context .",Results,Error analysis .,Sentence_classification,0,,,,53,0.8833333333333333,231,0.8651685393258427,7,0.5384615384615384,1,0,
233,And similarly in the last row the instance seems ambiguous without accessing to additional context .,Results,Error analysis .,Sentence_classification,0,,,,54,0.9,232,0.8689138576779026,8,0.6153846153846154,1,0,
234,Similarly as shown in two of FUTUREWORK labels are wrongly classified .,Results,Error analysis .,Sentence_classification,0,,,,55,0.9166666666666666,233,0.8726591760299626,9,0.6923076923076923,1,0,
235,One of them is illustrated in the forth row of where perhaps additional context could have helped the model in identifying the correct label .,Results,Error analysis .,Sentence_classification,0,,,,56,0.9333333333333333,234,0.8764044943820225,10,0.7692307692307693,1,0,
236,"One possible way to prevent this type of errors , is to provide the model with an additional input , modeling the extended surrounding context .",Results,Error analysis .,Sentence_classification,0,,,,57,0.95,235,0.8801498127340824,11,0.8461538461538461,1,0,
237,"We experimented with encoding the extended surrounding context using a BiLSTM and concatenating it with the main citation context vector ( z ) , but it resulted in a large decline in over all performance likely due to the over all noise introduced by the additional context .",Results,Error analysis .,Sentence_classification,0,,,,58,0.9666666666666667,236,0.8838951310861424,12,0.9230769230769231,1,0,
238,A possible future work is to investigate alternative effective approaches for incorporating the surrounding extended context .,Results,Error analysis .,Sentence_classification,0,,,,59,0.9833333333333333,237,0.8876404494382022,13,1.0,1,0,
239,BACKGROUND USE,Results,Error analysis .,Sentence_classification,0,,,,60,1.0,238,0.8913857677902621,0,0.0,1,0,
240,Related Work,,,Sentence_classification,0,,,,0,0.0,239,0.8951310861423221,0,0.0,1,0,
241,There is a large body of work studying the intent of citations and devising categorization systems .,Related Work,Related Work,Sentence_classification,0,,,,1,0.0625,240,0.898876404494382,1,0.0625,0,0,
242,"Most of these efforts provide citation categories thatare too fine - grained , some of which rarely occur in papers .",Related Work,Related Work,Sentence_classification,0,,,,2,0.125,241,0.9026217228464419,2,0.125,0,0,
243,"Therefore , they are hardly useful for automated analysis of scientific publications .",Related Work,Related Work,Sentence_classification,0,,,,3,0.1875,242,0.9063670411985019,3,0.1875,0,0,
244,"To address these problems and to unify previous efforts , in a recent work , proposed a six category system for citation intents .",Related Work,Related Work,Sentence_classification,0,,,,4,0.25,243,0.9101123595505618,4,0.25,0,0,
245,"In this work , we focus on two schemes : ( 1 ) the scheme proposed by and an additional , more coarse - grained generalpurpose category system that we propose ( details in 3 ) .",Related Work,Related Work,Sentence_classification,0,,,,5,0.3125,244,0.9138576779026217,5,0.3125,0,0,
246,"Unlike other schemes thatare domainspecific , our scheme is general and naturally fits in scientific discourse in multiple domains .",Related Work,Related Work,Sentence_classification,0,,,,6,0.375,245,0.9176029962546817,6,0.375,0,0,
247,"Early works in automated citation intent classification were based on rule - based systems ( e.g. , ) .",Related Work,Related Work,Sentence_classification,0,,,,7,0.4375,246,0.9213483146067416,7,0.4375,0,0,
248,"Later , machine learning methods based on linguistic patterns and other hand - engineered features from citation context were found to be effective .",Related Work,Related Work,Sentence_classification,0,,,,8,0.5,247,0.9250936329588015,8,0.5,0,0,
249,"For example , proposed use of "" cue phrases "" , a set of expressions that talk about the act of presenting research in a paper .",Related Work,Related Work,Sentence_classification,0,,,,9,0.5625,248,0.9288389513108615,9,0.5625,0,0,
250,"Abu - Jbara et al. ( 2013 ) relied on lexical , structural , and syntactic features and a linear SVM for classification .",Related Work,Related Work,Sentence_classification,0,,,,10,0.625,249,0.9325842696629213,10,0.625,0,0,
251,Researchers have also investigated methods of finding cited spans in the cited papers .,Related Work,Related Work,Sentence_classification,0,,,,11,0.6875,250,0.9363295880149812,11,0.6875,0,0,
252,"Examples include feature - based methods , domain - specific knowledge , and a recent CNNbased model for joint prediction of cited spans and citation function .",Related Work,Related Work,Sentence_classification,0,,,,12,0.75,251,0.9400749063670412,12,0.75,0,0,
253,We also experimented with CNNs but found the attention BiL - STM model to work significantly better .,Related Work,Related Work,Sentence_classification,0,,,,13,0.8125,252,0.9438202247191011,13,0.8125,0,0,
254,"expanded all pre-existing featurebased efforts on citation intent classification by proposing a comprehensive set of engineered features , including boostrapped patterns , topic modeling , dependency - based , and metadata features for the task .",Related Work,Related Work,Sentence_classification,0,,,,14,0.875,253,0.947565543071161,14,0.875,0,0,
255,"We argue that we can capture necessary information from the citation context using a data driven method , without the need for handengineered domain - dependent features or external resources .",Related Work,Related Work,Sentence_classification,0,,,,15,0.9375,254,0.951310861423221,15,0.9375,0,0,
256,"We propose a novel scaffold neural model for citation intent classification to incorporate structural information of scientific discourse into citations , borrowing the "" scaffold "" terminology from who use auxiliary syntactic tasks for semantic problems .",Related Work,Related Work,Sentence_classification,0,,,,16,1.0,255,0.9550561797752809,16,1.0,0,0,
257,Conclusions and future work,,,Sentence_classification,0,,,,0,0.0,256,0.9588014981273408,0,0.0,1,0,
258,"In this work , we show that structural properties related to scientific discourse can be effectively used to inform citation intent classification .",Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,1,0.1,257,0.9625468164794008,1,0.1,0,0,
259,We propose a multitask learning framework with two auxiliary tasks ( predicting section titles and citation worthiness ) as two scaffolds related to the main task of citation intent prediction .,Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,2,0.2,258,0.9662921348314607,2,0.2,0,0,
260,Our model achieves state - of - the - art result ( F1 score of 67.9 % ) on the ACL - ARC dataset with 13.3 absolute increase over the best previous results .,Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,3,0.3,259,0.9700374531835206,3,0.3,0,0,
261,"We additionally introduce SciCite , a new large dataset of citation intents and also show the effectiveness of our model on this dataset .",Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,4,0.4,260,0.9737827715355806,4,0.4,0,0,
262,"Our dataset , unlike existing datasets thatare designed based on a specific domain , is more general and fits in scientific discourse from multiple scientific domains .",Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,5,0.5,261,0.9775280898876404,5,0.5,0,0,
263,We demonstrate that carefully chosen auxiliary tasks thatare inherently relevant to a main task can be leveraged to improve the performance on the main task .,Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,6,0.6,262,0.9812734082397003,6,0.6,0,0,
264,An interesting line of future work is to explore the design of such tasks or explore the properties or similarities between the auxiliary and the main tasks .,Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,7,0.7,263,0.9850187265917603,7,0.7,0,0,
265,Another relevant line of work is adapting our model to other domains containing documents with similar linked structured such as Wikipedia articles .,Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,8,0.8,264,0.9887640449438202,8,0.8,0,0,
266,Future work may benefit from replacing ELMo with other types of contextualized representations such as BERT in our scaffold model .,Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,9,0.9,265,0.9925093632958801,9,0.9,0,0,
267,"For example , at the time of finalizing the camera ready version of this paper , showed that a BERT contextualized representation model trained on scientific text can achieve promising results on the SciCite dataset .",Conclusions and future work,Conclusions and future work,Sentence_classification,0,,,,10,1.0,266,0.9962546816479401,10,1.0,0,0,
